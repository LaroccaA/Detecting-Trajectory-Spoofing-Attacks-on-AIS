{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db3be638",
   "metadata": {},
   "source": [
    "## LNN Addestramento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc5080a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurazione Memoria OK: 1 GPU\n",
      "Configurazione LNN caricata.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"Configurazione Memoria OK: {len(gpus)} GPU\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Errore Configurazione Memoria: {e}\")\n",
    "\n",
    "from ncps.tf import CfC\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import gc\n",
    "import joblib\n",
    "import pyarrow.parquet as pq\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, TimeDistributed, RepeatVector\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts, ExponentialDecay\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "from ncps.wirings import AutoNCP\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# PATH\n",
    "INPUT_DIR = '../Pre-Elaborazione Dati/Dataset_Ready_For_AI_FINAL' \n",
    "SCALER_PATH = 'scaler.joblib' \n",
    "COLONNE_FEATURES = ['Latitude', 'Longitude', 'SOG', 'COG']\n",
    "\n",
    "WINDOW_SIZE = 30  \n",
    "BATCH_SIZE = 256 \n",
    "\n",
    "all_files = sorted(glob.glob(os.path.join(INPUT_DIR, '*.parquet')))\n",
    "TRAIN_FILES = all_files[0:16]\n",
    "VAL_FILES = all_files[16:20]\n",
    "\n",
    "print(\"Configurazione LNN caricata.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6e9543",
   "metadata": {},
   "source": [
    "#### Funzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2c48d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funzioni definite\n"
     ]
    }
   ],
   "source": [
    "def create_windows(data_np, window_size):\n",
    "    windows = []\n",
    "    for i in range(len(data_np) - window_size + 1):\n",
    "        windows.append(data_np[i : i + window_size])\n",
    "    return windows\n",
    "\n",
    "def data_generator(file_paths, scaler, features, window_size, batch_size, shuffle_files=False):\n",
    "    \n",
    "    file_buffer = {} \n",
    "    window_buffer = [] \n",
    "    CHUNK_SIZE_ROWS = 500_000\n",
    "\n",
    "    while True:\n",
    "        if shuffle_files:\n",
    "             # Shuffle disattivato forzatamente per garantire la sequenzialit√†\n",
    "            shuffle_files = False \n",
    "            \n",
    "        for file_path in file_paths:\n",
    "            chunk_buffer = {}\n",
    "            try:\n",
    "                pf = pq.ParquetFile(file_path)\n",
    "                for batch in pf.iter_batches(batch_size=CHUNK_SIZE_ROWS, columns=features + ['TrajectoryID']):\n",
    "                    df_chunk = batch.to_pandas()\n",
    "                    df_chunk[features] = scaler.transform(df_chunk[features])\n",
    "                    next_chunk_buffer = {}\n",
    "                    \n",
    "                    for tid, group in df_chunk.groupby('TrajectoryID'):\n",
    "                        if tid in chunk_buffer:\n",
    "                            trajectory_data = pd.concat([chunk_buffer.pop(tid), group])\n",
    "                        else:\n",
    "                            trajectory_data = group\n",
    "                        \n",
    "                        if tid in file_buffer:\n",
    "                            trajectory_data = pd.concat([file_buffer.pop(tid), trajectory_data])\n",
    "                        \n",
    "                        # Se la traiettoria tocca la fine del chunk, bufferizzala\n",
    "                        if trajectory_data.iloc[-1].name == df_chunk.iloc[-1].name:\n",
    "                            next_chunk_buffer[tid] = trajectory_data\n",
    "                            continue \n",
    "                            \n",
    "                        if len(trajectory_data) < window_size:\n",
    "                            continue \n",
    "                            \n",
    "                        trajectory_np = trajectory_data[features].to_numpy()\n",
    "                        new_windows = create_windows(trajectory_np, window_size)\n",
    "                        window_buffer.extend(new_windows)\n",
    "                        \n",
    "                        next_chunk_buffer[tid] = trajectory_data.iloc[-(window_size - 1):]\n",
    "\n",
    "                        while len(window_buffer) >= batch_size:\n",
    "                            batch_to_yield = window_buffer[:batch_size]\n",
    "                            window_buffer = window_buffer[batch_size:]\n",
    "                            yield (np.array(batch_to_yield), np.array(batch_to_yield))\n",
    "                    \n",
    "                    chunk_buffer = next_chunk_buffer\n",
    "                file_buffer = chunk_buffer\n",
    "            except Exception as e:\n",
    "                print(f\"\\nErrore lettura {file_path}: {e}\")\n",
    "                continue\n",
    "print(\"Funzioni definite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e638a97e",
   "metadata": {},
   "source": [
    "#### Scaler e Generatori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "205196ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inizializzazione generatori\n",
      "Generatori pronti.\n"
     ]
    }
   ],
   "source": [
    "print(\"Inizializzazione generatori\")\n",
    "scaler = joblib.load(SCALER_PATH)\n",
    "\n",
    "train_gen = data_generator(\n",
    "    file_paths=TRAIN_FILES,\n",
    "    scaler=scaler,\n",
    "    features=COLONNE_FEATURES,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle_files=False \n",
    ")\n",
    "\n",
    "val_gen = data_generator(\n",
    "    file_paths=VAL_FILES,\n",
    "    scaler=scaler,\n",
    "    features=COLONNE_FEATURES,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle_files=False\n",
    ")\n",
    "print(\"Generatori pronti.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab9bd18",
   "metadata": {},
   "source": [
    "#### Modello LNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13a65588",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1764548749.777508   60004 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4130 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 30, 4)]           0         \n",
      "                                                                 \n",
      " cf_c (CfC)                  (None, 64)                123844    \n",
      "                                                                 \n",
      " repeat_vector (RepeatVecto  (None, 30, 64)            0         \n",
      " r)                                                              \n",
      "                                                                 \n",
      " cf_c_1 (CfC)                (None, 30, 64)            154564    \n",
      "                                                                 \n",
      " time_distributed (TimeDist  (None, 30, 4)             260       \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 278668 (1.06 MB)\n",
      "Trainable params: 278668 (1.06 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_features = len(COLONNE_FEATURES)\n",
    "latent_dim = 128\n",
    "output_dim = 64\n",
    "wiring = AutoNCP(latent_dim,output_dim) # Definisce una wiring sparsa\n",
    "\n",
    "# Encoder\n",
    "inputs = Input(shape=(WINDOW_SIZE, n_features))\n",
    "# LAYER LIQUIDO 1 (Encoder): USIAMO WIRING SPARSA\n",
    "lnn_encoder = CfC(wiring, return_sequences=False, mixed_memory=True)(inputs) \n",
    "\n",
    "# Decoder\n",
    "repeat_vector = RepeatVector(WINDOW_SIZE)(lnn_encoder)\n",
    "lnn_decoder = CfC(wiring, return_sequences=True, mixed_memory=True)(repeat_vector)\n",
    "\n",
    "output = TimeDistributed(Dense(n_features))(lnn_decoder)\n",
    "\n",
    "model_lnn = Model(inputs, output) # Rinominato per evitare confusione\n",
    "\n",
    "model_lnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decce365",
   "metadata": {},
   "source": [
    "#### Addestramento LNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ca56dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1764548770.743326   60086 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - ETA: 0s - loss: 0.1455\n",
      "Epoch 1: val_loss improved from inf to 0.08676, saving model to lnn_autoencoder_best.weights.h5\n",
      "40000/40000 [==============================] - 4051s 101ms/step - loss: 0.1455 - val_loss: 0.0868\n",
      "Epoch 2/25\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.0572\n",
      "Epoch 2: val_loss improved from 0.08676 to 0.04938, saving model to lnn_autoencoder_best.weights.h5\n",
      "40000/40000 [==============================] - 4022s 101ms/step - loss: 0.0572 - val_loss: 0.0494\n",
      "Epoch 3/25\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.0342\n",
      "Epoch 3: val_loss improved from 0.04938 to 0.02213, saving model to lnn_autoencoder_best.weights.h5\n",
      "40000/40000 [==============================] - 4007s 100ms/step - loss: 0.0342 - val_loss: 0.0221\n",
      "Epoch 4/25\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.0232\n",
      "Epoch 4: val_loss improved from 0.02213 to 0.02159, saving model to lnn_autoencoder_best.weights.h5\n",
      "40000/40000 [==============================] - 3983s 100ms/step - loss: 0.0232 - val_loss: 0.0216\n",
      "Epoch 5/25\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.0186\n",
      "Epoch 5: val_loss improved from 0.02159 to 0.01976, saving model to lnn_autoencoder_best.weights.h5\n",
      "40000/40000 [==============================] - 3974s 99ms/step - loss: 0.0186 - val_loss: 0.0198\n",
      "Epoch 6/25\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.0317\n",
      "Epoch 6: val_loss did not improve from 0.01976\n",
      "40000/40000 [==============================] - 3957s 99ms/step - loss: 0.0317 - val_loss: 0.0303\n",
      "Epoch 7/25\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.0266\n",
      "Epoch 7: val_loss did not improve from 0.01976\n",
      "40000/40000 [==============================] - 3943s 99ms/step - loss: 0.0266 - val_loss: 0.0275\n",
      "Epoch 8/25\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.0229\n",
      "Epoch 8: val_loss did not improve from 0.01976\n",
      "40000/40000 [==============================] - 3927s 98ms/step - loss: 0.0229 - val_loss: 0.0245\n",
      "Epoch 9/25\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.0184\n",
      "Epoch 9: val_loss improved from 0.01976 to 0.01775, saving model to lnn_autoencoder_best.weights.h5\n",
      "40000/40000 [==============================] - 3922s 98ms/step - loss: 0.0184 - val_loss: 0.0177\n",
      "Epoch 10/25\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.0160\n",
      "Epoch 10: val_loss improved from 0.01775 to 0.01392, saving model to lnn_autoencoder_best.weights.h5\n",
      "40000/40000 [==============================] - 3921s 98ms/step - loss: 0.0160 - val_loss: 0.0139\n",
      "Epoch 11/25\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.0129\n",
      "Epoch 11: val_loss did not improve from 0.01392\n",
      "40000/40000 [==============================] - 3878s 97ms/step - loss: 0.0129 - val_loss: 0.0156\n",
      "Epoch 12/25\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.0124\n",
      "Epoch 12: val_loss improved from 0.01392 to 0.01105, saving model to lnn_autoencoder_best.weights.h5\n",
      "40000/40000 [==============================] - 3880s 97ms/step - loss: 0.0124 - val_loss: 0.0110\n",
      "Epoch 13/25\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.0103\n",
      "Epoch 13: val_loss improved from 0.01105 to 0.00759, saving model to lnn_autoencoder_best.weights.h5\n",
      "40000/40000 [==============================] - 3874s 97ms/step - loss: 0.0103 - val_loss: 0.0076\n",
      "Epoch 14/25\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.0090\n",
      "Epoch 14: val_loss improved from 0.00759 to 0.00730, saving model to lnn_autoencoder_best.weights.h5\n",
      "40000/40000 [==============================] - 3875s 97ms/step - loss: 0.0090 - val_loss: 0.0073\n",
      "Epoch 15/25\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.0085\n",
      "Epoch 15: val_loss did not improve from 0.00730\n",
      "40000/40000 [==============================] - 3892s 97ms/step - loss: 0.0085 - val_loss: 0.0087\n",
      "Epoch 16/25\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.0219\n",
      "Epoch 16: val_loss did not improve from 0.00730\n",
      "40000/40000 [==============================] - 3934s 98ms/step - loss: 0.0219 - val_loss: 0.0137\n",
      "Epoch 17/25\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.0173\n",
      "Epoch 17: val_loss did not improve from 0.00730\n",
      "40000/40000 [==============================] - 3909s 98ms/step - loss: 0.0173 - val_loss: 0.0299\n",
      "Epoch 18/25\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 1.7113\n",
      "Epoch 18: val_loss did not improve from 0.00730\n",
      "40000/40000 [==============================] - 4069s 102ms/step - loss: 1.7113 - val_loss: 1.7778\n",
      "Epoch 19/25\n",
      "  372/40000 [..............................] - ETA: 1:33:43 - loss: 1.8688\n",
      "Interrotto manualmente.\n"
     ]
    }
   ],
   "source": [
    "STEPS_PER_EPOCH_LNN = 40000 \n",
    "VALIDATION_STEPS_LNN = 8000 \n",
    "EPOCHS_LNN = 25\n",
    "\n",
    "initial_learning_rate = 0.0005  \n",
    "T_0 = 5\n",
    "lr_schedule = CosineDecayRestarts(\n",
    "    initial_learning_rate,\n",
    "    first_decay_steps=T_0 * STEPS_PER_EPOCH_LNN,\n",
    "    t_mul=2.0,                  \n",
    "    m_mul=0.9,                  \n",
    "    alpha=1e-6 # LR minimo\n",
    ")\n",
    "\n",
    "# OTTIMIZZATORE\n",
    "optimizer_lnn_final = Adam(\n",
    "    learning_rate=lr_schedule, \n",
    "    clipvalue=0.5             \n",
    ")\n",
    "\n",
    "model_lnn.compile(optimizer=optimizer_lnn_final, loss='mae')\n",
    "\n",
    "#CALLBACKS\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'lnn_autoencoder_best.weights.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,     \n",
    "    mode='min',\n",
    "    verbose=1,\n",
    "    save_weights_only=True # Salva solo i pesi numerici\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', patience=10, mode='min', verbose=1, restore_best_weights=True\n",
    ")\n",
    "\n",
    "csv_logger = tf.keras.callbacks.CSVLogger('training_log_lnn.csv', append=True)\n",
    "\n",
    "# START\n",
    "try:\n",
    "    history_lnn = model_lnn.fit(\n",
    "        train_gen,\n",
    "        steps_per_epoch=STEPS_PER_EPOCH_LNN,\n",
    "        epochs=EPOCHS_LNN,\n",
    "        validation_data=val_gen,\n",
    "        validation_steps=VALIDATION_STEPS_LNN,\n",
    "        callbacks=[checkpoint, early_stopping, csv_logger],\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"\\nAddestramento LNN Completato.\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nInterrotto manualmente.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
