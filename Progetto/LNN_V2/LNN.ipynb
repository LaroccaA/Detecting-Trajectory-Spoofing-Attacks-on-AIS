{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db3be638",
   "metadata": {},
   "source": [
    "## LNN Versione 2 Addestramento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc5080a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurazione Memoria OK: 1 GPU\n",
      "Configurazione LNN caricata.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"Configurazione Memoria OK: {len(gpus)} GPU\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Errore Configurazione Memoria: {e}\")\n",
    "\n",
    "from ncps.tf import CfC\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import gc\n",
    "import joblib\n",
    "import pyarrow.parquet as pq\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, TimeDistributed, RepeatVector\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts, ExponentialDecay\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "from ncps.wirings import AutoNCP\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# PATH\n",
    "INPUT_DIR = '../Pre-Elaborazione Dati/Dataset' \n",
    "SCALER_PATH = 'scaler.joblib' \n",
    "COLONNE_FEATURES = ['Latitude', 'Longitude', 'SOG', 'COG']\n",
    "\n",
    "WINDOW_SIZE = 30  \n",
    "BATCH_SIZE = 64 \n",
    "\n",
    "all_files = sorted(glob.glob(os.path.join(INPUT_DIR, '*.parquet')))\n",
    "TRAIN_FILES = all_files[0:16]\n",
    "VAL_FILES = all_files[16:20]\n",
    "\n",
    "print(\"Configurazione LNN caricata.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6e9543",
   "metadata": {},
   "source": [
    "#### Funzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2c48d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funzioni definite\n"
     ]
    }
   ],
   "source": [
    "def create_windows(data_np, window_size):\n",
    "    windows = []\n",
    "    for i in range(len(data_np) - window_size + 1):\n",
    "        windows.append(data_np[i : i + window_size])\n",
    "    return windows\n",
    "\n",
    "def data_generator(file_paths, scaler, features, window_size, batch_size, shuffle_files=False):\n",
    "    \n",
    "    file_buffer = {} \n",
    "    window_buffer = [] \n",
    "    CHUNK_SIZE_ROWS = 500_000\n",
    "\n",
    "    while True:\n",
    "        if shuffle_files:\n",
    "             # Shuffle disattivato forzatamente per garantire la sequenzialit√†\n",
    "            shuffle_files = False \n",
    "            \n",
    "        for file_path in file_paths:\n",
    "            chunk_buffer = {}\n",
    "            try:\n",
    "                pf = pq.ParquetFile(file_path)\n",
    "                for batch in pf.iter_batches(batch_size=CHUNK_SIZE_ROWS, columns=features + ['TrajectoryID']):\n",
    "                    df_chunk = batch.to_pandas()\n",
    "                    df_chunk[features] = scaler.transform(df_chunk[features])\n",
    "                    next_chunk_buffer = {}\n",
    "                    \n",
    "                    for tid, group in df_chunk.groupby('TrajectoryID'):\n",
    "                        if tid in chunk_buffer:\n",
    "                            trajectory_data = pd.concat([chunk_buffer.pop(tid), group])\n",
    "                        else:\n",
    "                            trajectory_data = group\n",
    "                        \n",
    "                        if tid in file_buffer:\n",
    "                            trajectory_data = pd.concat([file_buffer.pop(tid), trajectory_data])\n",
    "                        \n",
    "                        # Se la traiettoria tocca la fine del chunk, bufferizzala\n",
    "                        if trajectory_data.iloc[-1].name == df_chunk.iloc[-1].name:\n",
    "                            next_chunk_buffer[tid] = trajectory_data\n",
    "                            continue \n",
    "                            \n",
    "                        if len(trajectory_data) < window_size:\n",
    "                            continue \n",
    "                            \n",
    "                        trajectory_np = trajectory_data[features].to_numpy()\n",
    "                        new_windows = create_windows(trajectory_np, window_size)\n",
    "                        window_buffer.extend(new_windows)\n",
    "                        \n",
    "                        next_chunk_buffer[tid] = trajectory_data.iloc[-(window_size - 1):]\n",
    "\n",
    "                        while len(window_buffer) >= batch_size:\n",
    "                            batch_to_yield = window_buffer[:batch_size]\n",
    "                            window_buffer = window_buffer[batch_size:]\n",
    "                            yield (np.array(batch_to_yield), np.array(batch_to_yield))\n",
    "                    \n",
    "                    chunk_buffer = next_chunk_buffer\n",
    "                file_buffer = chunk_buffer\n",
    "            except Exception as e:\n",
    "                print(f\"\\nErrore lettura {file_path}: {e}\")\n",
    "                continue\n",
    "print(\"Funzioni definite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e638a97e",
   "metadata": {},
   "source": [
    "#### Scaler e Generatori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "205196ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inizializzazione generatori\n",
      "Generatori pronti.\n"
     ]
    }
   ],
   "source": [
    "print(\"Inizializzazione generatori\")\n",
    "scaler = joblib.load(SCALER_PATH)\n",
    "\n",
    "train_gen = data_generator(\n",
    "    file_paths=TRAIN_FILES,\n",
    "    scaler=scaler,\n",
    "    features=COLONNE_FEATURES,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle_files=False \n",
    ")\n",
    "\n",
    "val_gen = data_generator(\n",
    "    file_paths=VAL_FILES,\n",
    "    scaler=scaler,\n",
    "    features=COLONNE_FEATURES,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle_files=False\n",
    ")\n",
    "print(\"Generatori pronti.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab9bd18",
   "metadata": {},
   "source": [
    "#### Modello LNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13a65588",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1764265362.146125  374329 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3945 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 30, 4)]           0         \n",
      "                                                                 \n",
      " cf_c (CfC)                  (None, 64)                123844    \n",
      "                                                                 \n",
      " repeat_vector (RepeatVecto  (None, 30, 64)            0         \n",
      " r)                                                              \n",
      "                                                                 \n",
      " cf_c_1 (CfC)                (None, 30, 64)            154564    \n",
      "                                                                 \n",
      " time_distributed (TimeDist  (None, 30, 4)             260       \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 278668 (1.06 MB)\n",
      "Trainable params: 278668 (1.06 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_features = len(COLONNE_FEATURES)\n",
    "latent_dim = 128\n",
    "output_dim = 64\n",
    "wiring = AutoNCP(latent_dim,output_dim) # Definisce una wiring sparsa\n",
    "\n",
    "# Encoder\n",
    "inputs = Input(shape=(WINDOW_SIZE, n_features))\n",
    "# LAYER LIQUIDO 1 (Encoder): USIAMO WIRING SPARSA\n",
    "lnn_encoder = CfC(wiring, return_sequences=False, mixed_memory=True)(inputs) \n",
    "\n",
    "# Decoder\n",
    "repeat_vector = RepeatVector(WINDOW_SIZE)(lnn_encoder)\n",
    "lnn_decoder = CfC(wiring, return_sequences=True, mixed_memory=True)(repeat_vector)\n",
    "\n",
    "output = TimeDistributed(Dense(n_features))(lnn_decoder)\n",
    "\n",
    "model_lnn = Model(inputs, output) # Rinominato per evitare confusione\n",
    "\n",
    "model_lnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decce365",
   "metadata": {},
   "source": [
    "#### Addestramento LNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca56dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALER_PATH = 'scaler.joblib' \n",
    "\n",
    "try:\n",
    "    if os.path.exists(SCALER_PATH):\n",
    "        print(f\"Caricamento parametri fisici da {SCALER_PATH}...\")\n",
    "        scaler = joblib.load(SCALER_PATH)\n",
    "        \n",
    "        MEAN_LAT, STD_LAT = scaler.mean_[0], scaler.scale_[0]\n",
    "        MEAN_LON, STD_LON = scaler.mean_[1], scaler.scale_[1]\n",
    "        MEAN_SOG, STD_SOG = scaler.mean_[2], scaler.scale_[2]\n",
    "        MEAN_COG, STD_COG = scaler.mean_[3], scaler.scale_[3]\n",
    "        \n",
    "        print(\"Scaler letto correttamente per la LNN.\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"File scaler non trovato.\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore lettura Scaler: {e}. Uso default.\")\n",
    "    MEAN_LAT, STD_LAT = 44.0, 1.0\n",
    "    MEAN_LON, STD_LON = 9.0, 1.0\n",
    "    MEAN_SOG, STD_SOG = 10.0, 5.0\n",
    "    MEAN_COG, STD_COG = 180.0, 90.0\n",
    "\n",
    "\n",
    "# DEFINIZIONE LOSS IBRIDA\n",
    "\n",
    "def physics_informed_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    # Dati\n",
    "    lat_true, lon_true = y_true[:, :, 0], y_true[:, :, 1]\n",
    "    sog_true, cog_true = y_true[:, :, 2], y_true[:, :, 3]\n",
    "    lat_pred, lon_pred = y_pred[:, :, 0], y_pred[:, :, 1]\n",
    "    sog_pred, cog_pred = y_pred[:, :, 2], y_pred[:, :, 3]\n",
    "    \n",
    "    # MSE Ancoraggio (GPS) - Peso x2 sulla posizione\n",
    "    mse_pos = tf.reduce_mean(tf.square(lat_true - lat_pred) + tf.square(lon_true - lon_pred))\n",
    "    mse_dyn = tf.reduce_mean(tf.square(sog_true - sog_pred) + tf.square(cog_true - cog_pred))\n",
    "    data_loss = 2.0 * mse_pos + 1.0 * mse_dyn\n",
    "\n",
    "    # Fisica (Dead Reckoning)\n",
    "    d_lat_net = (lat_pred[:, 1:] - lat_pred[:, :-1]) * STD_LAT\n",
    "    d_lon_net = (lon_pred[:, 1:] - lon_pred[:, :-1]) * STD_LON\n",
    "    \n",
    "    pred_sog_real = (sog_pred[:, :-1] * STD_SOG) + MEAN_SOG\n",
    "    pred_cog_deg = (cog_pred[:, :-1] * STD_COG) + MEAN_COG\n",
    "    pred_cog_rad = pred_cog_deg * 0.0174533\n",
    "    \n",
    "    K = 0.00032410 \n",
    "    \n",
    "    d_lat_phys = (pred_sog_real * K) * tf.cos(pred_cog_rad)\n",
    "    d_lon_phys = (pred_sog_real * K) * tf.sin(pred_cog_rad)\n",
    "    \n",
    "    physics_error = tf.reduce_mean(tf.square(d_lat_net - d_lat_phys) + tf.square(d_lon_net - d_lon_phys))\n",
    "\n",
    "    return data_loss + 0.5 * physics_error\n",
    "\n",
    "\n",
    "# CONFIGURAZIONE TRAINING LNN\n",
    "\n",
    "STEPS_PER_EPOCH_LNN = 40000 \n",
    "VALIDATION_STEPS_LNN = 8000 \n",
    "EPOCHS_LNN = 40 \n",
    "\n",
    "# Learning Rate Schedule (Cosine Decay)\n",
    "initial_learning_rate = 0.0005  \n",
    "lr_schedule = CosineDecayRestarts(\n",
    "    initial_learning_rate,\n",
    "    first_decay_steps=5 * STEPS_PER_EPOCH_LNN,\n",
    "    t_mul=2.0,                  \n",
    "    m_mul=0.9,                  \n",
    "    alpha=1e-6\n",
    ")\n",
    "\n",
    "optimizer_lnn_final = Adam(learning_rate=lr_schedule, clipvalue=0.5)\n",
    "\n",
    "print(\"Compilazione LNN con Hybrid Physics Loss\")\n",
    "model_lnn.compile(optimizer=optimizer_lnn_final, loss=physics_informed_loss)\n",
    "\n",
    "# CALLBACKS\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'lnn_autoencoder_best.weights.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,     \n",
    "    mode='min',\n",
    "    verbose=1,\n",
    "    save_weights_only=True\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', patience=7, mode='min', verbose=1, restore_best_weights=True\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger('training_log_lnn.csv', append=True)\n",
    "\n",
    "# START TRAINING\n",
    "try:\n",
    "    print(\"Avvio Addestramento LNN\")\n",
    "    history_lnn = model_lnn.fit(\n",
    "        train_gen,\n",
    "        steps_per_epoch=STEPS_PER_EPOCH_LNN,\n",
    "        epochs=EPOCHS_LNN,\n",
    "        validation_data=val_gen,\n",
    "        validation_steps=VALIDATION_STEPS_LNN,\n",
    "        callbacks=[checkpoint, early_stopping, csv_logger],\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"\\nAddestramento LNN Completato.\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nInterrotto manualmente.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
