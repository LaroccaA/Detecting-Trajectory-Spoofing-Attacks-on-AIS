{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "200e8a47",
   "metadata": {},
   "source": [
    "# Modello LSMT (Long Short-Term Memory) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4d2742",
   "metadata": {},
   "source": [
    "Primo step split del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5896be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "import joblib\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, TimeDistributed, RepeatVector\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "INPUT_DIR = '../Pre-Elaborazione Dati/Dataset' \n",
    "SCALER_PATH = 'scaler.joblib' \n",
    "COLONNE_FEATURES = ['Latitude', 'Longitude', 'SOG', 'COG']\n",
    "\n",
    "WINDOW_SIZE = 30  \n",
    "BATCH_SIZE = 64   \n",
    "\n",
    "all_files = sorted(glob.glob(os.path.join(INPUT_DIR, '*.parquet')))\n",
    "TRAIN_FILES = all_files[0:16]\n",
    "VAL_FILES = all_files[16:20]\n",
    "TEST_FILES = all_files[20:24] \n",
    "\n",
    "print(\"Configurazione caricata.\")\n",
    "print(f\"Window Size: {WINDOW_SIZE}, Batch Size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80885758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:46: SyntaxWarning: invalid escape sequence '\\V'\n",
      "<>:46: SyntaxWarning: invalid escape sequence '\\V'\n",
      "/tmp/ipykernel_19344/47474260.py:46: SyntaxWarning: invalid escape sequence '\\V'\n",
      "  print(f\"\\VERIFICA CONTEGGI:\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trovati 25 file .parquet in '../Pre-Elaborazione Dati/Dataset'\n",
      "\n",
      "FASE 1: Avvio scansione unificata (UN FILE ALLA VOLTA)...\n",
      "  Processando blocco_000-segmentato.parquet...\n",
      "  Processando blocco_001-segmentato.parquet...\n",
      "  Processando blocco_002-segmentato.parquet...\n",
      "  Processando blocco_003-segmentato.parquet...\n",
      "  Processando blocco_004-segmentato.parquet...\n",
      "  Processando blocco_005-segmentato.parquet...\n",
      "  Processando blocco_006-segmentato.parquet...\n",
      "  Processando blocco_007-segmentato.parquet...\n",
      "  Processando blocco_008-segmentato.parquet...\n",
      "  Processando blocco_009-segmentato.parquet...\n",
      "  Processando blocco_010-segmentato.parquet...\n",
      "  Processando blocco_011-segmentato.parquet...\n",
      "  Processando blocco_012-segmentato.parquet...\n",
      "  Processando blocco_013-segmentato.parquet...\n",
      "  Processando blocco_014-segmentato.parquet...\n",
      "  Processando blocco_015-segmentato.parquet...\n",
      "\n",
      "------------------------------------------------------------\n",
      "FASE 1: Scansione completata.\n",
      "FASE 2: Verifica e Analisi.\n",
      "\\VERIFICA CONTEGGI:\n",
      "ID Unici (metodo 'set'):     4,369,967\n",
      "ID Unici (metodo 'Counter'): 4,369,967\n",
      "\n",
      "SUCCESSO! I conteggi corrispondono.\n",
      "\n",
      "Risultati dell'analisi (su 4,369,967 traiettorie totali di TRAINING):\n",
      "---------------------------------------------------------------------------\n",
      "Window Size     | Traiettorie Usabili  | Traiettorie Scartate | % Usabili \n",
      "---------------------------------------------------------------------------\n",
      "10              | 2957346              | 1412621              | 67.67     %\n",
      "15              | 2678465              | 1691502              | 61.29     %\n",
      "20              | 2461683              | 1908284              | 56.33     %\n",
      "25              | 2283891              | 2086076              | 52.26     %\n",
      "30              | 2131706              | 2238261              | 48.78     %\n",
      "40              | 1882078              | 2487889              | 43.07     %\n",
      "---------------------------------------------------------------------------\n",
      "Analisi completata.\n"
     ]
    }
   ],
   "source": [
    "all_files = sorted(glob.glob(os.path.join(INPUT_DIR, '*.parquet')))\n",
    "\n",
    "if len(all_files) == 0:\n",
    "    print(f\"ERRORE CRITICO: Nessun file .parquet trovato in '{INPUT_DIR}'\")\n",
    "else:\n",
    "    print(f\"Trovati {len(all_files)} file .parquet in '{INPUT_DIR}'\")\n",
    "\n",
    "TRAIN_FILES = all_files[0:16]\n",
    "IPERPARAMETRI_TIMESTEP = [10, 15, 20, 25, 30, 40]\n",
    "\n",
    "print(f\"\\nFASE 1: Avvio scansione unificata (UN FILE ALLA VOLTA)...\")\n",
    "\n",
    "total_counts_counter = Counter()\n",
    "total_ids_set = set()\n",
    "\n",
    "for file_path in TRAIN_FILES:\n",
    "    print(f\"  Processando {os.path.basename(file_path)}...\")\n",
    "    try:\n",
    "        df = pd.read_parquet(file_path, columns=['TrajectoryID'])\n",
    "        \n",
    "        total_ids_set.update(df['TrajectoryID'].unique())\n",
    "        \n",
    "        total_counts_counter.update(df['TrajectoryID'])\n",
    "        \n",
    "        del df\n",
    "        gc.collect()\n",
    "    except Exception as e:\n",
    "        print(f\"    ERRORE nel leggere {file_path}: {e}\")\n",
    "        \n",
    "\n",
    "print(\"FASE 2: Verifica e Analisi.\")\n",
    "\n",
    "conteggio_set = len(total_ids_set)\n",
    "conteggio_counter = len(total_counts_counter)\n",
    "\n",
    "print(f\"\\VERIFICA CONTEGGI:\")\n",
    "print(f\"ID Unici (metodo 'set'):     {conteggio_set:,}\")\n",
    "print(f\"ID Unici (metodo 'Counter'): {conteggio_counter:,}\")\n",
    "\n",
    "if conteggio_set != conteggio_counter:\n",
    "    print(\"ERRORE LOGICO: I conteggi non corrispondono. C'è ancora un problema.\")\n",
    "else:\n",
    "    print(\"\\nSUCCESSO! I conteggi corrispondono.\")\n",
    "    \n",
    "    all_lengths = list(total_counts_counter.values())\n",
    "    totale_traiettorie_analisi = conteggio_counter\n",
    "\n",
    "    print(f\"\\nRisultati dell'analisi (su {totale_traiettorie_analisi:,} traiettorie totali di TRAINING):\")\n",
    "    print(\"-\" * 75)\n",
    "    print(f\"{'Window Size':<15} | {'Traiettorie Usabili':<20} | {'Traiettorie Scartate':<20} | {'% Usabili':<10}\")\n",
    "    print(\"-\" * 75)\n",
    "\n",
    "    for timestep in IPERPARAMETRI_TIMESTEP:\n",
    "        traiettorie_usabili = sum(1 for length in all_lengths if length >= timestep)\n",
    "        traiettorie_scartate = totale_traiettorie_analisi - traiettorie_usabili\n",
    "        percentuale_usabili = (traiettorie_usabili / totale_traiettorie_analisi) * 100\n",
    "        print(f\"{timestep:<15} | {traiettorie_usabili:<20} | {traiettorie_scartate:<20} | {percentuale_usabili:<10.2f}%\")\n",
    "\n",
    "    print(\"-\" * 75)\n",
    "    print(\"Analisi completata.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cb7939",
   "metadata": {},
   "source": [
    "### Normalizzazione\n",
    "\n",
    "Passaggio fondamentale perchè le tue reti neurali (LSTM/LNN) faticano a imparare quando i dati di input hanno scale completamente diverse. Ad esempio, la colonna COG (rotta) va da 0 a 360, mentre la colonna SOG (velocità) potrebbe andare da 0 a 50. Questi intervalli così diversi \"confondono\" la rete durante l'addestramento.\n",
    "Dobbiamo quindi standardizzarli, portando tutte le colonne a una scala simile.  \n",
    "Il modo corretto per standardizzare è calcolare la media e la deviazione standard dell'intero set di addestramento (tutti i 4.3 milioni di traiettorie nei tuoi 16 file). Ovviamente, questi dati sono troppo grandi per essere caricati tutti insieme in memoria.\n",
    "\n",
    "Quello che facciamo è:  \n",
    "- Inizializziamo uno scaler vuoto e iteriamo sui 16 file di Train.\n",
    "- `partial_fit` è la parte fondamentale perchè per ogni file che carica. Questa funzione dice allo scaler: \"Prendi la media e la deviazione che hai calcolato finora e aggiornale includendo questo nuovo blocco di dati\".\n",
    "- `joblib.dump(scaler, SCALER_PATH)` salva questi parametri calcolati in un file esterno.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28d81766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando file 1/16: blocco_000-segmentato.parquet\n",
      "Processando file 2/16: blocco_001-segmentato.parquet\n",
      "Processando file 3/16: blocco_002-segmentato.parquet\n",
      "Processando file 4/16: blocco_003-segmentato.parquet\n",
      "Processando file 5/16: blocco_004-segmentato.parquet\n",
      "Processando file 6/16: blocco_005-segmentato.parquet\n",
      "Processando file 7/16: blocco_006-segmentato.parquet\n",
      "Processando file 8/16: blocco_007-segmentato.parquet\n",
      "Processando file 9/16: blocco_008-segmentato.parquet\n",
      "Processando file 10/16: blocco_009-segmentato.parquet\n",
      "Processando file 11/16: blocco_010-segmentato.parquet\n",
      "Processando file 12/16: blocco_011-segmentato.parquet\n",
      "Processando file 13/16: blocco_012-segmentato.parquet\n",
      "Processando file 14/16: blocco_013-segmentato.parquet\n",
      "Processando file 15/16: blocco_014-segmentato.parquet\n",
      "Processando file 16/16: blocco_015-segmentato.parquet\n",
      "\n",
      "Adattamento completato su tutti i 16 file di training.\n",
      "\n",
      "Scaler salvato come 'scaler.joblib'.\n"
     ]
    }
   ],
   "source": [
    "COLONNE_DA_NORMALIZZARE = ['Latitude', 'Longitude', 'SOG', 'COG']\n",
    "SCALER_PATH = 'scaler.joblib'\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "try:\n",
    "    \n",
    "    for i, file_path in enumerate(TRAIN_FILES):\n",
    "        \n",
    "        print(f\"Processando file {i+1}/{len(TRAIN_FILES)}: {os.path.basename(file_path)}\")\n",
    "        \n",
    "        df_chunk = pd.read_parquet(file_path, columns=COLONNE_DA_NORMALIZZARE)\n",
    "        \n",
    "        scaler.partial_fit(df_chunk)\n",
    "        \n",
    "        del df_chunk\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"\\nAdattamento completato su tutti i 16 file di training.\")\n",
    "    \n",
    "    joblib.dump(scaler, SCALER_PATH) #Salva file\n",
    "\n",
    "    print(f\"\\nScaler salvato come '{SCALER_PATH}'.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n Errore durante la preparazione dello scaler: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2fb059",
   "metadata": {},
   "source": [
    "#### Funzioni del Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20ffe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_steps_per_epoch(file_paths, window_size, batch_size):\n",
    "    \"\"\"\n",
    "    Scansiona tutti i file in modo efficiente (RAM-safe) per calcolare\n",
    "    il numero totale di finestre (campioni) che verranno generate,\n",
    "    e da lì calcola il numero di \"steps\" (batch) per epoca.\n",
    "    Usa il metodo Counter.update(Series) che sappiamo funzionare.\n",
    "    \"\"\"\n",
    "    print(f\"--- Calcolo Steps per {len(file_paths)} file ---\")\n",
    "    \n",
    "    total_lengths = Counter()\n",
    "    \n",
    "    # FASE 1: Trova le lunghezze di tutte le 4.3M traiettorie\n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        print(f\"  Scansione lunghezze file {i+1}/{len(file_paths)}...\", end='\\r')\n",
    "        try:\n",
    "            df = pd.read_parquet(file_path, columns=['TrajectoryID'])\n",
    "            # Metodo efficiente (provato) per aggiornare le lunghezze\n",
    "            total_lengths.update(df['TrajectoryID'])\n",
    "            del df\n",
    "            gc.collect()\n",
    "        except Exception as e:\n",
    "            print(f\"Errore nel leggere {file_path}: {e}\")\n",
    "\n",
    "    print(\"\\nScansione lunghezze completata.\")\n",
    "\n",
    "    # FASE 2: Calcola il numero totale di finestre\n",
    "    total_windows = 0\n",
    "    for length in total_lengths.values():\n",
    "        if length >= window_size:\n",
    "            # Una traiettoria di N righe produce (N - window_size + 1) finestre\n",
    "            total_windows += (length - window_size + 1)\n",
    "            \n",
    "    # FASE 3: Calcola gli steps\n",
    "    steps = int(np.ceil(total_windows / batch_size))\n",
    "    \n",
    "    print(f\"Trovate {total_windows:,} finestre totali.\")\n",
    "    print(f\"Steps per Epoca (Batch Size {batch_size}): {steps}\")\n",
    "    print(\"------------------------------------------\")\n",
    "    return steps\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_windows(data_np, window_size):\n",
    "    \"\"\"\n",
    "    Crea finestre mobili da un array numpy 2D.\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "    # Itera da 0 fino all'ultimo punto di inizio possibile\n",
    "    for i in range(len(data_np) - window_size + 1):\n",
    "        windows.append(data_np[i : i + window_size])\n",
    "    return windows\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def data_generator(file_paths, scaler, features, window_size, batch_size, shuffle_files=True):\n",
    "    \"\"\"\n",
    "    Generatore (yield) che carica i file uno per uno, gestisce la \n",
    "    RAM e \"cuce\" le traiettorie che si trovano a cavallo dei file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Questo buffer tiene in memoria le \"code\" delle traiettorie\n",
    "    # alla fine di un file, per cucirle con l'inizio del file successivo.\n",
    "    # Formato: { 'TrajectoryID': DataFrame_coda }\n",
    "    trajectory_buffer = {}\n",
    "    \n",
    "    # Questo buffer tiene le finestre pronte per essere messe in un batch\n",
    "    window_buffer = []\n",
    "    \n",
    "    # Loop infinito: Keras chiamerà questo generatore più volte\n",
    "    while True:\n",
    "        \n",
    "        # Opzionale: mescola l'ordine dei file ad ogni epoca\n",
    "        if shuffle_files:\n",
    "            np.random.shuffle(file_paths)\n",
    "            \n",
    "        # Itera sui file di training (o validazione)\n",
    "        for file_path in file_paths:\n",
    "            try:\n",
    "                # Carica UN file in memoria\n",
    "                df = pd.read_parquet(file_path)\n",
    "                \n",
    "                # Normalizza i dati USANDO LO SCALER CARICATO\n",
    "                df[features] = scaler.transform(df[features])\n",
    "                \n",
    "                # Questo buffer terrà le code del file *corrente*\n",
    "                # da passare al file successivo\n",
    "                next_file_buffer = {}\n",
    "\n",
    "                # Itera sui gruppi (questo è RAM-efficiente)\n",
    "                # Grazie all'ordinamento, i gruppi sono contigui\n",
    "                for tid, group in df.groupby('TrajectoryID'):\n",
    "                    \n",
    "                    # --- 1. CUCITURA ---\n",
    "                    # Controlla se questo ID era già nel buffer (dal file PRECEDENTE)\n",
    "                    if tid in trajectory_buffer:\n",
    "                        # Cucitura!\n",
    "                        trajectory_data = pd.concat([trajectory_buffer.pop(tid), group])\n",
    "                    else:\n",
    "                        # Nessuna cucitura, è una nuova traiettoria\n",
    "                        trajectory_data = group\n",
    "                        \n",
    "                    # --- 2. FILTRO LUNGHEZZA ---\n",
    "                    if len(trajectory_data) < window_size:\n",
    "                        # Troppo corta PER ORA. Mettila da parte\n",
    "                        next_file_buffer[tid] = trajectory_data\n",
    "                        continue # Passa al prossimo TrajectoryID\n",
    "                        \n",
    "                    # --- 3. CREAZIONE FINESTRE ---\n",
    "                    # Converti in NumPy per velocità\n",
    "                    trajectory_np = trajectory_data[features].to_numpy()\n",
    "                    \n",
    "                    # Crea tutte le finestre possibili da questa traiettoria\n",
    "                    new_windows = create_windows(trajectory_np, window_size)\n",
    "                    \n",
    "                    # Aggiungi le finestre al buffer dei batch\n",
    "                    window_buffer.extend(new_windows)\n",
    "                    \n",
    "                    # --- 4. SALVATAGGIO CODA PER PROSSIMO FILE ---\n",
    "                    # Salva le ultime (N-1) righe. Saranno la base\n",
    "                    # per la cucitura nel *prossimo* file.\n",
    "                    next_file_buffer[tid] = trajectory_data.iloc[-(window_size - 1):]\n",
    "\n",
    "                    # --- 5. PRODUZIONE (YIELD) BATCH ---\n",
    "                    # Abbiamo abbastanza finestre per un batch?\n",
    "                    while len(window_buffer) >= batch_size:\n",
    "                        \n",
    "                        # Estrai il batch\n",
    "                        batch_to_yield = window_buffer[:batch_size]\n",
    "                        # Rimuovilo dal buffer\n",
    "                        window_buffer = window_buffer[batch_size:]\n",
    "                        \n",
    "                        # Converti in array numpy finale\n",
    "                        batch_np = np.array(batch_to_yield)\n",
    "                        \n",
    "                        # L'output (y) è uguale all'input (X)\n",
    "                        yield (batch_np, batch_np)\n",
    "                \n",
    "                # Finito il file, aggiorna il buffer principale\n",
    "                # per il *prossimo* file\n",
    "                trajectory_buffer = next_file_buffer\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nERRORE nel generatore durante la lettura di {file_path}: {e}\")\n",
    "                # Continua con il file successivo\n",
    "                continue\n",
    "\n",
    "print(\"Funzioni definite: calculate_steps_per_epoch, create_windows, data_generator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa0ab7a",
   "metadata": {},
   "source": [
    "#### Calcolo degli steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac41e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Avvio calcolo steps per il Training Set...\")\n",
    "train_steps = calculate_steps_per_epoch(TRAIN_FILES, WINDOW_SIZE, BATCH_SIZE)\n",
    "\n",
    "print(\"\\nAvvio calcolo steps per il Validation Set...\")\n",
    "val_steps = calculate_steps_per_epoch(VAL_FILES, WINDOW_SIZE, BATCH_SIZE)\n",
    "\n",
    "print(\"\\n--- Calcolo Steps Completato ---\")\n",
    "print(f\"Training Steps per Epoca: {train_steps}\")\n",
    "print(f\"Validation Steps per Epoca: {val_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b98a059",
   "metadata": {},
   "source": [
    "#### Caricamento Scaler e creazione dei Generatori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b71cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- B. Carica lo Scaler ---\n",
    "print(f\"Caricamento scaler da {SCALER_PATH}...\")\n",
    "scaler = joblib.load(SCALER_PATH)\n",
    "\n",
    "# --- C. Inizializza i Generatori ---\n",
    "print(\"Inizializzazione dei generatori (oggetti pronti)...\")\n",
    "\n",
    "train_gen = data_generator(\n",
    "    file_paths=TRAIN_FILES,\n",
    "    scaler=scaler,\n",
    "    features=COLONNE_FEATURES,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle_files=True\n",
    ")\n",
    "\n",
    "val_gen = data_generator(\n",
    "    file_paths=VAL_FILES,\n",
    "    scaler=scaler,\n",
    "    features=COLONNE_FEATURES,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle_files=False\n",
    ")\n",
    "\n",
    "print(\"Generatori pronti.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3083506e",
   "metadata": {},
   "source": [
    "#### Definizione Modello LSMT Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb9e219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- D. Definisci il Modello (LSTM Autoencoder) ---\n",
    "\n",
    "n_features = len(COLONNE_FEATURES)\n",
    "latent_dim = 32 # Prova a cambiare questo iperparametro (es. 64, 128)\n",
    "\n",
    "# Encoder\n",
    "inputs = Input(shape=(WINDOW_SIZE, n_features))\n",
    "# Il primo LSTM comprime l'input\n",
    "lstm_encoder = LSTM(latent_dim, return_sequences=False)(inputs)\n",
    "\n",
    "# Decoder\n",
    "# Ripete il vettore compresso per ogni timestep\n",
    "repeat_vector = RepeatVector(WINDOW_SIZE)(lstm_encoder)\n",
    "# Il secondo LSTM \"legge\" il vettore compresso e ricostruisce la sequenza\n",
    "lstm_decoder = LSTM(latent_dim, return_sequences=True)(repeat_vector)\n",
    "# Un layer finale per rimappare all'output (n_features)\n",
    "output = TimeDistributed(Dense(n_features))(lstm_decoder)\n",
    "\n",
    "model_lstm = Model(inputs, output)\n",
    "model_lstm.compile(optimizer='adam', loss='mae') # MAE è ottimo per questo\n",
    "\n",
    "print(\"Modello LSTM-Autoencoder creato e compilato.\")\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112f9710",
   "metadata": {},
   "source": [
    "#### Addestramento LSMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c05ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- E. Avvia l'Addestramento ---\n",
    "\n",
    "# Definisci i Callback\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'lstm_autoencoder_best.h5', # Salva il modello migliore\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5, # Ferma se non migliora per 5 epoche\n",
    "    mode='min',\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"\\nAvvio addestramento LSTM-Autoencoder...\")\n",
    "\n",
    "history_lstm = model_lstm.fit(\n",
    "    train_gen,\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=50, # Metti un numero alto, EarlyStopping lo fermerà\n",
    "    validation_data=val_gen,\n",
    "    validation_steps=val_steps,\n",
    "    callbacks=[checkpoint, early_stopping]\n",
    ")\n",
    "\n",
    "print(\"\\nAddestramento LSTM completato.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
