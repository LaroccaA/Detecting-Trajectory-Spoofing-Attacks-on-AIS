{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "200e8a47",
   "metadata": {},
   "source": [
    "# Modello LSMT (Long Short-Term Memory) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4d2742",
   "metadata": {},
   "source": [
    "Primo step split del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a5896be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ad8acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File di Training: 16\n",
      "File di Validazione: 4\n",
      "File di Test: 4\n",
      "\n",
      "Primo file di Train: blocco_000-segmentato.parquet\n",
      "Primo file di Val: blocco_016-segmentato.parquet\n",
      "Primo file di Test: blocco_020-segmentato.parquet\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIR = '../Pre-Elaborazione Dati/Dataset'\n",
    "all_files = sorted(glob.glob(os.path.join(INPUT_DIR, '*.parquet')))\n",
    "\n",
    "TRAIN_FILES = all_files[0:16]\n",
    "VAL_FILES = all_files[16:20]\n",
    "TEST_FILES = all_files[20:24]\n",
    "print(f\"File di Training: {len(TRAIN_FILES)}\")\n",
    "print(f\"File di Validazione: {len(VAL_FILES)}\")\n",
    "print(f\"File di Test: {len(TEST_FILES)}\")\n",
    "\n",
    "print(f\"\\nPrimo file di Train: {os.path.basename(TRAIN_FILES[0])}\")\n",
    "print(f\"Primo file di Val: {os.path.basename(VAL_FILES[0])}\")\n",
    "print(f\"Primo file di Test: {os.path.basename(TEST_FILES[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80885758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:46: SyntaxWarning: invalid escape sequence '\\V'\n",
      "<>:46: SyntaxWarning: invalid escape sequence '\\V'\n",
      "/tmp/ipykernel_19344/47474260.py:46: SyntaxWarning: invalid escape sequence '\\V'\n",
      "  print(f\"\\VERIFICA CONTEGGI:\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trovati 25 file .parquet in '../Pre-Elaborazione Dati/Dataset'\n",
      "\n",
      "FASE 1: Avvio scansione unificata (UN FILE ALLA VOLTA)...\n",
      "  Processando blocco_000-segmentato.parquet...\n",
      "  Processando blocco_001-segmentato.parquet...\n",
      "  Processando blocco_002-segmentato.parquet...\n",
      "  Processando blocco_003-segmentato.parquet...\n",
      "  Processando blocco_004-segmentato.parquet...\n",
      "  Processando blocco_005-segmentato.parquet...\n",
      "  Processando blocco_006-segmentato.parquet...\n",
      "  Processando blocco_007-segmentato.parquet...\n",
      "  Processando blocco_008-segmentato.parquet...\n",
      "  Processando blocco_009-segmentato.parquet...\n",
      "  Processando blocco_010-segmentato.parquet...\n",
      "  Processando blocco_011-segmentato.parquet...\n",
      "  Processando blocco_012-segmentato.parquet...\n",
      "  Processando blocco_013-segmentato.parquet...\n",
      "  Processando blocco_014-segmentato.parquet...\n",
      "  Processando blocco_015-segmentato.parquet...\n",
      "\n",
      "------------------------------------------------------------\n",
      "FASE 1: Scansione completata.\n",
      "FASE 2: Verifica e Analisi.\n",
      "\\VERIFICA CONTEGGI:\n",
      "ID Unici (metodo 'set'):     4,369,967\n",
      "ID Unici (metodo 'Counter'): 4,369,967\n",
      "\n",
      "SUCCESSO! I conteggi corrispondono.\n",
      "\n",
      "Risultati dell'analisi (su 4,369,967 traiettorie totali di TRAINING):\n",
      "---------------------------------------------------------------------------\n",
      "Window Size     | Traiettorie Usabili  | Traiettorie Scartate | % Usabili \n",
      "---------------------------------------------------------------------------\n",
      "10              | 2957346              | 1412621              | 67.67     %\n",
      "15              | 2678465              | 1691502              | 61.29     %\n",
      "20              | 2461683              | 1908284              | 56.33     %\n",
      "25              | 2283891              | 2086076              | 52.26     %\n",
      "30              | 2131706              | 2238261              | 48.78     %\n",
      "40              | 1882078              | 2487889              | 43.07     %\n",
      "---------------------------------------------------------------------------\n",
      "Analisi completata.\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIR = '../Pre-Elaborazione Dati/Dataset' \n",
    "\n",
    "all_files = sorted(glob.glob(os.path.join(INPUT_DIR, '*.parquet')))\n",
    "\n",
    "if len(all_files) == 0:\n",
    "    print(f\"ERRORE CRITICO: Nessun file .parquet trovato in '{INPUT_DIR}'\")\n",
    "else:\n",
    "    print(f\"Trovati {len(all_files)} file .parquet in '{INPUT_DIR}'\")\n",
    "\n",
    "TRAIN_FILES = all_files[0:16]\n",
    "IPERPARAMETRI_TIMESTEP = [10, 15, 20, 25, 30, 40]\n",
    "\n",
    "print(f\"\\nFASE 1: Avvio scansione unificata (UN FILE ALLA VOLTA)...\")\n",
    "\n",
    "total_counts_counter = Counter()\n",
    "total_ids_set = set()\n",
    "\n",
    "for file_path in TRAIN_FILES:\n",
    "    print(f\"  Processando {os.path.basename(file_path)}...\")\n",
    "    try:\n",
    "        df = pd.read_parquet(file_path, columns=['TrajectoryID'])\n",
    "        \n",
    "        total_ids_set.update(df['TrajectoryID'].unique())\n",
    "        \n",
    "        total_counts_counter.update(df['TrajectoryID'])\n",
    "        \n",
    "        del df\n",
    "        gc.collect()\n",
    "    except Exception as e:\n",
    "        print(f\"    ERRORE nel leggere {file_path}: {e}\")\n",
    "        \n",
    "\n",
    "print(\"FASE 2: Verifica e Analisi.\")\n",
    "\n",
    "conteggio_set = len(total_ids_set)\n",
    "conteggio_counter = len(total_counts_counter)\n",
    "\n",
    "print(f\"\\VERIFICA CONTEGGI:\")\n",
    "print(f\"ID Unici (metodo 'set'):     {conteggio_set:,}\")\n",
    "print(f\"ID Unici (metodo 'Counter'): {conteggio_counter:,}\")\n",
    "\n",
    "if conteggio_set != conteggio_counter:\n",
    "    print(\"ERRORE LOGICO: I conteggi non corrispondono. C'è ancora un problema.\")\n",
    "else:\n",
    "    print(\"\\nSUCCESSO! I conteggi corrispondono.\")\n",
    "    \n",
    "    all_lengths = list(total_counts_counter.values())\n",
    "    totale_traiettorie_analisi = conteggio_counter\n",
    "\n",
    "    print(f\"\\nRisultati dell'analisi (su {totale_traiettorie_analisi:,} traiettorie totali di TRAINING):\")\n",
    "    print(\"-\" * 75)\n",
    "    print(f\"{'Window Size':<15} | {'Traiettorie Usabili':<20} | {'Traiettorie Scartate':<20} | {'% Usabili':<10}\")\n",
    "    print(\"-\" * 75)\n",
    "\n",
    "    for timestep in IPERPARAMETRI_TIMESTEP:\n",
    "        traiettorie_usabili = sum(1 for length in all_lengths if length >= timestep)\n",
    "        traiettorie_scartate = totale_traiettorie_analisi - traiettorie_usabili\n",
    "        percentuale_usabili = (traiettorie_usabili / totale_traiettorie_analisi) * 100\n",
    "        print(f\"{timestep:<15} | {traiettorie_usabili:<20} | {traiettorie_scartate:<20} | {percentuale_usabili:<10.2f}%\")\n",
    "\n",
    "    print(\"-\" * 75)\n",
    "    print(\"Analisi completata.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cb7939",
   "metadata": {},
   "source": [
    "### Normalizzazione\n",
    "\n",
    "Passaggio fondamentale perchè le tue reti neurali (LSTM/LNN) faticano a imparare quando i dati di input hanno scale completamente diverse. Ad esempio, la colonna COG (rotta) va da 0 a 360, mentre la colonna SOG (velocità) potrebbe andare da 0 a 50. Questi intervalli così diversi \"confondono\" la rete durante l'addestramento.\n",
    "Dobbiamo quindi standardizzarli, portando tutte le colonne a una scala simile.  \n",
    "Il modo corretto per standardizzare è calcolare la media e la deviazione standard dell'intero set di addestramento (tutti i 4.3 milioni di traiettorie nei tuoi 16 file). Ovviamente, questi dati sono troppo grandi per essere caricati tutti insieme in memoria.\n",
    "\n",
    "Quello che facciamo è:  \n",
    "- Inizializziamo uno scaler vuoto e iteriamo sui 16 file di Train.\n",
    "- `partial_fit` è la parte fondamentale perchè per ogni file che carica. Questa funzione dice allo scaler: \"Prendi la media e la deviazione che hai calcolato finora e aggiornale includendo questo nuovo blocco di dati\".\n",
    "- `joblib.dump(scaler, SCALER_PATH)` salva questi parametri calcolati in un file esterno.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d81766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processando file 1/16: blocco_000-segmentato.parquet...\n",
      "  Processando file 2/16: blocco_001-segmentato.parquet...\n",
      "  Processando file 3/16: blocco_002-segmentato.parquet...\n",
      "  Processando file 4/16: blocco_003-segmentato.parquet...\n",
      "  Processando file 5/16: blocco_004-segmentato.parquet...\n",
      "  Processando file 6/16: blocco_005-segmentato.parquet...\n",
      "  Processando file 7/16: blocco_006-segmentato.parquet...\n",
      "  Processando file 8/16: blocco_007-segmentato.parquet...\n",
      "  Processando file 9/16: blocco_008-segmentato.parquet...\n",
      "  Processando file 10/16: blocco_009-segmentato.parquet...\n",
      "  Processando file 11/16: blocco_010-segmentato.parquet...\n",
      "  Processando file 12/16: blocco_011-segmentato.parquet...\n",
      "  Processando file 13/16: blocco_012-segmentato.parquet...\n",
      "  Processando file 14/16: blocco_013-segmentato.parquet...\n",
      "  Processando file 15/16: blocco_014-segmentato.parquet...\n",
      "  Processando file 16/16: blocco_015-segmentato.parquet...\n",
      "\n",
      "Adattamento completato su tutti i 16 file di training.\n",
      "\n",
      "✅ SUCCESSO! Scaler (fittato su TUTTO il training set) salvato come 'scaler.joblib'.\n"
     ]
    }
   ],
   "source": [
    "COLONNE_DA_NORMALIZZARE = ['Latitude', 'Longitude', 'SOG', 'COG']\n",
    "SCALER_PATH = 'scaler.joblib'\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "try:\n",
    "    \n",
    "    for i, file_path in enumerate(TRAIN_FILES):\n",
    "        \n",
    "        print(f\"  Processando file {i+1}/{len(TRAIN_FILES)}: {os.path.basename(file_path)}...\")\n",
    "        \n",
    "        df_chunk = pd.read_parquet(file_path, columns=COLONNE_DA_NORMALIZZARE)\n",
    "        \n",
    "        scaler.partial_fit(df_chunk)\n",
    "        \n",
    "        del df_chunk\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"\\nAdattamento completato su tutti i 16 file di training.\")\n",
    "    \n",
    "    joblib.dump(scaler, SCALER_PATH) #Salva file\n",
    "\n",
    "    print(f\"\\nScaler salvato come '{SCALER_PATH}'.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n Errore durante la preparazione dello scaler: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
