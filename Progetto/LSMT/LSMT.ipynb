{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "200e8a47",
   "metadata": {},
   "source": [
    "# Modello LSMT (Long Short-Term Memory) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4d2742",
   "metadata": {},
   "source": [
    "Primo step split del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5896be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ad8acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = '../Pre-Elaborazione Dati/Dataset'\n",
    "all_stitched_files = sorted(glob.glob(os.path.join(INPUT_DIR, '*.parquet')))\n",
    "\n",
    "# 2. Definisci la suddivisione 16-4-4 (8-2-2 mesi)\n",
    "# (Assicurati che i tuoi file siano ordinati cronologicamente, \n",
    "#  'blocco_000', 'blocco_001', ecc. come abbiamo fatto)\n",
    "\n",
    "TRAIN_FILES = all_stitched_files[0:16]  # Primi 16 blocchi (8 mesi)\n",
    "VAL_FILES = all_stitched_files[16:20] # Successivi 4 blocchi (2 mesi)\n",
    "TEST_FILES = all_stitched_files[20:24] # Ultimi 4 blocchi (2 mesi)\n",
    "\n",
    "print(f\"File di Training: {len(TRAIN_FILES)}\")\n",
    "print(f\"File di Validazione: {len(VAL_FILES)}\")\n",
    "print(f\"File di Test: {len(TEST_FILES)}\")\n",
    "\n",
    "# Stampa un esempio\n",
    "print(f\"\\nPrimo file di Train: {os.path.basename(TRAIN_FILES[0])}\")\n",
    "print(f\"Primo file di Val: {os.path.basename(VAL_FILES[0])}\")\n",
    "print(f\"Primo file di Test: {os.path.basename(TEST_FILES[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80885758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:53: SyntaxWarning: invalid escape sequence '\\V'\n",
      "<>:53: SyntaxWarning: invalid escape sequence '\\V'\n",
      "/tmp/ipykernel_11288/674630576.py:53: SyntaxWarning: invalid escape sequence '\\V'\n",
      "  print(f\"\\VERIFICA CONTEGGI:\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trovati 25 file .parquet in '/home/al3th3ia/Scrivania/Cybersecurity/Detecting-Trajectory-Spoofing-Attacks-on-AIS/Progetto/Pre-Elaborazione Dati/Dataset'\n",
      "\n",
      "FASE 1: Avvio scansione unificata (UN FILE ALLA VOLTA)...\n",
      "  Processando blocco_000-segmentato.parquet...\n",
      "  Processando blocco_001-segmentato.parquet...\n",
      "  Processando blocco_002-segmentato.parquet...\n",
      "  Processando blocco_003-segmentato.parquet...\n",
      "  Processando blocco_004-segmentato.parquet...\n",
      "  Processando blocco_005-segmentato.parquet...\n",
      "  Processando blocco_006-segmentato.parquet...\n",
      "  Processando blocco_007-segmentato.parquet...\n",
      "  Processando blocco_008-segmentato.parquet...\n",
      "  Processando blocco_009-segmentato.parquet...\n",
      "  Processando blocco_010-segmentato.parquet...\n",
      "  Processando blocco_011-segmentato.parquet...\n",
      "  Processando blocco_012-segmentato.parquet...\n",
      "  Processando blocco_013-segmentato.parquet...\n",
      "  Processando blocco_014-segmentato.parquet...\n",
      "  Processando blocco_015-segmentato.parquet...\n",
      "\n",
      "------------------------------------------------------------\n",
      "FASE 1: Scansione completata.\n",
      "FASE 2: Verifica e Analisi.\n",
      "\\VERIFICA CONTEGGI:\n",
      "ID Unici (metodo 'set'):     4,370,475\n",
      "ID Unici (metodo 'Counter'): 4,370,475\n",
      "\n",
      "SUCCESSO! I conteggi corrispondono.\n",
      "\n",
      "Risultati dell'analisi (su 4,370,475 traiettorie totali di TRAINING):\n",
      "---------------------------------------------------------------------------\n",
      "Window Size     | Traiettorie Usabili  | Traiettorie Scartate | % Usabili \n",
      "---------------------------------------------------------------------------\n",
      "10              | 2957676              | 1412799              | 67.67     %\n",
      "15              | 2678739              | 1691736              | 61.29     %\n",
      "20              | 2461907              | 1908568              | 56.33     %\n",
      "25              | 2284093              | 2086382              | 52.26     %\n",
      "30              | 2131890              | 2238585              | 48.78     %\n",
      "40              | 1882235              | 2488240              | 43.07     %\n",
      "50              | 1685008              | 2685467              | 38.55     %\n",
      "60              | 1524298              | 2846177              | 34.88     %\n",
      "100             | 1089262              | 3281213              | 24.92     %\n",
      "---------------------------------------------------------------------------\n",
      "Analisi completata.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "from collections import Counter\n",
    "\n",
    "# --- 1. CONFIGURAZIONE ---\n",
    "INPUT_DIR = '/home/al3th3ia/Scrivania/Cybersecurity/Detecting-Trajectory-Spoofing-Attacks-on-AIS/Progetto/Pre-Elaborazione Dati/Dataset' \n",
    "\n",
    "all_files = sorted(glob.glob(os.path.join(INPUT_DIR, '*.parquet')))\n",
    "\n",
    "if len(all_files) == 0:\n",
    "    print(f\"ERRORE CRITICO: Nessun file .parquet trovato in '{INPUT_DIR}'\")\n",
    "else:\n",
    "    print(f\"Trovati {len(all_files)} file .parquet in '{INPUT_DIR}'\")\n",
    "\n",
    "TRAIN_FILES = all_files[0:16]\n",
    "IPERPARAMETRI_TIMESTEP = [10, 15, 20, 25, 30, 40]\n",
    "\n",
    "# --- FASE 1: Scansione Unificata (Anti-RAM Crash) ---\n",
    "print(f\"\\nFASE 1: Avvio scansione unificata (UN FILE ALLA VOLTA)...\")\n",
    "\n",
    "total_counts_counter = Counter()\n",
    "total_ids_set = set()\n",
    "\n",
    "for file_path in TRAIN_FILES:\n",
    "    print(f\"  Processando {os.path.basename(file_path)}...\")\n",
    "    try:\n",
    "        df = pd.read_parquet(file_path, columns=['TrajectoryID'])\n",
    "        \n",
    "        # 1. Metodo Set (affidabile)\n",
    "        total_ids_set.update(df['TrajectoryID'].unique())\n",
    "        \n",
    "        # 2. Metodo Counter (NUOVA VERSIONE CORRETTA)\n",
    "        # Invece di .groupby(), che fallisce, diamo in pasto\n",
    "        # l'intera colonna al Counter. È molto più efficiente.\n",
    "        total_counts_counter.update(df['TrajectoryID'])\n",
    "        \n",
    "        del df\n",
    "        gc.collect()\n",
    "    except Exception as e:\n",
    "        print(f\"    ERRORE nel leggere {file_path}: {e}\")\n",
    "        \n",
    "print(\"\\n\" + \"---\" * 20)\n",
    "print(\"FASE 1: Scansione completata.\")\n",
    "\n",
    "# --- FASE 2: Verifica e Analisi ---\n",
    "print(\"FASE 2: Verifica e Analisi.\")\n",
    "\n",
    "conteggio_set = len(total_ids_set)\n",
    "conteggio_counter = len(total_counts_counter)\n",
    "\n",
    "print(f\"\\VERIFICA CONTEGGI:\")\n",
    "print(f\"ID Unici (metodo 'set'):     {conteggio_set:,}\")\n",
    "print(f\"ID Unici (metodo 'Counter'): {conteggio_counter:,}\")\n",
    "\n",
    "# Ora DEVONO corrispondere\n",
    "if conteggio_set != conteggio_counter:\n",
    "    print(\"ERRORE LOGICO: I conteggi non corrispondono. C'è ancora un problema.\")\n",
    "else:\n",
    "    print(\"\\nSUCCESSO! I conteggi corrispondono.\")\n",
    "    \n",
    "    # Ora che siamo sicuri, procediamo con l'analisi\n",
    "    # Estraiamo le LUNGHEZZE dal counter\n",
    "    all_lengths = list(total_counts_counter.values())\n",
    "    totale_traiettorie_analisi = conteggio_counter\n",
    "\n",
    "    print(f\"\\nRisultati dell'analisi (su {totale_traiettorie_analisi:,} traiettorie totali di TRAINING):\")\n",
    "    print(\"-\" * 75)\n",
    "    print(f\"{'Window Size':<15} | {'Traiettorie Usabili':<20} | {'Traiettorie Scartate':<20} | {'% Usabili':<10}\")\n",
    "    print(\"-\" * 75)\n",
    "\n",
    "    for timestep in IPERPARAMETRI_TIMESTEP:\n",
    "        traiettorie_usabili = sum(1 for length in all_lengths if length >= timestep)\n",
    "        traiettorie_scartate = totale_traiettorie_analisi - traiettorie_usabili\n",
    "        percentuale_usabili = (traiettorie_usabili / totale_traiettorie_analisi) * 100\n",
    "        print(f\"{timestep:<15} | {traiettorie_usabili:<20} | {traiettorie_scartate:<20} | {percentuale_usabili:<10.2f}%\")\n",
    "\n",
    "    print(\"-\" * 75)\n",
    "    print(\"Analisi completata.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
