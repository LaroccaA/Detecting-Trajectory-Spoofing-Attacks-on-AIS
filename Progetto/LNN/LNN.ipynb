{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db3be638",
   "metadata": {},
   "source": [
    "## LNN Addestramento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc5080a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurazione Memoria OK: 1 GPU\n",
      "Configurazione LNN caricata.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"Configurazione Memoria OK: {len(gpus)} GPU\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Errore Configurazione Memoria: {e}\")\n",
    "\n",
    "from ncps.tf import CfC\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import gc\n",
    "import joblib\n",
    "import pyarrow.parquet as pq\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, TimeDistributed, RepeatVector\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts, ExponentialDecay\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "from ncps.wirings import AutoNCP\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# PATH\n",
    "INPUT_DIR = '../Pre-Elaborazione Dati/Dataset' \n",
    "SCALER_PATH = 'scaler.joblib' \n",
    "COLONNE_FEATURES = ['Latitude', 'Longitude', 'SOG', 'COG']\n",
    "\n",
    "WINDOW_SIZE = 30  \n",
    "BATCH_SIZE = 64 \n",
    "\n",
    "all_files = sorted(glob.glob(os.path.join(INPUT_DIR, '*.parquet')))\n",
    "TRAIN_FILES = all_files[0:16]\n",
    "VAL_FILES = all_files[16:20]\n",
    "\n",
    "print(\"Configurazione LNN caricata.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6e9543",
   "metadata": {},
   "source": [
    "#### Funzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2c48d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funzioni definite\n"
     ]
    }
   ],
   "source": [
    "def create_windows(data_np, window_size):\n",
    "    windows = []\n",
    "    for i in range(len(data_np) - window_size + 1):\n",
    "        windows.append(data_np[i : i + window_size])\n",
    "    return windows\n",
    "\n",
    "def data_generator(file_paths, scaler, features, window_size, batch_size, shuffle_files=False):\n",
    "    \n",
    "    file_buffer = {} \n",
    "    window_buffer = [] \n",
    "    CHUNK_SIZE_ROWS = 500_000\n",
    "\n",
    "    while True:\n",
    "        if shuffle_files:\n",
    "             # Shuffle disattivato forzatamente per garantire la sequenzialit√†\n",
    "            shuffle_files = False \n",
    "            \n",
    "        for file_path in file_paths:\n",
    "            chunk_buffer = {}\n",
    "            try:\n",
    "                pf = pq.ParquetFile(file_path)\n",
    "                for batch in pf.iter_batches(batch_size=CHUNK_SIZE_ROWS, columns=features + ['TrajectoryID']):\n",
    "                    df_chunk = batch.to_pandas()\n",
    "                    df_chunk[features] = scaler.transform(df_chunk[features])\n",
    "                    next_chunk_buffer = {}\n",
    "                    \n",
    "                    for tid, group in df_chunk.groupby('TrajectoryID'):\n",
    "                        if tid in chunk_buffer:\n",
    "                            trajectory_data = pd.concat([chunk_buffer.pop(tid), group])\n",
    "                        else:\n",
    "                            trajectory_data = group\n",
    "                        \n",
    "                        if tid in file_buffer:\n",
    "                            trajectory_data = pd.concat([file_buffer.pop(tid), trajectory_data])\n",
    "                        \n",
    "                        # Se la traiettoria tocca la fine del chunk, bufferizzala\n",
    "                        if trajectory_data.iloc[-1].name == df_chunk.iloc[-1].name:\n",
    "                            next_chunk_buffer[tid] = trajectory_data\n",
    "                            continue \n",
    "                            \n",
    "                        if len(trajectory_data) < window_size:\n",
    "                            continue \n",
    "                            \n",
    "                        trajectory_np = trajectory_data[features].to_numpy()\n",
    "                        new_windows = create_windows(trajectory_np, window_size)\n",
    "                        window_buffer.extend(new_windows)\n",
    "                        \n",
    "                        next_chunk_buffer[tid] = trajectory_data.iloc[-(window_size - 1):]\n",
    "\n",
    "                        while len(window_buffer) >= batch_size:\n",
    "                            batch_to_yield = window_buffer[:batch_size]\n",
    "                            window_buffer = window_buffer[batch_size:]\n",
    "                            yield (np.array(batch_to_yield), np.array(batch_to_yield))\n",
    "                    \n",
    "                    chunk_buffer = next_chunk_buffer\n",
    "                file_buffer = chunk_buffer\n",
    "            except Exception as e:\n",
    "                print(f\"\\nErrore lettura {file_path}: {e}\")\n",
    "                continue\n",
    "print(\"Funzioni definite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e638a97e",
   "metadata": {},
   "source": [
    "#### Scaler e Generatori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "205196ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inizializzazione generatori\n",
      "Generatori pronti.\n"
     ]
    }
   ],
   "source": [
    "print(\"Inizializzazione generatori\")\n",
    "scaler = joblib.load(SCALER_PATH)\n",
    "\n",
    "train_gen = data_generator(\n",
    "    file_paths=TRAIN_FILES,\n",
    "    scaler=scaler,\n",
    "    features=COLONNE_FEATURES,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle_files=False \n",
    ")\n",
    "\n",
    "val_gen = data_generator(\n",
    "    file_paths=VAL_FILES,\n",
    "    scaler=scaler,\n",
    "    features=COLONNE_FEATURES,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle_files=False\n",
    ")\n",
    "print(\"Generatori pronti.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab9bd18",
   "metadata": {},
   "source": [
    "#### Modello LNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13a65588",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1764090248.612338   12020 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4130 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 30, 4)]           0         \n",
      "                                                                 \n",
      " cf_c (CfC)                  (None, 64)                123844    \n",
      "                                                                 \n",
      " repeat_vector (RepeatVecto  (None, 30, 64)            0         \n",
      " r)                                                              \n",
      "                                                                 \n",
      " cf_c_1 (CfC)                (None, 30, 64)            154564    \n",
      "                                                                 \n",
      " time_distributed (TimeDist  (None, 30, 4)             260       \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 278668 (1.06 MB)\n",
      "Trainable params: 278668 (1.06 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_features = len(COLONNE_FEATURES)\n",
    "latent_dim = 128\n",
    "output_dim = 64\n",
    "wiring = AutoNCP(latent_dim,output_dim) # Definisce una wiring sparsa\n",
    "\n",
    "# Encoder\n",
    "inputs = Input(shape=(WINDOW_SIZE, n_features))\n",
    "# LAYER LIQUIDO 1 (Encoder): USIAMO WIRING SPARSA\n",
    "lnn_encoder = CfC(wiring, return_sequences=False, mixed_memory=True)(inputs) \n",
    "\n",
    "# Decoder\n",
    "repeat_vector = RepeatVector(WINDOW_SIZE)(lnn_encoder)\n",
    "lnn_decoder = CfC(wiring, return_sequences=True, mixed_memory=True)(repeat_vector)\n",
    "\n",
    "output = TimeDistributed(Dense(n_features))(lnn_decoder)\n",
    "\n",
    "model_lnn = Model(inputs, output) # Rinominato per evitare confusione\n",
    "\n",
    "model_lnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decce365",
   "metadata": {},
   "source": [
    "#### Addestramento LNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ca56dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.1244\n",
      "Epoch 1: val_loss improved from inf to 0.05828, saving model to lnn_autoencoder_best.weights.h5\n",
      "40000/40000 [==============================] - 3768s 94ms/step - loss: 0.1244 - val_loss: 0.0583\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.0950\n",
      "Epoch 2: val_loss improved from 0.05828 to 0.05225, saving model to lnn_autoencoder_best.weights.h5\n",
      "40000/40000 [==============================] - 3991s 100ms/step - loss: 0.0950 - val_loss: 0.0523\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.0402\n",
      "Epoch 3: val_loss improved from 0.05225 to 0.03114, saving model to lnn_autoencoder_best.weights.h5\n",
      "40000/40000 [==============================] - 4983s 125ms/step - loss: 0.0402 - val_loss: 0.0311\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.0327\n",
      "Epoch 4: val_loss improved from 0.03114 to 0.02890, saving model to lnn_autoencoder_best.weights.h5\n",
      "40000/40000 [==============================] - 3857s 96ms/step - loss: 0.0327 - val_loss: 0.0289\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.0342\n",
      "Epoch 5: val_loss did not improve from 0.02890\n",
      "40000/40000 [==============================] - 3899s 97ms/step - loss: 0.0342 - val_loss: 0.0422\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.0385\n",
      "Epoch 6: val_loss did not improve from 0.02890\n",
      "40000/40000 [==============================] - 3861s 97ms/step - loss: 0.0385 - val_loss: 0.0570\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.0331\n",
      "Epoch 7: val_loss did not improve from 0.02890\n",
      "40000/40000 [==============================] - 3815s 95ms/step - loss: 0.0331 - val_loss: 0.0694\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.0411\n",
      "Epoch 8: val_loss did not improve from 0.02890\n",
      "40000/40000 [==============================] - 3805s 95ms/step - loss: 0.0411 - val_loss: 0.0496\n",
      "Epoch 9/20\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.0324\n",
      "Epoch 9: val_loss did not improve from 0.02890\n",
      "40000/40000 [==============================] - 3806s 95ms/step - loss: 0.0324 - val_loss: 0.0527\n",
      "Epoch 10/20\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.0260\n",
      "Epoch 10: val_loss improved from 0.02890 to 0.02820, saving model to lnn_autoencoder_best.weights.h5\n",
      "40000/40000 [==============================] - 3798s 95ms/step - loss: 0.0260 - val_loss: 0.0282\n",
      "Epoch 11/20\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.0113\n",
      "Epoch 11: val_loss did not improve from 0.02820\n",
      "40000/40000 [==============================] - 3814s 95ms/step - loss: 0.0113 - val_loss: 0.0331\n",
      "Epoch 12/20\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.0072\n",
      "Epoch 12: val_loss improved from 0.02820 to 0.02764, saving model to lnn_autoencoder_best.weights.h5\n",
      "40000/40000 [==============================] - 3803s 95ms/step - loss: 0.0072 - val_loss: 0.0276\n",
      "Epoch 13/20\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.0075\n",
      "Epoch 13: val_loss improved from 0.02764 to 0.01760, saving model to lnn_autoencoder_best.weights.h5\n",
      "40000/40000 [==============================] - 3807s 95ms/step - loss: 0.0075 - val_loss: 0.0176\n",
      "Epoch 14/20\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.0161\n",
      "Epoch 14: val_loss improved from 0.01760 to 0.01702, saving model to lnn_autoencoder_best.weights.h5\n",
      "40000/40000 [==============================] - 3804s 95ms/step - loss: 0.0161 - val_loss: 0.0170\n",
      "Epoch 15/20\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.0288\n",
      "Epoch 15: val_loss improved from 0.01702 to 0.01623, saving model to lnn_autoencoder_best.weights.h5\n",
      "40000/40000 [==============================] - 3803s 95ms/step - loss: 0.0288 - val_loss: 0.0162\n",
      "Epoch 16/20\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.0273\n",
      "Epoch 16: val_loss did not improve from 0.01623\n",
      "40000/40000 [==============================] - 3693s 92ms/step - loss: 0.0273 - val_loss: 0.0214\n",
      "Epoch 17/20\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.0200\n",
      "Epoch 17: val_loss did not improve from 0.01623\n",
      "40000/40000 [==============================] - 3660s 91ms/step - loss: 0.0200 - val_loss: 0.0257\n",
      "Epoch 18/20\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.0204\n",
      "Epoch 18: val_loss did not improve from 0.01623\n",
      "40000/40000 [==============================] - 3700s 92ms/step - loss: 0.0204 - val_loss: 0.0306\n",
      "Epoch 19/20\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.0262\n",
      "Epoch 19: val_loss did not improve from 0.01623\n",
      "40000/40000 [==============================] - 3668s 92ms/step - loss: 0.0262 - val_loss: 0.0207\n",
      "Epoch 20/20\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.8200\n",
      "Epoch 20: val_loss did not improve from 0.01623\n",
      "40000/40000 [==============================] - 3628s 91ms/step - loss: 0.8200 - val_loss: 1.4613\n",
      "Restoring model weights from the end of the best epoch: 15.\n",
      "\n",
      "Addestramento LNN Completato.\n"
     ]
    }
   ],
   "source": [
    "STEPS_PER_EPOCH_LNN = 40000 \n",
    "VALIDATION_STEPS_LNN = 8000 \n",
    "EPOCHS_LNN = 20 \n",
    "\n",
    "initial_learning_rate = 0.0005  \n",
    "T_0 = 5\n",
    "lr_schedule = CosineDecayRestarts(\n",
    "    initial_learning_rate,\n",
    "    first_decay_steps=T_0 * STEPS_PER_EPOCH_LNN,\n",
    "    t_mul=2.0,                  \n",
    "    m_mul=0.9,                  \n",
    "    alpha=1e-6 # LR minimo\n",
    ")\n",
    "\n",
    "# OTTIMIZZATORE\n",
    "optimizer_lnn_final = Adam(\n",
    "    learning_rate=lr_schedule, \n",
    "    clipvalue=0.5             \n",
    ")\n",
    "\n",
    "model_lnn.compile(optimizer=optimizer_lnn_final, loss='mae')\n",
    "\n",
    "#CALLBACKS\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'lnn_autoencoder_best.weights.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,     \n",
    "    mode='min',\n",
    "    verbose=1,\n",
    "    save_weights_only=True # Salva solo i pesi numerici\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', patience=10, mode='min', verbose=1, restore_best_weights=True\n",
    ")\n",
    "\n",
    "csv_logger = tf.keras.callbacks.CSVLogger('training_log_lnn.csv', append=True)\n",
    "\n",
    "# START\n",
    "try:\n",
    "    history_lnn = model_lnn.fit(\n",
    "        train_gen,\n",
    "        steps_per_epoch=STEPS_PER_EPOCH_LNN,\n",
    "        epochs=EPOCHS_LNN,\n",
    "        validation_data=val_gen,\n",
    "        validation_steps=VALIDATION_STEPS_LNN,\n",
    "        callbacks=[checkpoint, early_stopping, csv_logger],\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"\\nAddestramento LNN Completato.\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nInterrotto manualmente.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
