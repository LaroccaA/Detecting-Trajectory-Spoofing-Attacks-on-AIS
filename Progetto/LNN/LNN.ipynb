{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db3be638",
   "metadata": {},
   "source": [
    "## LNN Addestramento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc5080a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurazione Memoria OK: 1 GPU\n",
      "Configurazione LNN caricata.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"Configurazione Memoria OK: {len(gpus)} GPU\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Errore Configurazione Memoria: {e}\")\n",
    "\n",
    "from ncps.tf import CfC\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import gc\n",
    "import joblib\n",
    "import pyarrow.parquet as pq\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, TimeDistributed, RepeatVector\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# PATH\n",
    "INPUT_DIR = '../Pre-Elaborazione Dati/Dataset' \n",
    "SCALER_PATH = 'scaler.joblib' \n",
    "COLONNE_FEATURES = ['Latitude', 'Longitude', 'SOG', 'COG']\n",
    "\n",
    "WINDOW_SIZE = 30  \n",
    "BATCH_SIZE = 64 \n",
    "\n",
    "all_files = sorted(glob.glob(os.path.join(INPUT_DIR, '*.parquet')))\n",
    "TRAIN_FILES = all_files[0:16]\n",
    "VAL_FILES = all_files[16:20]\n",
    "\n",
    "print(\"Configurazione LNN caricata.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6e9543",
   "metadata": {},
   "source": [
    "#### Funzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2c48d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funzioni definite\n"
     ]
    }
   ],
   "source": [
    "def create_windows(data_np, window_size):\n",
    "    windows = []\n",
    "    for i in range(len(data_np) - window_size + 1):\n",
    "        windows.append(data_np[i : i + window_size])\n",
    "    return windows\n",
    "\n",
    "def data_generator(file_paths, scaler, features, window_size, batch_size, shuffle_files=False):\n",
    "    \n",
    "    file_buffer = {} \n",
    "    window_buffer = [] \n",
    "    CHUNK_SIZE_ROWS = 500_000\n",
    "\n",
    "    while True:\n",
    "        if shuffle_files:\n",
    "             # Shuffle disattivato forzatamente per garantire la sequenzialità\n",
    "            shuffle_files = False \n",
    "            \n",
    "        for file_path in file_paths:\n",
    "            chunk_buffer = {}\n",
    "            try:\n",
    "                pf = pq.ParquetFile(file_path)\n",
    "                for batch in pf.iter_batches(batch_size=CHUNK_SIZE_ROWS, columns=features + ['TrajectoryID']):\n",
    "                    df_chunk = batch.to_pandas()\n",
    "                    df_chunk[features] = scaler.transform(df_chunk[features])\n",
    "                    next_chunk_buffer = {}\n",
    "                    \n",
    "                    for tid, group in df_chunk.groupby('TrajectoryID'):\n",
    "                        if tid in chunk_buffer:\n",
    "                            trajectory_data = pd.concat([chunk_buffer.pop(tid), group])\n",
    "                        else:\n",
    "                            trajectory_data = group\n",
    "                        \n",
    "                        if tid in file_buffer:\n",
    "                            trajectory_data = pd.concat([file_buffer.pop(tid), trajectory_data])\n",
    "                        \n",
    "                        # Se la traiettoria tocca la fine del chunk, bufferizzala\n",
    "                        if trajectory_data.iloc[-1].name == df_chunk.iloc[-1].name:\n",
    "                            next_chunk_buffer[tid] = trajectory_data\n",
    "                            continue \n",
    "                            \n",
    "                        if len(trajectory_data) < window_size:\n",
    "                            continue \n",
    "                            \n",
    "                        trajectory_np = trajectory_data[features].to_numpy()\n",
    "                        new_windows = create_windows(trajectory_np, window_size)\n",
    "                        window_buffer.extend(new_windows)\n",
    "                        \n",
    "                        next_chunk_buffer[tid] = trajectory_data.iloc[-(window_size - 1):]\n",
    "\n",
    "                        while len(window_buffer) >= batch_size:\n",
    "                            batch_to_yield = window_buffer[:batch_size]\n",
    "                            window_buffer = window_buffer[batch_size:]\n",
    "                            yield (np.array(batch_to_yield), np.array(batch_to_yield))\n",
    "                    \n",
    "                    chunk_buffer = next_chunk_buffer\n",
    "                file_buffer = chunk_buffer\n",
    "            except Exception as e:\n",
    "                print(f\"\\nErrore lettura {file_path}: {e}\")\n",
    "                continue\n",
    "print(\"Funzioni definite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e638a97e",
   "metadata": {},
   "source": [
    "#### Scaler e Generatori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "205196ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inizializzazione generatori\n",
      "Generatori pronti.\n"
     ]
    }
   ],
   "source": [
    "print(\"Inizializzazione generatori\")\n",
    "scaler = joblib.load(SCALER_PATH)\n",
    "\n",
    "train_gen = data_generator(\n",
    "    file_paths=TRAIN_FILES,\n",
    "    scaler=scaler,\n",
    "    features=COLONNE_FEATURES,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle_files=False \n",
    ")\n",
    "\n",
    "val_gen = data_generator(\n",
    "    file_paths=VAL_FILES,\n",
    "    scaler=scaler,\n",
    "    features=COLONNE_FEATURES,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle_files=False\n",
    ")\n",
    "print(\"Generatori pronti.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab9bd18",
   "metadata": {},
   "source": [
    "#### Modello LNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13a65588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello LNN (Liquid Neural Network) creato e compilato.\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 30, 4)]           0         \n",
      "                                                                 \n",
      " cf_c_2 (CfC)                (None, 32)                29568     \n",
      "                                                                 \n",
      " repeat_vector_1 (RepeatVec  (None, 30, 32)            0         \n",
      " tor)                                                            \n",
      "                                                                 \n",
      " cf_c_3 (CfC)                (None, 30, 32)            33152     \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDi  (None, 30, 4)             132       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 62852 (245.52 KB)\n",
      "Trainable params: 62852 (245.52 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_features = len(COLONNE_FEATURES)\n",
    "latent_dim = 32 # Stessa dimensione dell'LSTM\n",
    "\n",
    "# Encoder\n",
    "inputs = Input(shape=(WINDOW_SIZE, n_features))\n",
    "\n",
    "# LAYER LIQUIDO 1 (Encoder)\n",
    "lnn_encoder = CfC(latent_dim, return_sequences=False, mixed_memory=True)(inputs)\n",
    "\n",
    "# Ponte\n",
    "repeat_vector = RepeatVector(WINDOW_SIZE)(lnn_encoder)\n",
    "\n",
    "# LAYER LIQUIDO 2 (Decoder)\n",
    "lnn_decoder = CfC(latent_dim, return_sequences=True, mixed_memory=True)(repeat_vector)\n",
    "\n",
    "# Output\n",
    "output = TimeDistributed(Dense(n_features))(lnn_decoder)\n",
    "\n",
    "model_lnn = Model(inputs, output)\n",
    "model_lnn.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "print(\"Modello LNN (Liquid Neural Network) creato e compilato.\")\n",
    "model_lnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decce365",
   "metadata": {},
   "source": [
    "#### Addestramento LNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ca56dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello Resettato.\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.6322\n",
      "Epoch 1: val_loss improved from inf to 0.79569, saving model to lnn_autoencoder_best.keras\n",
      "40000/40000 [==============================] - 2488s 62ms/step - loss: 0.6322 - val_loss: 0.7957\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.5069\n",
      "Epoch 2: val_loss improved from 0.79569 to 0.68646, saving model to lnn_autoencoder_best.keras\n",
      "40000/40000 [==============================] - 2492s 62ms/step - loss: 0.5069 - val_loss: 0.6865\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.5474\n",
      "Epoch 3: val_loss did not improve from 0.68646\n",
      "40000/40000 [==============================] - 2500s 63ms/step - loss: 0.5474 - val_loss: 0.9672\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.5415\n",
      "Epoch 4: val_loss improved from 0.68646 to 0.62918, saving model to lnn_autoencoder_best.keras\n",
      "40000/40000 [==============================] - 2503s 63ms/step - loss: 0.5415 - val_loss: 0.6292\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.5472\n",
      "Epoch 5: val_loss improved from 0.62918 to 0.57174, saving model to lnn_autoencoder_best.keras\n",
      "40000/40000 [==============================] - 2504s 63ms/step - loss: 0.5472 - val_loss: 0.5717\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.5822\n",
      "Epoch 6: val_loss did not improve from 0.57174\n",
      "40000/40000 [==============================] - 2512s 63ms/step - loss: 0.5822 - val_loss: 0.5965\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.6142\n",
      "Epoch 7: val_loss improved from 0.57174 to 0.57095, saving model to lnn_autoencoder_best.keras\n",
      "40000/40000 [==============================] - 2507s 63ms/step - loss: 0.6142 - val_loss: 0.5710\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.6351\n",
      "Epoch 8: val_loss did not improve from 0.57095\n",
      "40000/40000 [==============================] - 2523s 63ms/step - loss: 0.6351 - val_loss: 0.8015\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.7393\n",
      "Epoch 9: val_loss did not improve from 0.57095\n",
      "40000/40000 [==============================] - 2525s 63ms/step - loss: 0.7393 - val_loss: 0.7529\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.6538\n",
      "Epoch 10: val_loss did not improve from 0.57095\n",
      "40000/40000 [==============================] - 2518s 63ms/step - loss: 0.6538 - val_loss: 0.7060\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.6680\n",
      "Epoch 11: val_loss did not improve from 0.57095\n",
      "40000/40000 [==============================] - 2524s 63ms/step - loss: 0.6680 - val_loss: 0.7260\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.6454\n",
      "Epoch 12: val_loss did not improve from 0.57095\n",
      "40000/40000 [==============================] - 2524s 63ms/step - loss: 0.6454 - val_loss: 0.7643\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.7540\n",
      "Epoch 13: val_loss did not improve from 0.57095\n",
      "40000/40000 [==============================] - 2467s 62ms/step - loss: 0.7540 - val_loss: 0.7785\n",
      "Epoch 14/100\n",
      " 5305/40000 [==>...........................] - ETA: 35:22 - loss: 0.7949\n",
      "Interrotto manualmente.\n"
     ]
    }
   ],
   "source": [
    "# Parametri\n",
    "STEPS_PER_EPOCH_LNN = 40000\n",
    "VALIDATION_STEPS_LNN = 8000 \n",
    "EPOCHS_LNN = 100 \n",
    "\n",
    "# DEFINIZIONE OTTIMIZZATORE\n",
    "optimizer_lnn = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.00025, \n",
    "    clipvalue=0.5          # Taglio netto ai gradienti\n",
    ")\n",
    "\n",
    "# RICOMPILAZIONE (Reset)\n",
    "model_lnn.compile(optimizer=optimizer_lnn, loss='mae')\n",
    "print(\"Modello Resettato.\")\n",
    "\n",
    "# CALLBACKS\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'lnn_autoencoder_best.keras', \n",
    "    monitor='val_loss', \n",
    "    save_best_only=True, \n",
    "    mode='min', \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=10, # Diamo tempo perché imparerà più lentamente\n",
    "    mode='min', \n",
    "    verbose=1, \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "csv_logger = tf.keras.callbacks.CSVLogger('training_log_lnn_v4.csv', append=True)\n",
    "\n",
    "#START\n",
    "try:\n",
    "    history_lnn = model_lnn.fit(\n",
    "        train_gen,\n",
    "        steps_per_epoch=STEPS_PER_EPOCH_LNN,\n",
    "        epochs=EPOCHS_LNN,\n",
    "        validation_data=val_gen,\n",
    "        validation_steps=VALIDATION_STEPS_LNN,\n",
    "        callbacks=[checkpoint, early_stopping, csv_logger],\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"\\nAddestramento LNN Completato!\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nInterrotto manualmente.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
