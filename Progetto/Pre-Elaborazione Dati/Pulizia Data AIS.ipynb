{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67cbf103",
   "metadata": {},
   "source": [
    "# Pre-Elaborazione dei Dati (Dataset di riferimento 2025 da Gennaio a Giugno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ec05d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import shutil\n",
    "import re "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbbdcf0",
   "metadata": {},
   "source": [
    "### Delineamo l'ambiente di lavoro\n",
    "\n",
    "In questa sezione vengono definite le directory di lavoro e tutti quei parametri per cui andiamo a filtrare i nostri dati.\n",
    "\n",
    "SOG_MIN --> Impostiamo il parametro a 2.0, questo ci serve per poi andare a scartare tutte le navi ferme.\n",
    "\n",
    "TIME_GAP --> Questa √® una soglia di tempo massima arbitraria permessa all'interno di una singola traiettoria. Se tra due messaggi consecutivi della stessa nave passano pi√π di 60 minuti, assumiamo che la rotta sia stata interrotta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbf94fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trovati 365 file Parquet da processare.\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIR = '../../../Dataset'\n",
    "SCRIPT_DIR = os.getcwd()                                # Restituisce la directory di lavoro corrente\n",
    "\n",
    "OUTPUT_DIR_NAME = 'Dataset_Pre-Cleaned_AIS' \n",
    "OUTPUT_DIR = os.path.join(SCRIPT_DIR, OUTPUT_DIR_NAME)\n",
    "\n",
    "SOG_MIN_THRESHOLD = 2.0\n",
    "TIME_GAP_THRESHOLD = pd.Timedelta(hours=1)\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "all_files = glob.glob(os.path.join(INPUT_DIR, '*.parquet'))\n",
    "\n",
    "all_clean_data = []\n",
    "\n",
    "print(f\"Trovati {len(all_files)} file Parquet da processare.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3397dad",
   "metadata": {},
   "source": [
    "#### Test\n",
    "\n",
    "- Proviamo a verificare la lettura di un file parquet e della corretta formattazione dei dati.  \n",
    "- Oltre a questo andiamo ad estrarre il numero di colonne per verificare se sono state selezionate le colonne corrette.  \n",
    "- Viene aggiunto anche un controllo sulle righe per vedere dopo la pulizia la percentuale di pulizia per ogni file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b38aa3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üîç DEBUG: Dati iniziali dal file ais-2025-01-01.parquet ---\n",
      "\n",
      "Head del DataFrame:\n",
      "        MMSI  Latitude  Longitude  SOG    COG           Timestamp\n",
      "0  671087100  18.46281  -66.10297  0.0  176.7 2025-01-01 00:00:00\n",
      "1  367733950  48.48503 -122.60927  0.0  215.5 2025-01-01 00:00:00\n",
      "2  368138010  40.47715  -73.84652  5.5  286.9 2025-01-01 00:00:02\n",
      "3  367637210  29.12033  -90.21215  0.0  227.6 2025-01-01 00:00:03\n",
      "4  368050000  41.27196  -72.46934  0.0  107.1 2025-01-01 00:00:03\n",
      "\n",
      "Tipi di Dati (Dtypes) dopo la conversione Timestamp:\n",
      "MMSI                  int64\n",
      "Latitude            float64\n",
      "Longitude           float64\n",
      "SOG                 float64\n",
      "COG                 float64\n",
      "Timestamp    datetime64[ns]\n",
      "dtype: object\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Numero di righe: 7337208, Numero di colonne: 6\n",
      "\n",
      "--- üîç DEBUG: Dati iniziali dal file AIS_2024_12_31.parquet ---\n",
      "\n",
      "Head del DataFrame:\n",
      "        MMSI  Latitude  Longitude   SOG    COG           Timestamp\n",
      "0  367776660  21.19308 -157.72342   8.0  112.1 2024-12-31 00:00:08\n",
      "1  368095340  29.76995  -95.07893   0.0  185.5 2024-12-31 00:00:05\n",
      "2  366847780  29.96697  -93.85909   0.1  186.2 2024-12-31 00:00:00\n",
      "3  367481310  27.68242  -82.58073  11.5   57.6 2024-12-31 00:00:04\n",
      "4  248669000  29.85743  -93.94083   3.1  220.9 2024-12-31 00:00:06\n",
      "\n",
      "Tipi di Dati (Dtypes) dopo la conversione Timestamp:\n",
      "MMSI                  int64\n",
      "Latitude            float64\n",
      "Longitude           float64\n",
      "SOG                 float64\n",
      "COG                 float64\n",
      "Timestamp    datetime64[ns]\n",
      "dtype: object\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Numero di righe: 7588976, Numero di colonne: 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = '../../../Dataset/'\n",
    "\n",
    "FILE_PATH_TEST = os.path.join(BASE_PATH, 'ais-2025-01-01.parquet')\n",
    "FILE_PATH_TEST2 = os.path.join(BASE_PATH, 'AIS_2024_12_31.parquet')\n",
    "\n",
    "COLUMN_MAPPING2025 = {\n",
    "    'mmsi': 'MMSI', \n",
    "    'latitude': 'Latitude', \n",
    "    'longitude': 'Longitude', \n",
    "    'sog': 'SOG', \n",
    "    'cog': 'COG', \n",
    "    'base_date_time': 'Timestamp' \n",
    "}\n",
    "COLUMNS_TO_READ_2025 = list(COLUMN_MAPPING2025.keys())\n",
    "\n",
    "try:\n",
    "    df = pd.read_parquet(\n",
    "        FILE_PATH_TEST, \n",
    "        columns=COLUMNS_TO_READ_2025,\n",
    "        engine='pyarrow' \n",
    "    )\n",
    "\n",
    "    df = df.rename(columns=COLUMN_MAPPING2025)\n",
    "\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "    \n",
    "    \n",
    "    print(f\"--- üîç DEBUG: Dati iniziali dal file {os.path.basename(FILE_PATH_TEST)} ---\")\n",
    "    print(\"\\nHead del DataFrame:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nTipi di Dati (Dtypes) dopo la conversione Timestamp:\")\n",
    "    print(df.dtypes)\n",
    "    print(\"----------------------------------------------------------------\\n\")\n",
    "    rows,columns = df.shape\n",
    "    print(f\"Numero di righe: {rows}, Numero di colonne: {columns}\\n\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Errore nel processare il file {FILE_PATH_TEST}: {e}\")\n",
    "\n",
    "\n",
    "COLUMN_MAPPING2024 = {\n",
    "    'MMSI': 'MMSI',\n",
    "    'LAT': 'Latitude',\n",
    "    'LON': 'Longitude',\n",
    "    'SOG': 'SOG',\n",
    "    'COG': 'COG',\n",
    "    'BaseDateTime': 'Timestamp' \n",
    "}\n",
    "COLUMNS_TO_READ_2024 = list(COLUMN_MAPPING2024.keys())\n",
    "\n",
    "try:\n",
    "    df = pd.read_parquet(\n",
    "        FILE_PATH_TEST2, \n",
    "        columns=COLUMNS_TO_READ_2024,\n",
    "        engine='pyarrow' \n",
    "    )\n",
    "\n",
    "    df = df.rename(columns=COLUMN_MAPPING2024)\n",
    "\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "\n",
    "\n",
    "    print(f\"--- üîç DEBUG: Dati iniziali dal file {os.path.basename(FILE_PATH_TEST2)} ---\")\n",
    "    print(\"\\nHead del DataFrame:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nTipi di Dati (Dtypes) dopo la conversione Timestamp:\")\n",
    "    print(df.dtypes)\n",
    "    print(\"----------------------------------------------------------------\\n\")\n",
    "    rows,columns = df.shape\n",
    "    print(f\"Numero di righe: {rows}, Numero di colonne: {columns}\\n\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Errore nel processare il file {FILE_PATH_TEST2}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d6f751",
   "metadata": {},
   "source": [
    "### Pulizia dei dati\n",
    " \n",
    "In questa sezione, iteriamo su ogni file del nostro dataset ed eseguiamo la pulizia vera e propria, applicando dei filtri. Il primo filtro filtro applicato √® sulla lettura delle colonne `COLUMNS_TO_READ` prima di caricare i dati. √à il modo pi√π efficiente per scartare le colonne inutili e riduce drasticamente l'utilizzo della RAM velocizzando l'intero processo.\n",
    "\n",
    "##### Filtri Navigazione Attiva e di Validit√†\n",
    "  \n",
    "Vengono applicati una serie di filtri per lasciare all'interno del dataset solo valori validi e di navigazione attiva:\n",
    "1. Applichiamo il filtro `df = df[df['SOG'] > SOG_MIN_THRESHOLD`, eliminando i dati statici come deciso sopra.\n",
    "2. Applichiamo il filtro `df[df['COG'] != 511]`,rimuovendo i record dove il COG (Course Over Ground) √® $511$. Questo √® un codice standard AIS che significa \"Dato Non Disponibile\". Senza una rotta (COG), l'informazione cinematica √® incompleta e inutile per il modello.\n",
    "3. Applichiamo il filtro `Filtro Lat/Lon (>= -90, <= 90, etc.)`, eliminiamo i record con coordinate geografiche errate (fuori dal globo). Questi sono errori di trasmissione o del sensore che inquinerebbero il dataset.\n",
    "4. Utilizziamo il metodo `df.dopna(...)` per rimuovere qualsiasi riga che abbia valori mancanti. Questo perch√® i modelli LSTM/LNN richiedono input completi per funzionare correttamente.\n",
    "5. Infine l'ultimo filtro √® `df['MMSI'].str.len()==9` per rimuovere i record con l'identificativo della nave non corretto. Questo perch√® l'MMSI deve essere di 9 cifre e questo ci garantisce che ogni traiettoria sia attribuita ad una nave valida.\n",
    "\n",
    "##### Segmentazione e Creazione delle Traiettorie\n",
    "\n",
    "Questa √® la fase finale prima del salvataggio, dove trasformiamo i dati puliti in sequenze coerenti (TrajectoryID).  \n",
    "Quello che andiamo a fare √® raggruppare i nostri dati prima per l'MMSI e poi per il TimeStamp. In questo modo abbiamo i dati ordinati ed  √® possibile delineare quelle che sono le tratiettorie diverse per ogni nave. Viene aggiunta una nuova colonna al dataset che √® `TrajectoryID` che ha il compito di raggruppare tutti i dati di ogni singola nave che fanno riferimento ad un intero spostamento.  \n",
    "Gli spostamenti sono stati delineati assumendo che spostamenti diversi vengono caratterizzati da uno stato di navigazione non attiva di almeno 1 ora.  \n",
    "Questa fase √® essenziale perch√® i modelli che andremo ad addestrare, impareranno non dai singoli punti ma dalle intere sequenze.\n",
    "\n",
    "```\n",
    "df = df.sort_values(by=['MMSI', 'Timestamp']).reset_index(drop=True)\n",
    "\n",
    "            df['TimeDiff'] = df.groupby('MMSI')['Timestamp'].diff()\n",
    "            df['IsNewTraj'] = (df['MMSI'] != df['MMSI'].shift(1)) | (df['TimeDiff'] > TIME_GAP_THRESHOLD)\n",
    "            df['TrajectoryID'] = df['IsNewTraj'].cumsum()\n",
    "```\n",
    "\n",
    "\n",
    "L'ultima parte serve solo per salvare i file .parquet puliti e segmentati nel dataset che poi servir√† per addestrare i modelli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "445983d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ais-2025-02-15.parquet pre-pulito\n",
      "File ais-2025-06-22.parquet pre-pulito\n",
      "File ais-2025-04-26.parquet pre-pulito\n",
      "File AIS_2024_08_21.parquet pre-pulito\n",
      "File ais-2025-01-22.parquet pre-pulito\n",
      "File ais-2025-02-27.parquet pre-pulito\n",
      "File ais-2025-06-18.parquet pre-pulito\n",
      "File ais-2025-06-14.parquet pre-pulito\n",
      "File AIS_2024_08_11.parquet pre-pulito\n",
      "File AIS_2024_08_06.parquet pre-pulito\n",
      "File AIS_2024_08_22.parquet pre-pulito\n",
      "File ais-2025-06-21.parquet pre-pulito\n",
      "File AIS_2024_11_05.parquet pre-pulito\n",
      "File AIS_2024_10_30.parquet pre-pulito\n",
      "File AIS_2024_10_16.parquet pre-pulito\n",
      "File ais-2025-01-23.parquet pre-pulito\n",
      "File ais-2025-03-26.parquet pre-pulito\n",
      "File AIS_2024_09_24.parquet pre-pulito\n",
      "File ais-2025-03-25.parquet pre-pulito\n",
      "File AIS_2024_07_01.parquet pre-pulito\n",
      "File AIS_2024_09_11.parquet pre-pulito\n",
      "File ais-2025-03-08.parquet pre-pulito\n",
      "File AIS_2024_11_21.parquet pre-pulito\n",
      "File ais-2025-04-25.parquet pre-pulito\n",
      "File AIS_2024_08_18.parquet pre-pulito\n",
      "File AIS_2024_12_19.parquet pre-pulito\n",
      "File AIS_2024_12_29.parquet pre-pulito\n",
      "File ais-2025-01-21.parquet pre-pulito\n",
      "File ais-2025-04-03.parquet pre-pulito\n",
      "File ais-2025-04-22.parquet pre-pulito\n",
      "File ais-2025-01-04.parquet pre-pulito\n",
      "File ais-2025-04-11.parquet pre-pulito\n",
      "File AIS_2024_12_05.parquet pre-pulito\n",
      "File AIS_2024_09_30.parquet pre-pulito\n",
      "File AIS_2024_12_28.parquet pre-pulito\n",
      "File AIS_2024_12_26.parquet pre-pulito\n",
      "File AIS_2024_09_21.parquet pre-pulito\n",
      "File ais-2025-05-10.parquet pre-pulito\n",
      "File AIS_2024_12_21.parquet pre-pulito\n",
      "File AIS_2024_07_14.parquet pre-pulito\n",
      "File ais-2025-04-05.parquet pre-pulito\n",
      "File AIS_2024_08_23.parquet pre-pulito\n",
      "File AIS_2024_08_30.parquet pre-pulito\n",
      "File ais-2025-05-01.parquet pre-pulito\n",
      "File AIS_2024_12_31.parquet pre-pulito\n",
      "File AIS_2024_07_11.parquet pre-pulito\n",
      "File AIS_2024_09_25.parquet pre-pulito\n",
      "File AIS_2024_11_17.parquet pre-pulito\n",
      "File AIS_2024_09_29.parquet pre-pulito\n",
      "File AIS_2024_07_27.parquet pre-pulito\n",
      "File ais-2025-06-25.parquet pre-pulito\n",
      "File ais-2025-04-15.parquet pre-pulito\n",
      "File ais-2025-02-20.parquet pre-pulito\n",
      "File AIS_2024_11_15.parquet pre-pulito\n",
      "File ais-2025-05-06.parquet pre-pulito\n",
      "File AIS_2024_09_18.parquet pre-pulito\n",
      "File AIS_2024_12_12.parquet pre-pulito\n",
      "File ais-2025-05-25.parquet pre-pulito\n",
      "File AIS_2024_10_09.parquet pre-pulito\n",
      "File ais-2025-04-18.parquet pre-pulito\n",
      "File ais-2025-02-18.parquet pre-pulito\n",
      "File AIS_2024_11_19.parquet pre-pulito\n",
      "File AIS_2024_11_14.parquet pre-pulito\n",
      "File ais-2025-02-17.parquet pre-pulito\n",
      "File ais-2025-03-16.parquet pre-pulito\n",
      "File ais-2025-06-24.parquet pre-pulito\n",
      "File ais-2025-06-17.parquet pre-pulito\n",
      "File AIS_2024_11_09.parquet pre-pulito\n",
      "File ais-2025-03-07.parquet pre-pulito\n",
      "File AIS_2024_09_17.parquet pre-pulito\n",
      "File AIS_2024_08_28.parquet pre-pulito\n",
      "File AIS_2024_09_01.parquet pre-pulito\n",
      "File ais-2025-01-16.parquet pre-pulito\n",
      "File AIS_2024_11_28.parquet pre-pulito\n",
      "File ais-2025-03-05.parquet pre-pulito\n",
      "File AIS_2024_08_05.parquet pre-pulito\n",
      "File AIS_2024_12_06.parquet pre-pulito\n",
      "File AIS_2024_11_04.parquet pre-pulito\n",
      "File ais-2025-06-28.parquet pre-pulito\n",
      "File ais-2025-05-29.parquet pre-pulito\n",
      "File ais-2025-01-07.parquet pre-pulito\n",
      "File AIS_2024_08_16.parquet pre-pulito\n",
      "File AIS_2024_12_20.parquet pre-pulito\n",
      "File AIS_2024_07_22.parquet pre-pulito\n",
      "File AIS_2024_11_22.parquet pre-pulito\n",
      "File AIS_2024_11_10.parquet pre-pulito\n",
      "File ais-2025-02-06.parquet pre-pulito\n",
      "File ais-2025-05-11.parquet pre-pulito\n",
      "File AIS_2024_12_30.parquet pre-pulito\n",
      "File ais-2025-05-21.parquet pre-pulito\n",
      "File ais-2025-04-06.parquet pre-pulito\n",
      "File AIS_2024_08_25.parquet pre-pulito\n",
      "File AIS_2024_07_07.parquet pre-pulito\n",
      "File AIS_2024_10_25.parquet pre-pulito\n",
      "File AIS_2024_09_06.parquet pre-pulito\n",
      "File ais-2025-05-04.parquet pre-pulito\n",
      "File ais-2025-05-18.parquet pre-pulito\n",
      "File AIS_2024_08_31.parquet pre-pulito\n",
      "File AIS_2024_12_10.parquet pre-pulito\n",
      "File ais-2025-06-11.parquet pre-pulito\n",
      "File AIS_2024_12_22.parquet pre-pulito\n",
      "File AIS_2024_09_13.parquet pre-pulito\n",
      "File AIS_2024_12_23.parquet pre-pulito\n",
      "File AIS_2024_11_01.parquet pre-pulito\n",
      "File ais-2025-06-29.parquet pre-pulito\n",
      "File ais-2025-02-26.parquet pre-pulito\n",
      "File AIS_2024_09_26.parquet pre-pulito\n",
      "File ais-2025-02-25.parquet pre-pulito\n",
      "File ais-2025-01-15.parquet pre-pulito\n",
      "File ais-2025-02-16.parquet pre-pulito\n",
      "File ais-2025-02-23.parquet pre-pulito\n",
      "File AIS_2024_09_22.parquet pre-pulito\n",
      "File ais-2025-05-15.parquet pre-pulito\n",
      "File AIS_2024_12_18.parquet pre-pulito\n",
      "File AIS_2024_10_31.parquet pre-pulito\n",
      "File ais-2025-04-13.parquet pre-pulito\n",
      "File ais-2025-03-03.parquet pre-pulito\n",
      "File ais-2025-03-01.parquet pre-pulito\n",
      "File ais-2025-01-05.parquet pre-pulito\n",
      "File ais-2025-06-16.parquet pre-pulito\n",
      "File ais-2025-02-22.parquet pre-pulito\n",
      "File ais-2025-01-09.parquet pre-pulito\n",
      "File AIS_2024_07_03.parquet pre-pulito\n",
      "File ais-2025-06-12.parquet pre-pulito\n",
      "File AIS_2024_12_11.parquet pre-pulito\n",
      "File AIS_2024_10_18.parquet pre-pulito\n",
      "File ais-2025-01-02.parquet pre-pulito\n",
      "File ais-2025-06-04.parquet pre-pulito\n",
      "File AIS_2024_12_14.parquet pre-pulito\n",
      "File AIS_2024_11_29.parquet pre-pulito\n",
      "File ais-2025-01-17.parquet pre-pulito\n",
      "File AIS_2024_08_13.parquet pre-pulito\n",
      "File ais-2025-02-28.parquet pre-pulito\n",
      "File ais-2025-05-26.parquet pre-pulito\n",
      "File ais-2025-02-11.parquet pre-pulito\n",
      "File ais-2025-06-05.parquet pre-pulito\n",
      "File AIS_2024_11_08.parquet pre-pulito\n",
      "File ais-2025-02-13.parquet pre-pulito\n",
      "File AIS_2024_10_11.parquet pre-pulito\n",
      "File ais-2025-03-31.parquet pre-pulito\n",
      "File ais-2025-05-08.parquet pre-pulito\n",
      "File ais-2025-04-10.parquet pre-pulito\n",
      "File ais-2025-03-19.parquet pre-pulito\n",
      "File ais-2025-02-05.parquet pre-pulito\n",
      "File AIS_2024_08_08.parquet pre-pulito\n",
      "File AIS_2024_09_03.parquet pre-pulito\n",
      "File AIS_2024_07_06.parquet pre-pulito\n",
      "File AIS_2024_11_11.parquet pre-pulito\n",
      "File AIS_2024_11_13.parquet pre-pulito\n",
      "File ais-2025-03-21.parquet pre-pulito\n",
      "File ais-2025-03-04.parquet pre-pulito\n",
      "File ais-2025-06-09.parquet pre-pulito\n",
      "File ais-2025-05-27.parquet pre-pulito\n",
      "File ais-2025-02-19.parquet pre-pulito\n",
      "File ais-2025-01-08.parquet pre-pulito\n",
      "File AIS_2024_12_07.parquet pre-pulito\n",
      "File AIS_2024_09_23.parquet pre-pulito\n",
      "File AIS_2024_10_03.parquet pre-pulito\n",
      "File AIS_2024_11_06.parquet pre-pulito\n",
      "File ais-2025-04-27.parquet pre-pulito\n",
      "File ais-2025-05-30.parquet pre-pulito\n",
      "File AIS_2024_10_12.parquet pre-pulito\n",
      "File AIS_2024_10_07.parquet pre-pulito\n",
      "File ais-2025-06-27.parquet pre-pulito\n",
      "File ais-2025-02-14.parquet pre-pulito\n",
      "File ais-2025-03-27.parquet pre-pulito\n",
      "File ais-2025-01-29.parquet pre-pulito\n",
      "File AIS_2024_12_13.parquet pre-pulito\n",
      "File AIS_2024_08_04.parquet pre-pulito\n",
      "File AIS_2024_07_12.parquet pre-pulito\n",
      "File AIS_2024_07_18.parquet pre-pulito\n",
      "File AIS_2024_11_16.parquet pre-pulito\n",
      "File AIS_2024_07_28.parquet pre-pulito\n",
      "File ais-2025-06-20.parquet pre-pulito\n",
      "File AIS_2024_09_10.parquet pre-pulito\n",
      "File AIS_2024_12_04.parquet pre-pulito\n",
      "File ais-2025-06-19.parquet pre-pulito\n",
      "File ais-2025-01-20.parquet pre-pulito\n",
      "File ais-2025-03-17.parquet pre-pulito\n",
      "File ais-2025-04-04.parquet pre-pulito\n",
      "File AIS_2024_08_01.parquet pre-pulito\n",
      "File AIS_2024_09_02.parquet pre-pulito\n",
      "File AIS_2024_07_13.parquet pre-pulito\n",
      "File ais-2025-03-10.parquet pre-pulito\n",
      "File AIS_2024_07_15.parquet pre-pulito\n",
      "File ais-2025-04-08.parquet pre-pulito\n",
      "File AIS_2024_10_20.parquet pre-pulito\n",
      "File ais-2025-03-28.parquet pre-pulito\n",
      "File ais-2025-03-29.parquet pre-pulito\n",
      "File AIS_2024_09_05.parquet pre-pulito\n",
      "File AIS_2024_08_15.parquet pre-pulito\n",
      "File AIS_2024_09_12.parquet pre-pulito\n",
      "File ais-2025-02-21.parquet pre-pulito\n",
      "File ais-2025-03-11.parquet pre-pulito\n",
      "File AIS_2024_07_25.parquet pre-pulito\n",
      "File AIS_2024_10_21.parquet pre-pulito\n",
      "File AIS_2024_11_12.parquet pre-pulito\n",
      "File ais-2025-05-17.parquet pre-pulito\n",
      "File ais-2025-04-01.parquet pre-pulito\n",
      "File ais-2025-05-09.parquet pre-pulito\n",
      "File AIS_2024_11_27.parquet pre-pulito\n",
      "File ais-2025-01-01.parquet pre-pulito\n",
      "File AIS_2024_07_24.parquet pre-pulito\n",
      "File AIS_2024_09_28.parquet pre-pulito\n",
      "File ais-2025-01-25.parquet pre-pulito\n",
      "File ais-2025-04-28.parquet pre-pulito\n",
      "File AIS_2024_09_14.parquet pre-pulito\n",
      "File AIS_2024_12_25.parquet pre-pulito\n",
      "File ais-2025-04-16.parquet pre-pulito\n",
      "File ais-2025-05-28.parquet pre-pulito\n",
      "File AIS_2024_12_08.parquet pre-pulito\n",
      "File ais-2025-06-03.parquet pre-pulito\n",
      "File ais-2025-04-14.parquet pre-pulito\n",
      "File ais-2025-06-13.parquet pre-pulito\n",
      "File AIS_2024_07_26.parquet pre-pulito\n",
      "File AIS_2024_10_04.parquet pre-pulito\n",
      "File ais-2025-01-18.parquet pre-pulito\n",
      "File ais-2025-04-19.parquet pre-pulito\n",
      "File AIS_2024_11_03.parquet pre-pulito\n",
      "File AIS_2024_12_09.parquet pre-pulito\n",
      "File AIS_2024_07_31.parquet pre-pulito\n",
      "File ais-2025-04-12.parquet pre-pulito\n",
      "File AIS_2024_08_07.parquet pre-pulito\n",
      "File AIS_2024_10_13.parquet pre-pulito\n",
      "File AIS_2024_11_20.parquet pre-pulito\n",
      "File ais-2025-04-20.parquet pre-pulito\n",
      "File ais-2025-06-10.parquet pre-pulito\n",
      "File ais-2025-02-03.parquet pre-pulito\n",
      "File ais-2025-05-03.parquet pre-pulito\n",
      "File ais-2025-04-02.parquet pre-pulito\n",
      "File AIS_2024_07_29.parquet pre-pulito\n",
      "File ais-2025-05-22.parquet pre-pulito\n",
      "File ais-2025-05-02.parquet pre-pulito\n",
      "File AIS_2024_07_19.parquet pre-pulito\n",
      "File ais-2025-02-04.parquet pre-pulito\n",
      "File AIS_2024_07_21.parquet pre-pulito\n",
      "File ais-2025-01-28.parquet pre-pulito\n",
      "File AIS_2024_07_20.parquet pre-pulito\n",
      "File AIS_2024_08_17.parquet pre-pulito\n",
      "File ais-2025-05-24.parquet pre-pulito\n",
      "File ais-2025-01-06.parquet pre-pulito\n",
      "File ais-2025-02-12.parquet pre-pulito\n",
      "File ais-2025-06-30.parquet pre-pulito\n",
      "File AIS_2024_10_15.parquet pre-pulito\n",
      "File ais-2025-05-20.parquet pre-pulito\n",
      "File ais-2025-02-10.parquet pre-pulito\n",
      "File ais-2025-05-12.parquet pre-pulito\n",
      "File AIS_2024_07_05.parquet pre-pulito\n",
      "File ais-2025-03-12.parquet pre-pulito\n",
      "File ais-2025-04-09.parquet pre-pulito\n",
      "File ais-2025-03-06.parquet pre-pulito\n",
      "File ais-2025-01-03.parquet pre-pulito\n",
      "File ais-2025-03-15.parquet pre-pulito\n",
      "File ais-2025-04-17.parquet pre-pulito\n",
      "File AIS_2024_09_19.parquet pre-pulito\n",
      "File AIS_2024_10_02.parquet pre-pulito\n",
      "File AIS_2024_08_29.parquet pre-pulito\n",
      "File ais-2025-01-14.parquet pre-pulito\n",
      "File ais-2025-03-18.parquet pre-pulito\n",
      "File ais-2025-01-13.parquet pre-pulito\n",
      "File AIS_2024_07_17.parquet pre-pulito\n",
      "File AIS_2024_08_12.parquet pre-pulito\n",
      "File AIS_2024_10_10.parquet pre-pulito\n",
      "File AIS_2024_12_03.parquet pre-pulito\n",
      "File AIS_2024_11_26.parquet pre-pulito\n",
      "File AIS_2024_12_15.parquet pre-pulito\n",
      "File AIS_2024_11_02.parquet pre-pulito\n",
      "File AIS_2024_09_07.parquet pre-pulito\n",
      "File ais-2025-06-08.parquet pre-pulito\n",
      "File AIS_2024_07_10.parquet pre-pulito\n",
      "File AIS_2024_08_14.parquet pre-pulito\n",
      "File ais-2025-04-23.parquet pre-pulito\n",
      "File ais-2025-03-24.parquet pre-pulito\n",
      "File ais-2025-04-30.parquet pre-pulito\n",
      "File AIS_2024_07_04.parquet pre-pulito\n",
      "File ais-2025-01-24.parquet pre-pulito\n",
      "File AIS_2024_10_24.parquet pre-pulito\n",
      "File AIS_2024_12_01.parquet pre-pulito\n",
      "File ais-2025-06-07.parquet pre-pulito\n",
      "File ais-2025-06-02.parquet pre-pulito\n",
      "File AIS_2024_08_10.parquet pre-pulito\n",
      "File AIS_2024_09_27.parquet pre-pulito\n",
      "File ais-2025-03-20.parquet pre-pulito\n",
      "File AIS_2024_09_15.parquet pre-pulito\n",
      "File ais-2025-01-10.parquet pre-pulito\n",
      "File AIS_2024_07_08.parquet pre-pulito\n",
      "File ais-2025-06-01.parquet pre-pulito\n",
      "File AIS_2024_12_27.parquet pre-pulito\n",
      "File ais-2025-06-15.parquet pre-pulito\n",
      "File ais-2025-05-07.parquet pre-pulito\n",
      "File AIS_2024_08_24.parquet pre-pulito\n",
      "File ais-2025-02-02.parquet pre-pulito\n",
      "File ais-2025-06-06.parquet pre-pulito\n",
      "File AIS_2024_10_01.parquet pre-pulito\n",
      "File ais-2025-04-24.parquet pre-pulito\n",
      "File AIS_2024_10_19.parquet pre-pulito\n",
      "File ais-2025-02-08.parquet pre-pulito\n",
      "File AIS_2024_07_02.parquet pre-pulito\n",
      "File ais-2025-05-13.parquet pre-pulito\n",
      "File AIS_2024_10_28.parquet pre-pulito\n",
      "File AIS_2024_08_09.parquet pre-pulito\n",
      "File ais-2025-01-11.parquet pre-pulito\n",
      "File ais-2025-05-31.parquet pre-pulito\n",
      "File ais-2025-03-23.parquet pre-pulito\n",
      "File AIS_2024_11_07.parquet pre-pulito\n",
      "File AIS_2024_10_05.parquet pre-pulito\n",
      "File ais-2025-03-02.parquet pre-pulito\n",
      "File ais-2025-01-30.parquet pre-pulito\n",
      "File ais-2025-01-31.parquet pre-pulito\n",
      "File AIS_2024_07_23.parquet pre-pulito\n",
      "File ais-2025-03-22.parquet pre-pulito\n",
      "File ais-2025-01-19.parquet pre-pulito\n",
      "File AIS_2024_12_24.parquet pre-pulito\n",
      "File ais-2025-05-23.parquet pre-pulito\n",
      "File ais-2025-05-05.parquet pre-pulito\n",
      "File ais-2025-02-09.parquet pre-pulito\n",
      "File ais-2025-05-16.parquet pre-pulito\n",
      "File AIS_2024_10_14.parquet pre-pulito\n",
      "File AIS_2024_10_27.parquet pre-pulito\n",
      "File ais-2025-03-14.parquet pre-pulito\n",
      "File AIS_2024_12_17.parquet pre-pulito\n",
      "File AIS_2024_09_08.parquet pre-pulito\n",
      "File AIS_2024_10_08.parquet pre-pulito\n",
      "File AIS_2024_09_16.parquet pre-pulito\n",
      "File AIS_2024_08_03.parquet pre-pulito\n",
      "File AIS_2024_07_30.parquet pre-pulito\n",
      "File AIS_2024_10_23.parquet pre-pulito\n",
      "File AIS_2024_07_16.parquet pre-pulito\n",
      "File AIS_2024_11_18.parquet pre-pulito\n",
      "File ais-2025-03-09.parquet pre-pulito\n",
      "File AIS_2024_08_26.parquet pre-pulito\n",
      "File AIS_2024_10_17.parquet pre-pulito\n",
      "File AIS_2024_10_06.parquet pre-pulito\n",
      "File ais-2025-02-07.parquet pre-pulito\n",
      "File ais-2025-02-01.parquet pre-pulito\n",
      "File AIS_2024_10_29.parquet pre-pulito\n",
      "File ais-2025-05-14.parquet pre-pulito\n",
      "File AIS_2024_10_26.parquet pre-pulito\n",
      "File ais-2025-04-29.parquet pre-pulito\n",
      "File AIS_2024_11_24.parquet pre-pulito\n",
      "File AIS_2024_08_19.parquet pre-pulito\n",
      "File ais-2025-04-07.parquet pre-pulito\n",
      "File AIS_2024_11_23.parquet pre-pulito\n",
      "File AIS_2024_11_30.parquet pre-pulito\n",
      "File ais-2025-02-24.parquet pre-pulito\n",
      "File AIS_2024_11_25.parquet pre-pulito\n",
      "File ais-2025-06-26.parquet pre-pulito\n",
      "File ais-2025-03-13.parquet pre-pulito\n",
      "File AIS_2024_08_27.parquet pre-pulito\n",
      "File AIS_2024_09_09.parquet pre-pulito\n",
      "File AIS_2024_12_02.parquet pre-pulito\n",
      "File ais-2025-06-23.parquet pre-pulito\n",
      "File AIS_2024_07_09.parquet pre-pulito\n",
      "File AIS_2024_08_20.parquet pre-pulito\n",
      "File ais-2025-01-12.parquet pre-pulito\n",
      "File ais-2025-03-30.parquet pre-pulito\n",
      "File ais-2025-04-21.parquet pre-pulito\n",
      "File AIS_2024_08_02.parquet pre-pulito\n",
      "File AIS_2024_10_22.parquet pre-pulito\n",
      "File ais-2025-01-27.parquet pre-pulito\n",
      "File ais-2025-01-26.parquet pre-pulito\n",
      "File AIS_2024_09_20.parquet pre-pulito\n",
      "File AIS_2024_12_16.parquet pre-pulito\n",
      "File AIS_2024_09_04.parquet pre-pulito\n",
      "File ais-2025-05-19.parquet pre-pulito\n",
      "\n",
      "--- FASE 1 (Pre-Pulizia) completata. ---\n"
     ]
    }
   ],
   "source": [
    "MAPPING_2025 = {\n",
    "    'mmsi': 'MMSI', \n",
    "    'latitude': 'Latitude', \n",
    "    'longitude': 'Longitude', \n",
    "    'sog': 'SOG', \n",
    "    'cog': 'COG', \n",
    "    'base_date_time': 'Timestamp' \n",
    "}\n",
    "\n",
    "COLUMNS_2025 = list(MAPPING_2025.keys())\n",
    "\n",
    "MAPPING_2024 = {\n",
    "    'MMSI': 'MMSI',\n",
    "    'LAT': 'Latitude',\n",
    "    'LON': 'Longitude',\n",
    "    'SOG': 'SOG',\n",
    "    'COG': 'COG',\n",
    "    'BaseDateTime': 'Timestamp'\n",
    "}\n",
    "COLUMNS_2024 = list(MAPPING_2024.keys())\n",
    "\n",
    "for file_path in all_files:\n",
    "    df = None\n",
    "    mapping_usato = None\n",
    "\n",
    "    try:\n",
    "        \n",
    "        df = pd.read_parquet(\n",
    "            file_path, \n",
    "            columns=COLUMNS_2025,\n",
    "            engine='pyarrow' \n",
    "        )\n",
    "        df = df.rename(columns=MAPPING_2025)\n",
    "        mapping_usato = \"2025\"\n",
    "    \n",
    "    except Exception as e1:\n",
    "        try:\n",
    "            df = pd.read_parquet(\n",
    "                file_path, \n",
    "                columns=COLUMNS_2024,\n",
    "                engine='pyarrow' \n",
    "            )\n",
    "            df = df.rename(columns=MAPPING_2024)\n",
    "            mapping_usato = \"2024\"\n",
    "        \n",
    "        except Exception as e2:\n",
    "            print(f\"Errore IRRISOLVIBILE nel caricare {file_path}: Schema non riconosciuto.\")\n",
    "            continue\n",
    "\n",
    "    if df is not None:\n",
    "        try:\n",
    "            \n",
    "            df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "            \n",
    "            # Filtri cinematici e geografici\n",
    "            df = df[df['SOG'] > SOG_MIN_THRESHOLD]\n",
    "            df = df[df['COG'] != 511]\n",
    "            df = df[(df['Latitude'] >= -90) & (df['Latitude'] <= 90)]\n",
    "            df = df[(df['Longitude'] >= -180) & (df['Longitude'] <= 180)]\n",
    "            \n",
    "            # Filtri di integrit√†\n",
    "            df = df.dropna(subset=['MMSI', 'Latitude', 'Longitude', 'SOG', 'COG'])\n",
    "            df['MMSI'] = df['MMSI'].astype(str).str.replace(r'\\D', '', regex=True)\n",
    "            df = df[df['MMSI'].str.len() == 9]\n",
    "\n",
    "            if not df.empty:\n",
    "\n",
    "                output_filename = os.path.basename(file_path).lower()\n",
    "                output_file = os.path.join(OUTPUT_DIR, output_filename)\n",
    "                \n",
    "                df.to_parquet(output_file, index=False)\n",
    "                \n",
    "                print(f\"File {os.path.basename(file_path)} pre-pulito\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Errore nella FASE DI PULIZIA per il file {file_path}: {e}\")\n",
    "\n",
    "print(\"\\n--- FASE 1 (Pre-Pulizia) completata. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076e33fe",
   "metadata": {},
   "source": [
    "#### Test file pre-pulizia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65b55661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MMSI</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>SOG</th>\n",
       "      <th>COG</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>368138010</td>\n",
       "      <td>40.47715</td>\n",
       "      <td>-73.84652</td>\n",
       "      <td>5.5</td>\n",
       "      <td>286.9</td>\n",
       "      <td>2025-01-01 00:00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>367188610</td>\n",
       "      <td>27.93936</td>\n",
       "      <td>-82.45703</td>\n",
       "      <td>2.2</td>\n",
       "      <td>147.6</td>\n",
       "      <td>2025-01-01 00:00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>366938780</td>\n",
       "      <td>46.04232</td>\n",
       "      <td>-83.93567</td>\n",
       "      <td>11.8</td>\n",
       "      <td>126.0</td>\n",
       "      <td>2025-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>316028554</td>\n",
       "      <td>49.28782</td>\n",
       "      <td>-123.10689</td>\n",
       "      <td>7.8</td>\n",
       "      <td>215.6</td>\n",
       "      <td>2025-01-01 00:00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>338122081</td>\n",
       "      <td>37.78262</td>\n",
       "      <td>-122.38452</td>\n",
       "      <td>3.7</td>\n",
       "      <td>196.6</td>\n",
       "      <td>2025-01-01 00:00:12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        MMSI  Latitude  Longitude   SOG    COG           Timestamp\n",
       "0  368138010  40.47715  -73.84652   5.5  286.9 2025-01-01 00:00:02\n",
       "1  367188610  27.93936  -82.45703   2.2  147.6 2025-01-01 00:00:04\n",
       "2  366938780  46.04232  -83.93567  11.8  126.0 2025-01-01 00:00:00\n",
       "3  316028554  49.28782 -123.10689   7.8  215.6 2025-01-01 00:00:06\n",
       "4  338122081  37.78262 -122.38452   3.7  196.6 2025-01-01 00:00:12"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRE_CLEANED_FILE_PATH_TEST = 'Dataset_Pre-Cleaned_AIS/ais-2025-01-01.parquet'\n",
    "COLUMNS_TO_READ_2025 = ['MMSI', 'Latitude', 'Longitude','SOG', 'COG', 'Timestamp']\n",
    "\n",
    "df = pd.read_parquet(\n",
    "        PRE_CLEANED_FILE_PATH_TEST, \n",
    "        columns=COLUMNS_TO_READ_2025,\n",
    "        engine='pyarrow' \n",
    "    )\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78c1631",
   "metadata": {},
   "source": [
    "#### Unificazione in un unico file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efbdc5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ricerca file pre-puliti in: Dataset_Pre-Cleaned_AIS\n",
      "Trovati 365 file, raggruppati in 25 blocchi da 15 giorni.\n",
      "\n",
      "--- Inizio elaborazione Blocco 1/25 ---\n",
      "Caricamento di 15 file...\n",
      "Blocco caricato. Righe: 39966991. Memoria: 3.65 GB\n",
      "Inizio ordinamento...\n",
      "Inizio calcolo TimeDiff...\n",
      "Inizio calcolo IsNewTraj...\n",
      "Inizio calcolo TrajectoryID...\n",
      "Salvataggio in corso...\n",
      "--- ‚úÖ Blocco 1 salvato! (Max ID: 333169) ---\n",
      "\n",
      "--- Inizio elaborazione Blocco 2/25 ---\n",
      "Caricamento di 15 file...\n",
      "Blocco caricato. Righe: 40481433. Memoria: 3.69 GB\n",
      "Inizio ordinamento...\n",
      "Inizio calcolo TimeDiff...\n",
      "Inizio calcolo IsNewTraj...\n",
      "Inizio calcolo TrajectoryID...\n",
      "Salvataggio in corso...\n",
      "--- ‚úÖ Blocco 2 salvato! (Max ID: 672634) ---\n",
      "\n",
      "--- Inizio elaborazione Blocco 3/25 ---\n",
      "Caricamento di 15 file...\n",
      "Blocco caricato. Righe: 41344475. Memoria: 3.77 GB\n",
      "Inizio ordinamento...\n",
      "Inizio calcolo TimeDiff...\n",
      "Inizio calcolo IsNewTraj...\n",
      "Inizio calcolo TrajectoryID...\n",
      "Salvataggio in corso...\n",
      "--- ‚úÖ Blocco 3 salvato! (Max ID: 1011871) ---\n",
      "\n",
      "--- Inizio elaborazione Blocco 4/25 ---\n",
      "Caricamento di 15 file...\n",
      "Blocco caricato. Righe: 40121023. Memoria: 3.66 GB\n",
      "Inizio ordinamento...\n",
      "Inizio calcolo TimeDiff...\n",
      "Inizio calcolo IsNewTraj...\n",
      "Inizio calcolo TrajectoryID...\n",
      "Salvataggio in corso...\n",
      "--- ‚úÖ Blocco 4 salvato! (Max ID: 1352084) ---\n",
      "\n",
      "--- Inizio elaborazione Blocco 5/25 ---\n",
      "Caricamento di 15 file...\n",
      "Blocco caricato. Righe: 36820811. Memoria: 3.36 GB\n",
      "Inizio ordinamento...\n",
      "Inizio calcolo TimeDiff...\n",
      "Inizio calcolo IsNewTraj...\n",
      "Inizio calcolo TrajectoryID...\n",
      "Salvataggio in corso...\n",
      "--- ‚úÖ Blocco 5 salvato! (Max ID: 1666464) ---\n",
      "\n",
      "--- Inizio elaborazione Blocco 6/25 ---\n",
      "Caricamento di 15 file...\n",
      "Blocco caricato. Righe: 32966161. Memoria: 3.01 GB\n",
      "Inizio ordinamento...\n",
      "Inizio calcolo TimeDiff...\n",
      "Inizio calcolo IsNewTraj...\n",
      "Inizio calcolo TrajectoryID...\n",
      "Salvataggio in corso...\n",
      "--- ‚úÖ Blocco 6 salvato! (Max ID: 1967927) ---\n",
      "\n",
      "--- Inizio elaborazione Blocco 7/25 ---\n",
      "Caricamento di 15 file...\n",
      "Blocco caricato. Righe: 31917986. Memoria: 2.91 GB\n",
      "Inizio ordinamento...\n",
      "Inizio calcolo TimeDiff...\n",
      "Inizio calcolo IsNewTraj...\n",
      "Inizio calcolo TrajectoryID...\n",
      "Salvataggio in corso...\n",
      "--- ‚úÖ Blocco 7 salvato! (Max ID: 2251460) ---\n",
      "\n",
      "--- Inizio elaborazione Blocco 8/25 ---\n",
      "Caricamento di 15 file...\n",
      "Blocco caricato. Righe: 31242882. Memoria: 2.85 GB\n",
      "Inizio ordinamento...\n",
      "Inizio calcolo TimeDiff...\n",
      "Inizio calcolo IsNewTraj...\n",
      "Inizio calcolo TrajectoryID...\n",
      "Salvataggio in corso...\n",
      "--- ‚úÖ Blocco 8 salvato! (Max ID: 2524584) ---\n",
      "\n",
      "--- Inizio elaborazione Blocco 9/25 ---\n",
      "Caricamento di 15 file...\n",
      "Blocco caricato. Righe: 28871022. Memoria: 2.64 GB\n",
      "Inizio ordinamento...\n",
      "Inizio calcolo TimeDiff...\n",
      "Inizio calcolo IsNewTraj...\n",
      "Inizio calcolo TrajectoryID...\n",
      "Salvataggio in corso...\n",
      "--- ‚úÖ Blocco 9 salvato! (Max ID: 2776683) ---\n",
      "\n",
      "--- Inizio elaborazione Blocco 10/25 ---\n",
      "Caricamento di 15 file...\n",
      "Blocco caricato. Righe: 27624638. Memoria: 2.52 GB\n",
      "Inizio ordinamento...\n",
      "Inizio calcolo TimeDiff...\n",
      "Inizio calcolo IsNewTraj...\n",
      "Inizio calcolo TrajectoryID...\n",
      "Salvataggio in corso...\n",
      "--- ‚úÖ Blocco 10 salvato! (Max ID: 3040930) ---\n",
      "\n",
      "--- Inizio elaborazione Blocco 11/25 ---\n",
      "Caricamento di 15 file...\n",
      "Blocco caricato. Righe: 26816402. Memoria: 2.45 GB\n",
      "Inizio ordinamento...\n",
      "Inizio calcolo TimeDiff...\n",
      "Inizio calcolo IsNewTraj...\n",
      "Inizio calcolo TrajectoryID...\n",
      "Salvataggio in corso...\n",
      "--- ‚úÖ Blocco 11 salvato! (Max ID: 3282227) ---\n",
      "\n",
      "--- Inizio elaborazione Blocco 12/25 ---\n",
      "Caricamento di 15 file...\n",
      "Blocco caricato. Righe: 23406324. Memoria: 2.14 GB\n",
      "Inizio ordinamento...\n",
      "Inizio calcolo TimeDiff...\n",
      "Inizio calcolo IsNewTraj...\n",
      "Inizio calcolo TrajectoryID...\n",
      "Salvataggio in corso...\n",
      "--- ‚úÖ Blocco 12 salvato! (Max ID: 3510473) ---\n",
      "\n",
      "--- Inizio elaborazione Blocco 13/25 ---\n",
      "Caricamento di 15 file...\n",
      "Blocco caricato. Righe: 20795633. Memoria: 1.90 GB\n",
      "Inizio ordinamento...\n",
      "Inizio calcolo TimeDiff...\n",
      "Inizio calcolo IsNewTraj...\n",
      "Inizio calcolo TrajectoryID...\n",
      "Salvataggio in corso...\n",
      "--- ‚úÖ Blocco 13 salvato! (Max ID: 3731763) ---\n",
      "\n",
      "--- Inizio elaborazione Blocco 14/25 ---\n",
      "Caricamento di 15 file...\n",
      "Blocco caricato. Righe: 20730500. Memoria: 1.89 GB\n",
      "Inizio ordinamento...\n",
      "Inizio calcolo TimeDiff...\n",
      "Inizio calcolo IsNewTraj...\n",
      "Inizio calcolo TrajectoryID...\n",
      "Salvataggio in corso...\n",
      "--- ‚úÖ Blocco 14 salvato! (Max ID: 3947060) ---\n",
      "\n",
      "--- Inizio elaborazione Blocco 15/25 ---\n",
      "Caricamento di 15 file...\n",
      "Blocco caricato. Righe: 22152986. Memoria: 2.02 GB\n",
      "Inizio ordinamento...\n",
      "Inizio calcolo TimeDiff...\n",
      "Inizio calcolo IsNewTraj...\n",
      "Inizio calcolo TrajectoryID...\n",
      "Salvataggio in corso...\n",
      "--- ‚úÖ Blocco 15 salvato! (Max ID: 4183828) ---\n",
      "\n",
      "--- Inizio elaborazione Blocco 16/25 ---\n",
      "Caricamento di 15 file...\n",
      "Blocco caricato. Righe: 21197425. Memoria: 1.93 GB\n",
      "Inizio ordinamento...\n",
      "Inizio calcolo TimeDiff...\n",
      "Inizio calcolo IsNewTraj...\n",
      "Inizio calcolo TrajectoryID...\n",
      "Salvataggio in corso...\n",
      "--- ‚úÖ Blocco 16 salvato! (Max ID: 4411146) ---\n",
      "\n",
      "--- Inizio elaborazione Blocco 17/25 ---\n",
      "Caricamento di 15 file...\n",
      "Blocco caricato. Righe: 25491484. Memoria: 2.33 GB\n",
      "Inizio ordinamento...\n",
      "Inizio calcolo TimeDiff...\n",
      "Inizio calcolo IsNewTraj...\n",
      "Inizio calcolo TrajectoryID...\n",
      "Salvataggio in corso...\n",
      "--- ‚úÖ Blocco 17 salvato! (Max ID: 4655667) ---\n",
      "\n",
      "--- Inizio elaborazione Blocco 18/25 ---\n",
      "Caricamento di 15 file...\n",
      "Blocco caricato. Righe: 27263663. Memoria: 2.49 GB\n",
      "Inizio ordinamento...\n",
      "Inizio calcolo TimeDiff...\n",
      "Inizio calcolo IsNewTraj...\n",
      "Inizio calcolo TrajectoryID...\n",
      "Salvataggio in corso...\n",
      "--- ‚úÖ Blocco 18 salvato! (Max ID: 4904156) ---\n",
      "\n",
      "--- Inizio elaborazione Blocco 19/25 ---\n",
      "Caricamento di 15 file...\n",
      "Blocco caricato. Righe: 26518021. Memoria: 2.42 GB\n",
      "Inizio ordinamento...\n",
      "Inizio calcolo TimeDiff...\n",
      "Inizio calcolo IsNewTraj...\n",
      "Inizio calcolo TrajectoryID...\n",
      "Salvataggio in corso...\n",
      "--- ‚úÖ Blocco 19 salvato! (Max ID: 5161279) ---\n",
      "\n",
      "--- Inizio elaborazione Blocco 20/25 ---\n",
      "Caricamento di 15 file...\n",
      "Blocco caricato. Righe: 29799414. Memoria: 2.72 GB\n",
      "Inizio ordinamento...\n",
      "Inizio calcolo TimeDiff...\n",
      "Inizio calcolo IsNewTraj...\n",
      "Inizio calcolo TrajectoryID...\n",
      "Salvataggio in corso...\n",
      "--- ‚úÖ Blocco 20 salvato! (Max ID: 5435857) ---\n",
      "\n",
      "--- Inizio elaborazione Blocco 21/25 ---\n",
      "Caricamento di 15 file...\n",
      "Blocco caricato. Righe: 32693600. Memoria: 2.98 GB\n",
      "Inizio ordinamento...\n",
      "Inizio calcolo TimeDiff...\n",
      "Inizio calcolo IsNewTraj...\n",
      "Inizio calcolo TrajectoryID...\n",
      "Salvataggio in corso...\n",
      "--- ‚úÖ Blocco 21 salvato! (Max ID: 5731629) ---\n",
      "\n",
      "--- Inizio elaborazione Blocco 22/25 ---\n",
      "Caricamento di 15 file...\n",
      "Blocco caricato. Righe: 33569176. Memoria: 3.06 GB\n",
      "Inizio ordinamento...\n",
      "Inizio calcolo TimeDiff...\n",
      "Inizio calcolo IsNewTraj...\n",
      "Inizio calcolo TrajectoryID...\n",
      "Salvataggio in corso...\n",
      "--- ‚úÖ Blocco 22 salvato! (Max ID: 6055794) ---\n",
      "\n",
      "--- Inizio elaborazione Blocco 23/25 ---\n",
      "Caricamento di 15 file...\n",
      "Blocco caricato. Righe: 33191929. Memoria: 3.03 GB\n",
      "Inizio ordinamento...\n",
      "Inizio calcolo TimeDiff...\n",
      "Inizio calcolo IsNewTraj...\n",
      "Inizio calcolo TrajectoryID...\n",
      "Salvataggio in corso...\n",
      "--- ‚úÖ Blocco 23 salvato! (Max ID: 6362322) ---\n",
      "\n",
      "--- Inizio elaborazione Blocco 24/25 ---\n",
      "Caricamento di 15 file...\n",
      "Blocco caricato. Righe: 34452113. Memoria: 3.14 GB\n",
      "Inizio ordinamento...\n",
      "Inizio calcolo TimeDiff...\n",
      "Inizio calcolo IsNewTraj...\n",
      "Inizio calcolo TrajectoryID...\n",
      "Salvataggio in corso...\n",
      "--- ‚úÖ Blocco 24 salvato! (Max ID: 6693665) ---\n",
      "\n",
      "--- Inizio elaborazione Blocco 25/25 ---\n",
      "Caricamento di 5 file...\n",
      "Blocco caricato. Righe: 12309221. Memoria: 1.12 GB\n",
      "Inizio ordinamento...\n",
      "Inizio calcolo TimeDiff...\n",
      "Inizio calcolo IsNewTraj...\n",
      "Inizio calcolo TrajectoryID...\n",
      "Salvataggio in corso...\n",
      "--- ‚úÖ Blocco 25 salvato! (Max ID: 6809680) ---\n",
      "\n",
      "\n",
      "--- üéâ FASE A (Blocchi 15 Giorni) completata! ---\n",
      "I file intermedi sono in: /home/al3th3ia/Scrivania/Cybersecurity/Detecting-Trajectory-Spoofing-Attacks-on-AIS/Progetto/Pre-Elaborazione Dati/Dataset_Segmentato_15Giorni\n",
      "\n",
      "Ora sei pronto per la FASE B (Stitching).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import math # Per dividere in blocchi\n",
    "\n",
    "# --- 1. CONFIGURAZIONE ---\n",
    "INPUT_DIR = 'Dataset_Pre-Cleaned_AIS' \n",
    "SCRIPT_DIR = os.getcwd()\n",
    "\n",
    "# ‚≠êÔ∏è Output per i blocchi da 15 giorni\n",
    "OUTPUT_DIR_NAME = 'Dataset_Segmentato_15Giorni' \n",
    "OUTPUT_DIR = os.path.join(SCRIPT_DIR, OUTPUT_DIR_NAME)\n",
    "\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    shutil.rmtree(OUTPUT_DIR)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "TIME_GAP_THRESHOLD = pd.Timedelta(hours=1)\n",
    "# ‚≠êÔ∏è MODIFICA CHIAVE: Come hai suggerito!\n",
    "GIORNI_PER_BLOCCO = 15\n",
    "\n",
    "# --- 2. TROVA E RAGGRUPPA FILE PER BLOCCHI ---\n",
    "print(f\"Ricerca file pre-puliti in: {INPUT_DIR}\")\n",
    "all_files = glob.glob(os.path.join(INPUT_DIR, '*.parquet'))\n",
    "all_files.sort() # Fondamentale per ordinare i giorni!\n",
    "\n",
    "num_blocchi = math.ceil(len(all_files) / GIORNI_PER_BLOCCO)\n",
    "print(f\"Trovati {len(all_files)} file, raggruppati in {num_blocchi} blocchi da 15 giorni.\")\n",
    "\n",
    "# --- 3. LOOP DI ELABORAZIONE \"BLOCCHI 15 GIORNI\" (SOLO PANDAS) ---\n",
    "\n",
    "max_trajectory_id_globale = 0 \n",
    "\n",
    "for i in range(num_blocchi):\n",
    "    start_index = i * GIORNI_PER_BLOCCO\n",
    "    end_index = (i + 1) * GIORNI_PER_BLOCCO\n",
    "    \n",
    "    file_list_blocco = all_files[start_index:end_index]\n",
    "    \n",
    "    print(f\"\\n--- Inizio elaborazione Blocco {i+1}/{num_blocchi} ---\")\n",
    "    \n",
    "    try:\n",
    "        # --- FASE A: Carica il blocco in RAM ---\n",
    "        print(f\"Caricamento di {len(file_list_blocco)} file...\")\n",
    "        \n",
    "        df_list = [pd.read_parquet(f) for f in file_list_blocco]\n",
    "        df_blocco = pd.concat(df_list, ignore_index=True)\n",
    "        \n",
    "        print(f\"Blocco caricato. Righe: {len(df_blocco)}. Memoria: {df_blocco.memory_usage(deep=True).sum() / (1024**3):.2f} GB\")\n",
    "\n",
    "        # --- FASE B: Segmentazione (in RAM) ---\n",
    "        print(\"Inizio ordinamento...\")\n",
    "        df_blocco = df_blocco.sort_values(by=['MMSI', 'Timestamp']).reset_index(drop=True)\n",
    "        \n",
    "        print(\"Inizio calcolo TimeDiff...\")\n",
    "        df_blocco['TimeDiff'] = df_blocco.groupby('MMSI')['Timestamp'].diff()\n",
    "        \n",
    "        print(\"Inizio calcolo IsNewTraj...\")\n",
    "        df_blocco['IsNewTraj'] = (df_blocco['MMSI'] != df_blocco['MMSI'].shift(1)) | (df_blocco['TimeDiff'] > TIME_GAP_THRESHOLD)\n",
    "        \n",
    "        print(\"Inizio calcolo TrajectoryID...\")\n",
    "        # ‚≠êÔ∏è CORREZIONE DEL BUG 'bool + int' ‚≠êÔ∏è\n",
    "        df_blocco['IsNewTraj_int'] = df_blocco['IsNewTraj'].astype(int)\n",
    "        df_blocco['TrajectoryID'] = df_blocco['IsNewTraj_int'].cumsum() + max_trajectory_id_globale\n",
    "        \n",
    "        # --- FASE C: Salvataggio ---\n",
    "        print(\"Salvataggio in corso...\")\n",
    "        df_blocco = df_blocco.drop(columns=['TimeDiff', 'IsNewTraj', 'IsNewTraj_int'])\n",
    "        \n",
    "        max_trajectory_id_globale = df_blocco['TrajectoryID'].max()\n",
    "        \n",
    "        output_file = os.path.join(OUTPUT_DIR, f\"blocco_{i:03d}-segmentato.parquet\")\n",
    "        df_blocco.to_parquet(output_file, index=False, engine='pyarrow', compression='snappy')\n",
    "        \n",
    "        print(f\"--- ‚úÖ Blocco {i+1} salvato! (Max ID: {max_trajectory_id_globale}) ---\")\n",
    "\n",
    "    except MemoryError:\n",
    "        print(f\"--- ‚ùå ERRORE DI MEMORIA: Blocco {i+1} (15 giorni) √® ancora troppo grande! ---\")\n",
    "        print(\"Riprova con GIORNI_PER_BLOCCO = 7.\")\n",
    "        break \n",
    "    except Exception as e:\n",
    "        print(f\"--- ‚ùå ERRORE SCONOSCIUTO nel blocco {i+1}: {e} ---\")\n",
    "\n",
    "print(\"\\n\\n--- üéâ FASE A (Blocchi 15 Giorni) completata! ---\")\n",
    "print(f\"I file intermedi sono in: {OUTPUT_DIR}\")\n",
    "print(\"\\nOra sei pronto per la FASE B (Stitching).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6398b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FASE B: Stitching Sequenziale avviato ---\n",
      "Lettura blocchi da: Dataset_Segmentato_15Giorni\n",
      "Salvataggio finale in: /home/al3th3ia/Scrivania/Cybersecurity/Detecting-Trajectory-Spoofing-Attacks-on-AIS/Progetto/Pre-Elaborazione Dati/Dataset_Stitched_Finale\n",
      "\n",
      "Blocco 0 (blocco_000-segmentato.parquet) copiato, nessuna correzione necessaria.\n",
      "\n",
      "--- Inizio cucitura: blocco_000-segmentato.parquet -> blocco_001-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 3127 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_001-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_001-segmentato.parquet -> blocco_002-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2913 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_002-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_002-segmentato.parquet -> blocco_003-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 3666 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_003-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_003-segmentato.parquet -> blocco_004-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 3450 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_004-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_004-segmentato.parquet -> blocco_005-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 3353 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_005-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_005-segmentato.parquet -> blocco_006-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2838 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_006-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_006-segmentato.parquet -> blocco_007-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 3095 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_007-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_007-segmentato.parquet -> blocco_008-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2501 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_008-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_008-segmentato.parquet -> blocco_009-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2540 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_009-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_009-segmentato.parquet -> blocco_010-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2657 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_010-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_010-segmentato.parquet -> blocco_011-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2115 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_011-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_011-segmentato.parquet -> blocco_012-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2019 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_012-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_012-segmentato.parquet -> blocco_013-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2353 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_013-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_013-segmentato.parquet -> blocco_014-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 1890 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_014-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_014-segmentato.parquet -> blocco_015-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2154 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_015-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_015-segmentato.parquet -> blocco_016-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2295 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_016-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_016-segmentato.parquet -> blocco_017-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2602 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_017-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_017-segmentato.parquet -> blocco_018-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2559 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_018-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_018-segmentato.parquet -> blocco_019-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2847 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_019-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_019-segmentato.parquet -> blocco_020-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2463 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_020-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_020-segmentato.parquet -> blocco_021-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2449 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_021-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_021-segmentato.parquet -> blocco_022-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 3181 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_022-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_022-segmentato.parquet -> blocco_023-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 3053 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_023-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_023-segmentato.parquet -> blocco_024-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 3396 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_024-segmentato.parquet corretto e salvato.\n",
      "\n",
      "\n",
      "--- üéâ FASE B (Stitching Sequenziale) completata! ---\n",
      "Dataset perfetto e cucito salvato in: /home/al3th3ia/Scrivania/Cybersecurity/Detecting-Trajectory-Spoofing-Attacks-on-AIS/Progetto/Pre-Elaborazione Dati/Dataset_Stitched_Finale\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import gc\n",
    "\n",
    "# --- 1. CONFIGURAZIONE ---\n",
    "INPUT_DIR = 'Dataset_Segmentato_15Giorni' \n",
    "OUTPUT_DIR_NAME = 'Dataset_Stitched_Finale' \n",
    "OUTPUT_DIR = os.path.join(os.getcwd(), OUTPUT_DIR_NAME)\n",
    "\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    shutil.rmtree(OUTPUT_DIR)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "TIME_GAP_THRESHOLD = pd.Timedelta(hours=1)\n",
    "\n",
    "print(f\"--- FASE B: Stitching Sequenziale avviato ---\")\n",
    "print(f\"Lettura blocchi da: {INPUT_DIR}\")\n",
    "print(f\"Salvataggio finale in: {OUTPUT_DIR}\\n\")\n",
    "\n",
    "all_blocks = sorted(glob.glob(os.path.join(INPUT_DIR, '*.parquet')))\n",
    "\n",
    "if not all_blocks:\n",
    "    print(f\"ERRORE: Nessun file blocco trovato in {INPUT_DIR}.\")\n",
    "else:\n",
    "    # --- PASSAGGIO 1: Copia il primo blocco ---\n",
    "    # Il primo blocco (blocco_000) non ha correzioni, √® gi√† perfetto.\n",
    "    # Lo copiamo e lo usiamo come punto di partenza.\n",
    "    \n",
    "    path_A_corretto = all_blocks[0]\n",
    "    output_path_A = os.path.join(OUTPUT_DIR, os.path.basename(path_A_corretto))\n",
    "    shutil.copy(path_A_corretto, output_path_A)\n",
    "    print(f\"Blocco 0 ({os.path.basename(path_A_corretto)}) copiato, nessuna correzione necessaria.\")\n",
    "\n",
    "    # --- PASSAGGIO 2: Loop di aggiornamento sequenziale ---\n",
    "    # path_A_corretto ora punta al file *gi√† salvato* e corretto nella cartella di output\n",
    "    \n",
    "    for i in range(len(all_blocks) - 1):\n",
    "        # A = Blocco N (gi√† corretto, nella cartella OUTPUT)\n",
    "        # B = Blocco N+1 (grezzo, dalla cartella INPUT)\n",
    "        \n",
    "        path_A_corretto = os.path.join(OUTPUT_DIR, os.path.basename(all_blocks[i]))\n",
    "        path_B_grezzo = all_blocks[i+1]\n",
    "        \n",
    "        print(f\"\\n--- Inizio cucitura: {os.path.basename(path_A_corretto)} -> {os.path.basename(path_B_grezzo)} ---\")\n",
    "        \n",
    "        try:\n",
    "            # 1. Carica A (corretto) e B (grezzo)\n",
    "            df_A = pd.read_parquet(path_A_corretto)\n",
    "            df_B = pd.read_parquet(path_B_grezzo)\n",
    "            \n",
    "            # 2. Trova la mappa (A_corretto -> B_grezzo)\n",
    "            print(\"  Trovati confini, calcolo mappa...\")\n",
    "            last_records_A = df_A.loc[df_A.groupby('MMSI')['Timestamp'].idxmax()]\n",
    "            last_records_A = last_records_A[['MMSI', 'Timestamp', 'TrajectoryID']].rename(\n",
    "                columns={'Timestamp': 'Last_Timestamp', 'TrajectoryID': 'Correct_ID'}\n",
    "            )\n",
    "\n",
    "            first_records_B = df_B.loc[df_B.groupby('MMSI')['Timestamp'].idxmin()]\n",
    "            first_records_B = first_records_B[['MMSI', 'Timestamp', 'TrajectoryID']].rename(\n",
    "                columns={'Timestamp': 'First_Timestamp', 'TrajectoryID': 'Old_ID'}\n",
    "            )\n",
    "\n",
    "            boundary_check = pd.merge(last_records_A, first_records_B, on='MMSI')\n",
    "            boundary_check['TimeDiff'] = boundary_check['First_Timestamp'] - boundary_check['Last_Timestamp']\n",
    "            stitch_candidates = boundary_check[boundary_check['TimeDiff'] <= TIME_GAP_THRESHOLD]\n",
    "            \n",
    "            # Mappa di correzione: {ID_sbagliato_in_B -> ID_corretto_di_A}\n",
    "            local_fix_map = stitch_candidates.set_index('Old_ID')['Correct_ID'].to_dict()\n",
    "            print(f\"  -> Trovate {len(local_fix_map)} cuciture da applicare.\")\n",
    "\n",
    "            # 3. Libera A dalla memoria (come hai suggerito tu)\n",
    "            print(\"  Rilascio memoria Blocco A...\")\n",
    "            del df_A, last_records_A, first_records_B, boundary_check, stitch_candidates\n",
    "            gc.collect()\n",
    "\n",
    "            # 4. Applica la mappa a B (Memoria: B + B_copia)\n",
    "            print(\"  Applicazione correzioni a Blocco B...\")\n",
    "            # map() √® pi√π veloce di replace() per questo compito\n",
    "            df_B['TrajectoryID'] = df_B['TrajectoryID'].map(local_fix_map).fillna(df_B['TrajectoryID']).astype(int)\n",
    "\n",
    "            # 5. Salva B corretto\n",
    "            output_path_B = os.path.join(OUTPUT_DIR, os.path.basename(path_B_grezzo))\n",
    "            df_B.to_parquet(output_path_B, index=False, engine='pyarrow', compression='snappy')\n",
    "            print(f\"  -> Blocco {os.path.basename(output_path_B)} corretto e salvato.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  -> ‚ùå ERRORE durante la cucitura: {e}\")\n",
    "            break # Interrompi se qualcosa va storto\n",
    "        finally:\n",
    "            # Pulizia finale del loop\n",
    "            if 'df_A' in locals(): del df_A\n",
    "            if 'df_B' in locals(): del df_B\n",
    "            gc.collect()\n",
    "\n",
    "    print(\"\\n\\n--- üéâ FASE B (Stitching Sequenziale) completata! ---\")\n",
    "    print(f\"Dataset perfetto e cucito salvato in: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea250b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üîç SCRIPT DI VERIFICA FINALE (Controllo ID) ---\n",
      "Lettura blocchi da: Dataset_Stitched_Finale\n",
      "\n",
      "--- Inizio scansione dei confini... ---\n",
      "Confine 1: Trovati 3127 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 2: Trovati 2913 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 3: Trovati 3666 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 4: Trovati 3450 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 5: Trovati 3353 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 6: Trovati 2838 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 7: Trovati 3095 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 8: Trovati 2501 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 9: Trovati 2540 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 10: Trovati 2657 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 11: Trovati 2115 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 12: Trovati 2019 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 13: Trovati 2353 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 14: Trovati 1890 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 15: Trovati 2154 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 16: Trovati 2295 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 17: Trovati 2602 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 18: Trovati 2559 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 19: Trovati 2847 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 20: Trovati 2463 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 21: Trovati 2449 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 22: Trovati 3181 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 23: Trovati 3053 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 24: Trovati 3396 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "\n",
      "--- ‚úÖ VERIFICA COMPLETATA ---\n",
      "RISULTATO FINALE: Trovate 0 cuciture mancate.\n",
      "üéâ CONGRATULAZIONI: Il dataset √® perfettamente cucito!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import gc # Importa il Garbage Collector\n",
    "\n",
    "# --- 1. CONFIGURAZIONE ---\n",
    "# ‚≠êÔ∏è Punta alla cartella finale cucita\n",
    "INPUT_DIR = 'Dataset_Stitched_Finale' \n",
    "TIME_GAP_THRESHOLD = pd.Timedelta(hours=1)\n",
    "\n",
    "print(f\"--- üîç SCRIPT DI VERIFICA FINALE (Controllo ID) ---\")\n",
    "print(f\"Lettura blocchi da: {INPUT_DIR}\\n\")\n",
    "\n",
    "all_blocks = sorted(glob.glob(os.path.join(INPUT_DIR, '*.parquet')))\n",
    "\n",
    "if not all_blocks:\n",
    "    print(f\"ERRORE: Nessun file blocco trovato in {INPUT_DIR}.\")\n",
    "else:\n",
    "    total_missed_stitches = 0 # Contatore per le cuciture mancate\n",
    "\n",
    "    print(\"--- Inizio scansione dei confini... ---\")\n",
    "\n",
    "    for i in range(len(all_blocks) - 1):\n",
    "        path_A = all_blocks[i]\n",
    "        path_B = all_blocks[i+1]\n",
    "        \n",
    "        try:\n",
    "            df_A = pd.read_parquet(path_A)\n",
    "            df_B = pd.read_parquet(path_B)\n",
    "\n",
    "            # 1. Trova l'ULTIMO record di A (con il suo ID corretto)\n",
    "            last_records_A = df_A.loc[df_A.groupby('MMSI')['Timestamp'].idxmax()]\n",
    "            last_records_A = last_records_A[['MMSI', 'Timestamp', 'TrajectoryID']].rename(\n",
    "                columns={'Timestamp': 'Last_Timestamp', 'TrajectoryID': 'ID_A'}\n",
    "            )\n",
    "\n",
    "            # 2. Trova il PRIMO record di B (con il suo ID (si spera) corretto)\n",
    "            first_records_B = df_B.loc[df_B.groupby('MMSI')['Timestamp'].idxmin()]\n",
    "            first_records_B = first_records_B[['MMSI', 'Timestamp', 'TrajectoryID']].rename(\n",
    "                columns={'Timestamp': 'First_Timestamp', 'TrajectoryID': 'ID_B'}\n",
    "            )\n",
    "\n",
    "            # 3. Unisci i confini\n",
    "            boundary_check = pd.merge(last_records_A, first_records_B, on='MMSI')\n",
    "\n",
    "            # 4. Calcola il TimeDiff\n",
    "            boundary_check['TimeDiff'] = boundary_check['First_Timestamp'] - boundary_check['Last_Timestamp']\n",
    "\n",
    "            # 5. Trova solo i gap che DOVEVANO essere cuciti (<= 1 ora)\n",
    "            stitchable_gaps = boundary_check[boundary_check['TimeDiff'] <= TIME_GAP_THRESHOLD]\n",
    "            \n",
    "            if not stitchable_gaps.empty:\n",
    "                # 6. VERIFICA: Trova se qualcuno di questi ha ID diversi\n",
    "                missed_stitches = stitchable_gaps[stitchable_gaps['ID_A'] != stitchable_gaps['ID_B']]\n",
    "                \n",
    "                local_missed_count = len(missed_stitches)\n",
    "                total_missed_stitches += local_missed_count\n",
    "                \n",
    "                print(f\"Confine {i+1}: Trovati {len(stitchable_gaps)} gap (<= 1h). Di questi, {local_missed_count} cuciture mancate.\")\n",
    "            else:\n",
    "                print(f\"Confine {i+1}: Nessun gap (<= 1h) trovato.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  -> ERRORE durante il controllo del confine {i+1}: {e}\")\n",
    "            \n",
    "        # Pulisci la memoria\n",
    "        finally:\n",
    "            del df_A, df_B, last_records_A, first_records_B, boundary_check, stitchable_gaps\n",
    "            if 'missed_stitches' in locals(): del missed_stitches\n",
    "            gc.collect()\n",
    "\n",
    "    print(\"\\n--- ‚úÖ VERIFICA COMPLETATA ---\")\n",
    "    print(f\"RISULTATO FINALE: Trovate {total_missed_stitches} cuciture mancate.\")\n",
    "    \n",
    "    if total_missed_stitches == 0:\n",
    "        print(\"üéâ CONGRATULAZIONI: Il dataset √® perfettamente cucito!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è ATTENZIONE: Ci sono ancora traiettorie spezzate nel dataset.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
