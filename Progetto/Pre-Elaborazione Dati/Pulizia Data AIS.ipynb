{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67cbf103",
   "metadata": {},
   "source": [
    "# Pre-Elaborazione dei Dati (Dataset di riferimento 2025 da Gennaio a Giugno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ec05d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbbdcf0",
   "metadata": {},
   "source": [
    "### Delineamo l'ambiente di lavoro\n",
    "\n",
    "In questa sezione vengono definite le directory di lavoro e tutti quei parametri per cui andiamo a filtrare i nostri dati.\n",
    "\n",
    "SOG_MIN --> Impostiamo il parametro a 2.0, questo ci serve per poi andare a scartare tutte le navi ferme.\n",
    "\n",
    "TIME_GAP --> Questa Ã¨ una soglia di tempo massima arbitraria permessa all'interno di una singola traiettoria. Se tra due messaggi consecutivi della stessa nave passano piÃ¹ di 60 minuti, assumiamo che la rotta sia stata interrotta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbf94fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trovati 365 file Parquet da processare.\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIR = '../../../Dataset'\n",
    "SCRIPT_DIR = os.getcwd()                                # Restituisce la directory di lavoro corrente\n",
    "\n",
    "OUTPUT_DIR_NAME = 'Dataset_Pre-Cleaned_AIS' \n",
    "OUTPUT_DIR = os.path.join(SCRIPT_DIR, OUTPUT_DIR_NAME)\n",
    "\n",
    "SOG_MIN_THRESHOLD = 2.0\n",
    "TIME_GAP_THRESHOLD = pd.Timedelta(hours=1)\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "all_files = glob.glob(os.path.join(INPUT_DIR, '*.parquet'))\n",
    "\n",
    "all_clean_data = []\n",
    "\n",
    "print(f\"Trovati {len(all_files)} file Parquet da processare.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3397dad",
   "metadata": {},
   "source": [
    "#### Test\n",
    "\n",
    "- Proviamo a verificare la lettura di un file parquet e della corretta formattazione dei dati.  \n",
    "- Oltre a questo andiamo ad estrarre il numero di colonne per verificare se sono state selezionate le colonne corrette.  \n",
    "- Viene aggiunto anche un controllo sulle righe per vedere dopo la pulizia la percentuale di pulizia per ogni file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b38aa3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ðŸ” DEBUG: Dati iniziali dal file ais-2025-01-01.parquet ---\n",
      "\n",
      "Head del DataFrame:\n",
      "        MMSI  Latitude  Longitude  SOG    COG           Timestamp\n",
      "0  671087100  18.46281  -66.10297  0.0  176.7 2025-01-01 00:00:00\n",
      "1  367733950  48.48503 -122.60927  0.0  215.5 2025-01-01 00:00:00\n",
      "2  368138010  40.47715  -73.84652  5.5  286.9 2025-01-01 00:00:02\n",
      "3  367637210  29.12033  -90.21215  0.0  227.6 2025-01-01 00:00:03\n",
      "4  368050000  41.27196  -72.46934  0.0  107.1 2025-01-01 00:00:03\n",
      "\n",
      "Tipi di Dati (Dtypes) dopo la conversione Timestamp:\n",
      "MMSI                  int64\n",
      "Latitude            float64\n",
      "Longitude           float64\n",
      "SOG                 float64\n",
      "COG                 float64\n",
      "Timestamp    datetime64[ns]\n",
      "dtype: object\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Numero di righe: 7337208, Numero di colonne: 6\n",
      "\n",
      "--- ðŸ” DEBUG: Dati iniziali dal file AIS_2024_12_31.parquet ---\n",
      "\n",
      "Head del DataFrame:\n",
      "        MMSI  Latitude  Longitude   SOG    COG           Timestamp\n",
      "0  367776660  21.19308 -157.72342   8.0  112.1 2024-12-31 00:00:08\n",
      "1  368095340  29.76995  -95.07893   0.0  185.5 2024-12-31 00:00:05\n",
      "2  366847780  29.96697  -93.85909   0.1  186.2 2024-12-31 00:00:00\n",
      "3  367481310  27.68242  -82.58073  11.5   57.6 2024-12-31 00:00:04\n",
      "4  248669000  29.85743  -93.94083   3.1  220.9 2024-12-31 00:00:06\n",
      "\n",
      "Tipi di Dati (Dtypes) dopo la conversione Timestamp:\n",
      "MMSI                  int64\n",
      "Latitude            float64\n",
      "Longitude           float64\n",
      "SOG                 float64\n",
      "COG                 float64\n",
      "Timestamp    datetime64[ns]\n",
      "dtype: object\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Numero di righe: 7588976, Numero di colonne: 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = '../../../Dataset/'\n",
    "\n",
    "FILE_PATH_TEST = os.path.join(BASE_PATH, 'ais-2025-01-01.parquet')\n",
    "FILE_PATH_TEST2 = os.path.join(BASE_PATH, 'AIS_2024_12_31.parquet')\n",
    "\n",
    "COLUMN_MAPPING2025 = {\n",
    "    'mmsi': 'MMSI', \n",
    "    'latitude': 'Latitude', \n",
    "    'longitude': 'Longitude', \n",
    "    'sog': 'SOG', \n",
    "    'cog': 'COG', \n",
    "    'base_date_time': 'Timestamp' \n",
    "}\n",
    "COLUMNS_TO_READ_2025 = list(COLUMN_MAPPING2025.keys())\n",
    "\n",
    "try:\n",
    "    df = pd.read_parquet(\n",
    "        FILE_PATH_TEST, \n",
    "        columns=COLUMNS_TO_READ_2025,\n",
    "        engine='pyarrow' \n",
    "    )\n",
    "\n",
    "    df = df.rename(columns=COLUMN_MAPPING2025)\n",
    "\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "    \n",
    "    \n",
    "    print(f\"--- ðŸ” DEBUG: Dati iniziali dal file {os.path.basename(FILE_PATH_TEST)} ---\")\n",
    "    print(\"\\nHead del DataFrame:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nTipi di Dati (Dtypes) dopo la conversione Timestamp:\")\n",
    "    print(df.dtypes)\n",
    "    print(\"----------------------------------------------------------------\\n\")\n",
    "    rows,columns = df.shape\n",
    "    print(f\"Numero di righe: {rows}, Numero di colonne: {columns}\\n\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Errore nel processare il file {FILE_PATH_TEST}: {e}\")\n",
    "\n",
    "\n",
    "COLUMN_MAPPING2024 = {\n",
    "    'MMSI': 'MMSI',\n",
    "    'LAT': 'Latitude',\n",
    "    'LON': 'Longitude',\n",
    "    'SOG': 'SOG',\n",
    "    'COG': 'COG',\n",
    "    'BaseDateTime': 'Timestamp' \n",
    "}\n",
    "COLUMNS_TO_READ_2024 = list(COLUMN_MAPPING2024.keys())\n",
    "\n",
    "try:\n",
    "    df = pd.read_parquet(\n",
    "        FILE_PATH_TEST2, \n",
    "        columns=COLUMNS_TO_READ_2024,\n",
    "        engine='pyarrow' \n",
    "    )\n",
    "\n",
    "    df = df.rename(columns=COLUMN_MAPPING2024)\n",
    "\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "\n",
    "\n",
    "    print(f\"--- ðŸ” DEBUG: Dati iniziali dal file {os.path.basename(FILE_PATH_TEST2)} ---\")\n",
    "    print(\"\\nHead del DataFrame:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nTipi di Dati (Dtypes) dopo la conversione Timestamp:\")\n",
    "    print(df.dtypes)\n",
    "    print(\"----------------------------------------------------------------\\n\")\n",
    "    rows,columns = df.shape\n",
    "    print(f\"Numero di righe: {rows}, Numero di colonne: {columns}\\n\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Errore nel processare il file {FILE_PATH_TEST2}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d6f751",
   "metadata": {},
   "source": [
    "### Pulizia dei dati\n",
    " \n",
    "In questa sezione, iteriamo su ogni file del nostro dataset ed eseguiamo la pulizia vera e propria, applicando dei filtri. Il primo filtro filtro applicato Ã¨ sulla lettura delle colonne `COLUMNS_TO_READ` prima di caricare i dati. Ãˆ il modo piÃ¹ efficiente per scartare le colonne inutili e riduce drasticamente l'utilizzo della RAM velocizzando l'intero processo.\n",
    "\n",
    "##### Filtri Navigazione Attiva e di ValiditÃ \n",
    "  \n",
    "Vengono applicati una serie di filtri per lasciare all'interno del dataset solo valori validi e di navigazione attiva:\n",
    "1. Applichiamo il filtro `df = df[df['SOG'] > SOG_MIN_THRESHOLD`, eliminando i dati statici come deciso sopra.\n",
    "2. Applichiamo il filtro `df[df['COG'] != 511]`,rimuovendo i record dove il COG (Course Over Ground) Ã¨ $511$. Questo Ã¨ un codice standard AIS che significa \"Dato Non Disponibile\". Senza una rotta (COG), l'informazione cinematica Ã¨ incompleta e inutile per il modello.\n",
    "3. Applichiamo il filtro `Filtro Lat/Lon (>= -90, <= 90, etc.)`, eliminiamo i record con coordinate geografiche errate (fuori dal globo). Questi sono errori di trasmissione o del sensore che inquinerebbero il dataset.\n",
    "4. Utilizziamo il metodo `df.dopna(...)` per rimuovere qualsiasi riga che abbia valori mancanti. Questo perchÃ¨ i modelli LSTM/LNN richiedono input completi per funzionare correttamente.\n",
    "5. Infine l'ultimo filtro Ã¨ `df['MMSI'].str.len()==9` per rimuovere i record con l'identificativo della nave non corretto. Questo perchÃ¨ l'MMSI deve essere di 9 cifre e questo ci garantisce che ogni traiettoria sia attribuita ad una nave valida.\n",
    "\n",
    "##### Segmentazione e Creazione delle Traiettorie\n",
    "\n",
    "Questa Ã¨ la fase finale prima del salvataggio, dove trasformiamo i dati puliti in sequenze coerenti (TrajectoryID).  \n",
    "Quello che andiamo a fare Ã¨ raggruppare i nostri dati prima per l'MMSI e poi per il TimeStamp. In questo modo abbiamo i dati ordinati ed  Ã¨ possibile delineare quelle che sono le tratiettorie diverse per ogni nave. Viene aggiunta una nuova colonna al dataset che Ã¨ `TrajectoryID` che ha il compito di raggruppare tutti i dati di ogni singola nave che fanno riferimento ad un intero spostamento.  \n",
    "Gli spostamenti sono stati delineati assumendo che spostamenti diversi vengono caratterizzati da uno stato di navigazione non attiva di almeno 1 ora.  \n",
    "Questa fase Ã¨ essenziale perchÃ¨ i modelli che andremo ad addestrare, impareranno non dai singoli punti ma dalle intere sequenze.\n",
    "\n",
    "```\n",
    "df = df.sort_values(by=['MMSI', 'Timestamp']).reset_index(drop=True)\n",
    "\n",
    "            df['TimeDiff'] = df.groupby('MMSI')['Timestamp'].diff()\n",
    "            df['IsNewTraj'] = (df['MMSI'] != df['MMSI'].shift(1)) | (df['TimeDiff'] > TIME_GAP_THRESHOLD)\n",
    "            df['TrajectoryID'] = df['IsNewTraj'].cumsum()\n",
    "```\n",
    "\n",
    "\n",
    "L'ultima parte serve solo per salvare i file .parquet puliti e segmentati nel dataset che poi servirÃ  per addestrare i modelli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445983d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPPING_2025 = {\n",
    "    'mmsi': 'MMSI', \n",
    "    'latitude': 'Latitude', \n",
    "    'longitude': 'Longitude', \n",
    "    'sog': 'SOG', \n",
    "    'cog': 'COG', \n",
    "    'base_date_time': 'Timestamp' \n",
    "}\n",
    "\n",
    "COLUMNS_2025 = list(MAPPING_2025.keys())\n",
    "\n",
    "MAPPING_2024 = {\n",
    "    'MMSI': 'MMSI',\n",
    "    'LAT': 'Latitude',\n",
    "    'LON': 'Longitude',\n",
    "    'SOG': 'SOG',\n",
    "    'COG': 'COG',\n",
    "    'BaseDateTime': 'Timestamp'\n",
    "}\n",
    "COLUMNS_2024 = list(MAPPING_2024.keys())\n",
    "\n",
    "for file_path in all_files:\n",
    "    df = None\n",
    "    mapping_usato = None\n",
    "\n",
    "    try:\n",
    "        \n",
    "        df = pd.read_parquet(\n",
    "            file_path, \n",
    "            columns=COLUMNS_2025,\n",
    "            engine='pyarrow' \n",
    "        )\n",
    "        df = df.rename(columns=MAPPING_2025)\n",
    "        mapping_usato = \"2025\"\n",
    "    \n",
    "    except Exception as e1:\n",
    "        try:\n",
    "            df = pd.read_parquet(\n",
    "                file_path, \n",
    "                columns=COLUMNS_2024,\n",
    "                engine='pyarrow' \n",
    "            )\n",
    "            df = df.rename(columns=MAPPING_2024)\n",
    "            mapping_usato = \"2024\"\n",
    "        \n",
    "        except Exception as e2:\n",
    "            print(f\"Errore IRRISOLVIBILE nel caricare {file_path}: Schema non riconosciuto.\")\n",
    "            continue\n",
    "\n",
    "    if df is not None:\n",
    "        try:\n",
    "            \n",
    "            df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "            \n",
    "            # Filtri cinematici e geografici\n",
    "            df = df[df['SOG'] > SOG_MIN_THRESHOLD]\n",
    "            df = df[df['COG'] != 511]\n",
    "            df = df[(df['Latitude'] >= -90) & (df['Latitude'] <= 90)]\n",
    "            df = df[(df['Longitude'] >= -180) & (df['Longitude'] <= 180)]\n",
    "            \n",
    "            # Filtri di integritÃ \n",
    "            df = df.dropna(subset=['MMSI', 'Latitude', 'Longitude', 'SOG', 'COG'])\n",
    "            df['MMSI'] = df['MMSI'].astype(str).str.replace(r'\\D', '', regex=True)\n",
    "            df = df[df['MMSI'].str.len() == 9]\n",
    "\n",
    "            if not df.empty:\n",
    "\n",
    "                output_filename = os.path.basename(file_path).lower()\n",
    "                output_file = os.path.join(OUTPUT_DIR, output_filename)\n",
    "                \n",
    "                df.to_parquet(output_file, index=False)\n",
    "                \n",
    "                print(f\"File {os.path.basename(file_path)} pre-pulito\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Errore nella FASE DI PULIZIA per il file {file_path}: {e}\")\n",
    "\n",
    "print(\"\\n--- FASE 1 (Pre-Pulizia) completata. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076e33fe",
   "metadata": {},
   "source": [
    "#### Test file pulito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b55661",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEANED_FILE_PATH_TEST = 'Dataset_Cleaned_AIS/ais-2025-01-01.parquet'\n",
    "\n",
    "df = pd.read_parquet(\n",
    "        FILE_PATH_TEST, \n",
    "        columns=COLUMNS_TO_READ_2025,\n",
    "        engine='pyarrow' \n",
    "    )\n",
    "\n",
    "df.head()\n",
    "df.info()\n",
    "df.describe()\n",
    "print(df.dtypes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
