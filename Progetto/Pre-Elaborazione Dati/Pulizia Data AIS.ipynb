{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67cbf103",
   "metadata": {},
   "source": [
    "# Pre-Elaborazione dei Dati (Dataset di riferimento 2025 da Gennaio a Giugno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ec05d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbbdcf0",
   "metadata": {},
   "source": [
    "### Delineamo l'ambiente di lavoro\n",
    "\n",
    "In questa sezione vengono definite le directory di lavoro e tutti quei parametri per cui andiamo a filtrare i nostri dati.\n",
    "\n",
    "SOG_MIN --> Impostiamo il parametro a 2.0, questo ci serve per poi andare a scartare tutte le navi ferme.\n",
    "\n",
    "TIME_GAP --> Questa Ã¨ una soglia di tempo massima arbitraria permessa all'interno di una singola traiettoria. Se tra due messaggi consecutivi della stessa nave passano piÃ¹ di 60 minuti, assumiamo che la rotta sia stata interrotta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbf94fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trovati 181 file Parquet da processare.\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIR = '../../../Dataset/AIS_Daily_Data'\n",
    "SCRIPT_DIR = os.getcwd()                                # Restituisce la directory di lavoro corrente\n",
    "\n",
    "OUTPUT_DIR_NAME = 'Dataset_Cleaned_AIS' \n",
    "OUTPUT_DIR = os.path.join(SCRIPT_DIR, OUTPUT_DIR_NAME)\n",
    "\n",
    "SOG_MIN_THRESHOLD = 2.0\n",
    "TIME_GAP_THRESHOLD = pd.Timedelta(hours=1)\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "all_files = glob.glob(os.path.join(INPUT_DIR, '*.parquet'))\n",
    "\n",
    "all_clean_data = []\n",
    "\n",
    "print(f\"Trovati {len(all_files)} file Parquet da processare.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3397dad",
   "metadata": {},
   "source": [
    "#### Test\n",
    "\n",
    "- Proviamo a verificare la lettura di un file parquet e della corretta formattazione dei dati.  \n",
    "- Oltre a questo andiamo ad estrarre il numero di colonne per verificare se sono state selezionate le colonne corrette.  \n",
    "- Viene aggiunto anche un controllo sulle righe per vedere dopo la pulizia la percentuale di pulizia per ogni file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b38aa3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ðŸ” DEBUG: Dati iniziali dal file ais-2025-01-01.parquet ---\n",
      "\n",
      "Head del DataFrame:\n",
      "        MMSI  Latitude  Longitude  SOG    COG           Timestamp\n",
      "0  671087100  18.46281  -66.10297  0.0  176.7 2025-01-01 00:00:00\n",
      "1  367733950  48.48503 -122.60927  0.0  215.5 2025-01-01 00:00:00\n",
      "2  368138010  40.47715  -73.84652  5.5  286.9 2025-01-01 00:00:02\n",
      "3  367637210  29.12033  -90.21215  0.0  227.6 2025-01-01 00:00:03\n",
      "4  368050000  41.27196  -72.46934  0.0  107.1 2025-01-01 00:00:03\n",
      "\n",
      "Tipi di Dati (Dtypes) dopo la conversione Timestamp:\n",
      "MMSI                  int64\n",
      "Latitude            float64\n",
      "Longitude           float64\n",
      "SOG                 float64\n",
      "COG                 float64\n",
      "Timestamp    datetime64[ns]\n",
      "dtype: object\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Numero di righe: 7337208, Numero di colonne: 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FILE_PATH_TEST = '../../../Dataset/AIS_Daily_Data/ais-2025-01-01.parquet'\n",
    "\n",
    "COLUMN_MAPPING = {\n",
    "    'mmsi': 'MMSI', \n",
    "    'latitude': 'Latitude', \n",
    "    'longitude': 'Longitude', \n",
    "    'sog': 'SOG', \n",
    "    'cog': 'COG', \n",
    "    'base_date_time': 'Timestamp' \n",
    "}\n",
    "COLUMNS_TO_READ = list(COLUMN_MAPPING.keys())\n",
    "\n",
    "try:\n",
    "    df = pd.read_parquet(\n",
    "        FILE_PATH_TEST, \n",
    "        columns=COLUMNS_TO_READ,\n",
    "        engine='pyarrow' \n",
    "    )\n",
    "\n",
    "    df = df.rename(columns=COLUMN_MAPPING)\n",
    "\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "    \n",
    "    \n",
    "    print(f\"--- ðŸ” DEBUG: Dati iniziali dal file {os.path.basename(FILE_PATH_TEST)} ---\")\n",
    "    print(\"\\nHead del DataFrame:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nTipi di Dati (Dtypes) dopo la conversione Timestamp:\")\n",
    "    print(df.dtypes)\n",
    "    print(\"----------------------------------------------------------------\\n\")\n",
    "    rows,columns = df.shape\n",
    "    print(f\"Numero di righe: {rows}, Numero di colonne: {columns}\\n\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Errore nel processare il file {FILE_PATH_TEST}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d6f751",
   "metadata": {},
   "source": [
    "### Pulizia dei dati\n",
    " \n",
    "In questa sezione, iteriamo su ogni file del nostro dataset ed eseguiamo la pulizia vera e propria, applicando dei filtri. Il primo filtro filtro applicato Ã¨ sulla lettura delle colonne `COLUMNS_TO_READ` prima di caricare i dati. Ãˆ il modo piÃ¹ efficiente per scartare le colonne inutili e riduce drasticamente l'utilizzo della RAM velocizzando l'intero processo.\n",
    "\n",
    "##### Filtri Navigazione Attiva e di ValiditÃ \n",
    "  \n",
    "Vengono applicati una serie di filtri per lasciare all'interno del dataset solo valori validi e di navigazione attiva:\n",
    "1. Applichiamo il filtro `df = df[df['SOG'] > SOG_MIN_THRESHOLD`, eliminando i dati statici come deciso sopra.\n",
    "2. Applichiamo il filtro `df[df['COG'] != 511]`,rimuovendo i record dove il COG (Course Over Ground) Ã¨ $511$. Questo Ã¨ un codice standard AIS che significa \"Dato Non Disponibile\". Senza una rotta (COG), l'informazione cinematica Ã¨ incompleta e inutile per il modello.\n",
    "3. Applichiamo il filtro `Filtro Lat/Lon (>= -90, <= 90, etc.)`, eliminiamo i record con coordinate geografiche errate (fuori dal globo). Questi sono errori di trasmissione o del sensore che inquinerebbero il dataset.\n",
    "4. Utilizziamo il metodo `df.dopna(...)` per rimuovere qualsiasi riga che abbia valori mancanti. Questo perchÃ¨ i modelli LSTM/LNN richiedono input completi per funzionare correttamente.\n",
    "5. Infine l'ultimo filtro Ã¨ `df['MMSI'].str.len()==9` per rimuovere i record con l'identificativo della nave non corretto. Questo perchÃ¨ l'MMSI deve essere di 9 cifre e questo ci garantisce che ogni traiettoria sia attribuita ad una nave valida.\n",
    "\n",
    "##### Segmentazione e Creazione delle Traiettorie\n",
    "\n",
    "Questa Ã¨ la fase finale prima del salvataggio, dove trasformiamo i dati puliti in sequenze coerenti (TrajectoryID).  \n",
    "Quello che andiamo a fare Ã¨ raggruppare i nostri dati prima per l'MMSI e poi per il TimeStamp. In questo modo abbiamo i dati ordinati ed  Ã¨ possibile delineare quelle che sono le tratiettorie diverse per ogni nave. Viene aggiunta una nuova colonna al dataset che Ã¨ `TrajectoryID` che ha il compito di raggruppare tutti i dati di ogni singola nave che fanno riferimento ad un intero spostamento.  \n",
    "Gli spostamenti sono stati delineati assumendo che spostamenti diversi vengono caratterizzati da uno stato di navigazione non attiva di almeno 1 ora.  \n",
    "Questa fase Ã¨ essenziale perchÃ¨ i modelli che andremo ad addestrare, impareranno non dai singoli punti ma dalle intere sequenze.\n",
    "\n",
    "```\n",
    "df = df.sort_values(by=['MMSI', 'Timestamp']).reset_index(drop=True)\n",
    "\n",
    "            df['TimeDiff'] = df.groupby('MMSI')['Timestamp'].diff()\n",
    "            df['IsNewTraj'] = (df['MMSI'] != df['MMSI'].shift(1)) | (df['TimeDiff'] > TIME_GAP_THRESHOLD)\n",
    "            df['TrajectoryID'] = df['IsNewTraj'].cumsum()\n",
    "```\n",
    "\n",
    "\n",
    "L'ultima parte serve solo per salvare i file .parquet puliti e segmentati nel dataset che poi servirÃ  per addestrare i modelli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445983d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Riscrivo per ogni file il nome delle colonne\n",
    "COLUMN_MAPPING = {\n",
    "    'mmsi': 'MMSI', \n",
    "    'latitude': 'Latitude', \n",
    "    'longitude': 'Longitude', \n",
    "    'sog': 'SOG', \n",
    "    'cog': 'COG', \n",
    "    'base_date_time': 'Timestamp' \n",
    "}\n",
    "COLUMNS_TO_READ = list(COLUMN_MAPPING.keys())\n",
    "\n",
    "# Processa ogni file Parquet nella directory di input e applica la pulizia\n",
    "for file_path in all_files:\n",
    "    try:\n",
    "\n",
    "        df = pd.read_parquet(\n",
    "            file_path, \n",
    "            columns=COLUMNS_TO_READ,\n",
    "            engine='pyarrow' \n",
    "        )\n",
    "\n",
    "        df = df.rename(columns=COLUMN_MAPPING)\n",
    "\n",
    "        df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "        \n",
    "\n",
    "        df = df[df['SOG'] > SOG_MIN_THRESHOLD]                                                   #Filtro per navi ferme\n",
    "        df = df[df['COG'] != 511]                                                                #Filtro per COG non valido\n",
    "\n",
    "        df = df[(df['Latitude'] >= -90) & (df['Latitude'] <= 90)]                                #filtro latitudine \n",
    "        df = df[(df['Longitude'] >= -180) & (df['Longitude'] <= 180)]                            #filtro longitudine \n",
    "        df = df.dropna(subset=['MMSI', 'Latitude', 'Longitude', 'SOG', 'COG'])\n",
    "        \n",
    "        \n",
    "        df['MMSI'] = df['MMSI'].astype(str).str.replace(r'\\D', '', regex=True)\n",
    "        df = df[df['MMSI'].str.len() == 9]                                                       #Corretto numero di cifre MMSI\n",
    "\n",
    "        if not df.empty:\n",
    "            df = df.sort_values(by=['MMSI', 'Timestamp']).reset_index(drop=True)                 #Ordina per MMSI e Timestamp\n",
    "\n",
    "            #Raggruppa le traiettorie\n",
    "            df['TimeDiff'] = df.groupby('MMSI')['Timestamp'].diff()\n",
    "            df['IsNewTraj'] = (df['MMSI'] != df['MMSI'].shift(1)) | (df['TimeDiff'] > TIME_GAP_THRESHOLD)\n",
    "            df['TrajectoryID'] = df['IsNewTraj'].cumsum()\n",
    "\n",
    "           \n",
    "            output_file = os.path.join(OUTPUT_DIR, os.path.basename(file_path))\n",
    "            df.to_parquet(output_file, index=False)\n",
    "            print(f\"File {os.path.basename(file_path)} pulito e salvato in {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Errore nel processare il file {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076e33fe",
   "metadata": {},
   "source": [
    "#### Test file pulito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b55661",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEANED_FILE_PATH_TEST = 'Dataset_Cleaned_AIS/ais-2025-01-01.parquet'\n",
    "\n",
    "df = pd.read_parquet(\n",
    "        FILE_PATH_TEST, \n",
    "        columns=COLUMNS_TO_READ,\n",
    "        engine='pyarrow' \n",
    "    )\n",
    "\n",
    "df.head()\n",
    "df.info()\n",
    "df.describe()\n",
    "print(df.dtypes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
