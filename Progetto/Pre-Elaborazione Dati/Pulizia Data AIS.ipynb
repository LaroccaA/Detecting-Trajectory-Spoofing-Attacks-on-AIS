{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67cbf103",
   "metadata": {},
   "source": [
    "# Pre-Elaborazione dei Dati (Dataset di riferimento da Luglio 2024 a Giugno 2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ec05d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import shutil\n",
    "import re \n",
    "import gc\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbbdcf0",
   "metadata": {},
   "source": [
    "### Delineamo l'ambiente di lavoro\n",
    "\n",
    "In questa sezione vengono definite le directory di lavoro e tutti quei parametri per cui andiamo a filtrare i nostri dati.\n",
    "\n",
    "SOG_MIN --> Impostiamo il parametro a 2.0, questo ci serve per poi andare a scartare tutte le navi ferme.\n",
    "\n",
    "TIME_GAP --> Questa Ã¨ una soglia di tempo massima arbitraria permessa all'interno di una singola traiettoria. Se tra due messaggi consecutivi della stessa nave passano piÃ¹ di 60 minuti, assumiamo che la rotta sia stata interrotta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf94fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trovati 365 file Parquet da processare.\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIR = '../../../Dataset'\n",
    "SCRIPT_DIR = os.getcwd()                                # Restituisce la directory di lavoro corrente\n",
    "\n",
    "OUTPUT_DIR_NAME = 'Dataset_Pre-Cleaned_AIS' \n",
    "OUTPUT_DIR = os.path.join(SCRIPT_DIR, OUTPUT_DIR_NAME)\n",
    "\n",
    "SOG_MIN_THRESHOLD = 2.0\n",
    "TIME_GAP_THRESHOLD = pd.Timedelta(hours=1)\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "all_files = glob.glob(os.path.join(INPUT_DIR, '*.parquet'))\n",
    "\n",
    "all_clean_data = []\n",
    "\n",
    "print(f\"Trovati {len(all_files)} file Parquet da processare.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3397dad",
   "metadata": {},
   "source": [
    "#### Test\n",
    "\n",
    "- Proviamo a verificare la lettura di un file parquet e della corretta formattazione dei dati.  \n",
    "- Oltre a questo andiamo ad estrarre il numero di colonne per verificare se sono state selezionate le colonne corrette.  \n",
    "- Viene aggiunto anche un controllo sulle righe per vedere dopo la pulizia la percentuale di pulizia per ogni file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38aa3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ðŸ” DEBUG: Dati iniziali dal file ais-2025-01-01.parquet ---\n",
      "\n",
      "Head del DataFrame:\n",
      "        MMSI  Latitude  Longitude  SOG    COG           Timestamp\n",
      "0  671087100  18.46281  -66.10297  0.0  176.7 2025-01-01 00:00:00\n",
      "1  367733950  48.48503 -122.60927  0.0  215.5 2025-01-01 00:00:00\n",
      "2  368138010  40.47715  -73.84652  5.5  286.9 2025-01-01 00:00:02\n",
      "3  367637210  29.12033  -90.21215  0.0  227.6 2025-01-01 00:00:03\n",
      "4  368050000  41.27196  -72.46934  0.0  107.1 2025-01-01 00:00:03\n",
      "\n",
      "Tipi di Dati (Dtypes) dopo la conversione Timestamp:\n",
      "MMSI                  int64\n",
      "Latitude            float64\n",
      "Longitude           float64\n",
      "SOG                 float64\n",
      "COG                 float64\n",
      "Timestamp    datetime64[ns]\n",
      "dtype: object\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Numero di righe: 7337208, Numero di colonne: 6\n",
      "\n",
      "--- ðŸ” DEBUG: Dati iniziali dal file AIS_2024_12_31.parquet ---\n",
      "\n",
      "Head del DataFrame:\n",
      "        MMSI  Latitude  Longitude   SOG    COG           Timestamp\n",
      "0  367776660  21.19308 -157.72342   8.0  112.1 2024-12-31 00:00:08\n",
      "1  368095340  29.76995  -95.07893   0.0  185.5 2024-12-31 00:00:05\n",
      "2  366847780  29.96697  -93.85909   0.1  186.2 2024-12-31 00:00:00\n",
      "3  367481310  27.68242  -82.58073  11.5   57.6 2024-12-31 00:00:04\n",
      "4  248669000  29.85743  -93.94083   3.1  220.9 2024-12-31 00:00:06\n",
      "\n",
      "Tipi di Dati (Dtypes) dopo la conversione Timestamp:\n",
      "MMSI                  int64\n",
      "Latitude            float64\n",
      "Longitude           float64\n",
      "SOG                 float64\n",
      "COG                 float64\n",
      "Timestamp    datetime64[ns]\n",
      "dtype: object\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Numero di righe: 7588976, Numero di colonne: 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = '../../../Dataset/'\n",
    "\n",
    "FILE_PATH_TEST = os.path.join(BASE_PATH, 'ais-2025-01-01.parquet')\n",
    "FILE_PATH_TEST2 = os.path.join(BASE_PATH, 'AIS_2024_12_31.parquet')\n",
    "\n",
    "COLUMN_MAPPING2025 = {\n",
    "    'mmsi': 'MMSI', \n",
    "    'latitude': 'Latitude', \n",
    "    'longitude': 'Longitude', \n",
    "    'sog': 'SOG', \n",
    "    'cog': 'COG', \n",
    "    'base_date_time': 'Timestamp' \n",
    "}\n",
    "COLUMNS_TO_READ_2025 = list(COLUMN_MAPPING2025.keys())\n",
    "\n",
    "try:\n",
    "    df = pd.read_parquet(\n",
    "        FILE_PATH_TEST, \n",
    "        columns=COLUMNS_TO_READ_2025,\n",
    "        engine='pyarrow' \n",
    "    )\n",
    "\n",
    "    df = df.rename(columns=COLUMN_MAPPING2025)\n",
    "\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "    \n",
    "    \n",
    "    print(f\"--- ðŸ” DEBUG: Dati iniziali dal file {os.path.basename(FILE_PATH_TEST)} ---\")\n",
    "    print(\"\\nHead del DataFrame:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nTipi di Dati (Dtypes) dopo la conversione Timestamp:\")\n",
    "    print(df.dtypes)\n",
    "    print(\"----------------------------------------------------------------\\n\")\n",
    "    rows,columns = df.shape\n",
    "    print(f\"Numero di righe: {rows}, Numero di colonne: {columns}\\n\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Errore nel processare il file {FILE_PATH_TEST}: {e}\")\n",
    "\n",
    "\n",
    "COLUMN_MAPPING2024 = {\n",
    "    'MMSI': 'MMSI',\n",
    "    'LAT': 'Latitude',\n",
    "    'LON': 'Longitude',\n",
    "    'SOG': 'SOG',\n",
    "    'COG': 'COG',\n",
    "    'BaseDateTime': 'Timestamp' \n",
    "}\n",
    "COLUMNS_TO_READ_2024 = list(COLUMN_MAPPING2024.keys())\n",
    "\n",
    "try:\n",
    "    df = pd.read_parquet(\n",
    "        FILE_PATH_TEST2, \n",
    "        columns=COLUMNS_TO_READ_2024,\n",
    "        engine='pyarrow' \n",
    "    )\n",
    "\n",
    "    df = df.rename(columns=COLUMN_MAPPING2024)\n",
    "\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "\n",
    "\n",
    "    print(f\" DEBUG: Dati iniziali dal file {os.path.basename(FILE_PATH_TEST2)}\")\n",
    "    print(\"\\nHead del DataFrame:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nTipi di Dati (Dtypes) dopo la conversione Timestamp:\")\n",
    "    print(df.dtypes)\n",
    "    print(\"----------------------------------------------------------------\\n\")\n",
    "    rows,columns = df.shape\n",
    "    print(f\"Numero di righe: {rows}, Numero di colonne: {columns}\\n\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Errore nel processare il file {FILE_PATH_TEST2}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d6f751",
   "metadata": {},
   "source": [
    "### Pulizia dei dati\n",
    " \n",
    "In questa sezione, iteriamo su ogni file del nostro dataset ed eseguiamo la pulizia vera e propria, applicando dei filtri. Il primo filtro filtro applicato Ã¨ sulla lettura delle colonne `COLUMNS_TO_READ` prima di caricare i dati. Ãˆ il modo piÃ¹ efficiente per scartare le colonne inutili e riduce drasticamente l'utilizzo della RAM velocizzando l'intero processo.\n",
    "\n",
    "##### Filtri Navigazione Attiva e di ValiditÃ \n",
    "  \n",
    "Vengono applicati una serie di filtri per lasciare all'interno del dataset solo valori validi e di navigazione attiva:\n",
    "1. Applichiamo il filtro `df = df[df['SOG'] > SOG_MIN_THRESHOLD`, eliminando i dati statici come deciso sopra.\n",
    "2. Applichiamo il filtro `df[df['COG'] != 511]`,rimuovendo i record dove il COG (Course Over Ground) Ã¨ $511$. Questo Ã¨ un codice standard AIS che significa \"Dato Non Disponibile\". Senza una rotta (COG), l'informazione cinematica Ã¨ incompleta e inutile per il modello.\n",
    "3. Applichiamo il filtro `Filtro Lat/Lon (>= -90, <= 90, etc.)`, eliminiamo i record con coordinate geografiche errate (fuori dal globo). Questi sono errori di trasmissione o del sensore che inquinerebbero il dataset.\n",
    "4. Utilizziamo il metodo `df.dopna(...)` per rimuovere qualsiasi riga che abbia valori mancanti. Questo perchÃ¨ i modelli LSTM/LNN richiedono input completi per funzionare correttamente.\n",
    "5. Infine l'ultimo filtro Ã¨ `df['MMSI'].str.len()==9` per rimuovere i record con l'identificativo della nave non corretto. Questo perchÃ¨ l'MMSI deve essere di 9 cifre e questo ci garantisce che ogni traiettoria sia attribuita ad una nave valida.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "445983d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ais-2025-02-15.parquet pre-pulito\n",
      "File ais-2025-06-22.parquet pre-pulito\n",
      "File ais-2025-04-26.parquet pre-pulito\n",
      "File AIS_2024_08_21.parquet pre-pulito\n",
      "File ais-2025-01-22.parquet pre-pulito\n",
      "File ais-2025-02-27.parquet pre-pulito\n",
      "File ais-2025-06-18.parquet pre-pulito\n",
      "File ais-2025-06-14.parquet pre-pulito\n",
      "File AIS_2024_08_11.parquet pre-pulito\n",
      "File AIS_2024_08_06.parquet pre-pulito\n",
      "File AIS_2024_08_22.parquet pre-pulito\n",
      "File ais-2025-06-21.parquet pre-pulito\n",
      "File AIS_2024_11_05.parquet pre-pulito\n",
      "File AIS_2024_10_30.parquet pre-pulito\n",
      "File AIS_2024_10_16.parquet pre-pulito\n",
      "File ais-2025-01-23.parquet pre-pulito\n",
      "File ais-2025-03-26.parquet pre-pulito\n",
      "File AIS_2024_09_24.parquet pre-pulito\n",
      "File ais-2025-03-25.parquet pre-pulito\n",
      "File AIS_2024_07_01.parquet pre-pulito\n",
      "File AIS_2024_09_11.parquet pre-pulito\n",
      "File ais-2025-03-08.parquet pre-pulito\n",
      "File AIS_2024_11_21.parquet pre-pulito\n",
      "File ais-2025-04-25.parquet pre-pulito\n",
      "File AIS_2024_08_18.parquet pre-pulito\n",
      "File AIS_2024_12_19.parquet pre-pulito\n",
      "File AIS_2024_12_29.parquet pre-pulito\n",
      "File ais-2025-01-21.parquet pre-pulito\n",
      "File ais-2025-04-03.parquet pre-pulito\n",
      "File ais-2025-04-22.parquet pre-pulito\n",
      "File ais-2025-01-04.parquet pre-pulito\n",
      "File ais-2025-04-11.parquet pre-pulito\n",
      "File AIS_2024_12_05.parquet pre-pulito\n",
      "File AIS_2024_09_30.parquet pre-pulito\n",
      "File AIS_2024_12_28.parquet pre-pulito\n",
      "File AIS_2024_12_26.parquet pre-pulito\n",
      "File AIS_2024_09_21.parquet pre-pulito\n",
      "File ais-2025-05-10.parquet pre-pulito\n",
      "File AIS_2024_12_21.parquet pre-pulito\n",
      "File AIS_2024_07_14.parquet pre-pulito\n",
      "File ais-2025-04-05.parquet pre-pulito\n",
      "File AIS_2024_08_23.parquet pre-pulito\n",
      "File AIS_2024_08_30.parquet pre-pulito\n",
      "File ais-2025-05-01.parquet pre-pulito\n",
      "File AIS_2024_12_31.parquet pre-pulito\n",
      "File AIS_2024_07_11.parquet pre-pulito\n",
      "File AIS_2024_09_25.parquet pre-pulito\n",
      "File AIS_2024_11_17.parquet pre-pulito\n",
      "File AIS_2024_09_29.parquet pre-pulito\n",
      "File AIS_2024_07_27.parquet pre-pulito\n",
      "File ais-2025-06-25.parquet pre-pulito\n",
      "File ais-2025-04-15.parquet pre-pulito\n",
      "File ais-2025-02-20.parquet pre-pulito\n",
      "File AIS_2024_11_15.parquet pre-pulito\n",
      "File ais-2025-05-06.parquet pre-pulito\n",
      "File AIS_2024_09_18.parquet pre-pulito\n",
      "File AIS_2024_12_12.parquet pre-pulito\n",
      "File ais-2025-05-25.parquet pre-pulito\n",
      "File AIS_2024_10_09.parquet pre-pulito\n",
      "File ais-2025-04-18.parquet pre-pulito\n",
      "File ais-2025-02-18.parquet pre-pulito\n",
      "File AIS_2024_11_19.parquet pre-pulito\n",
      "File AIS_2024_11_14.parquet pre-pulito\n",
      "File ais-2025-02-17.parquet pre-pulito\n",
      "File ais-2025-03-16.parquet pre-pulito\n",
      "File ais-2025-06-24.parquet pre-pulito\n",
      "File ais-2025-06-17.parquet pre-pulito\n",
      "File AIS_2024_11_09.parquet pre-pulito\n",
      "File ais-2025-03-07.parquet pre-pulito\n",
      "File AIS_2024_09_17.parquet pre-pulito\n",
      "File AIS_2024_08_28.parquet pre-pulito\n",
      "File AIS_2024_09_01.parquet pre-pulito\n",
      "File ais-2025-01-16.parquet pre-pulito\n",
      "File AIS_2024_11_28.parquet pre-pulito\n",
      "File ais-2025-03-05.parquet pre-pulito\n",
      "File AIS_2024_08_05.parquet pre-pulito\n",
      "File AIS_2024_12_06.parquet pre-pulito\n",
      "File AIS_2024_11_04.parquet pre-pulito\n",
      "File ais-2025-06-28.parquet pre-pulito\n",
      "File ais-2025-05-29.parquet pre-pulito\n",
      "File ais-2025-01-07.parquet pre-pulito\n",
      "File AIS_2024_08_16.parquet pre-pulito\n",
      "File AIS_2024_12_20.parquet pre-pulito\n",
      "File AIS_2024_07_22.parquet pre-pulito\n",
      "File AIS_2024_11_22.parquet pre-pulito\n",
      "File AIS_2024_11_10.parquet pre-pulito\n",
      "File ais-2025-02-06.parquet pre-pulito\n",
      "File ais-2025-05-11.parquet pre-pulito\n",
      "File AIS_2024_12_30.parquet pre-pulito\n",
      "File ais-2025-05-21.parquet pre-pulito\n",
      "File ais-2025-04-06.parquet pre-pulito\n",
      "File AIS_2024_08_25.parquet pre-pulito\n",
      "File AIS_2024_07_07.parquet pre-pulito\n",
      "File AIS_2024_10_25.parquet pre-pulito\n",
      "File AIS_2024_09_06.parquet pre-pulito\n",
      "File ais-2025-05-04.parquet pre-pulito\n",
      "File ais-2025-05-18.parquet pre-pulito\n",
      "File AIS_2024_08_31.parquet pre-pulito\n",
      "File AIS_2024_12_10.parquet pre-pulito\n",
      "File ais-2025-06-11.parquet pre-pulito\n",
      "File AIS_2024_12_22.parquet pre-pulito\n",
      "File AIS_2024_09_13.parquet pre-pulito\n",
      "File AIS_2024_12_23.parquet pre-pulito\n",
      "File AIS_2024_11_01.parquet pre-pulito\n",
      "File ais-2025-06-29.parquet pre-pulito\n",
      "File ais-2025-02-26.parquet pre-pulito\n",
      "File AIS_2024_09_26.parquet pre-pulito\n",
      "File ais-2025-02-25.parquet pre-pulito\n",
      "File ais-2025-01-15.parquet pre-pulito\n",
      "File ais-2025-02-16.parquet pre-pulito\n",
      "File ais-2025-02-23.parquet pre-pulito\n",
      "File AIS_2024_09_22.parquet pre-pulito\n",
      "File ais-2025-05-15.parquet pre-pulito\n",
      "File AIS_2024_12_18.parquet pre-pulito\n",
      "File AIS_2024_10_31.parquet pre-pulito\n",
      "File ais-2025-04-13.parquet pre-pulito\n",
      "File ais-2025-03-03.parquet pre-pulito\n",
      "File ais-2025-03-01.parquet pre-pulito\n",
      "File ais-2025-01-05.parquet pre-pulito\n",
      "File ais-2025-06-16.parquet pre-pulito\n",
      "File ais-2025-02-22.parquet pre-pulito\n",
      "File ais-2025-01-09.parquet pre-pulito\n",
      "File AIS_2024_07_03.parquet pre-pulito\n",
      "File ais-2025-06-12.parquet pre-pulito\n",
      "File AIS_2024_12_11.parquet pre-pulito\n",
      "File AIS_2024_10_18.parquet pre-pulito\n",
      "File ais-2025-01-02.parquet pre-pulito\n",
      "File ais-2025-06-04.parquet pre-pulito\n",
      "File AIS_2024_12_14.parquet pre-pulito\n",
      "File AIS_2024_11_29.parquet pre-pulito\n",
      "File ais-2025-01-17.parquet pre-pulito\n",
      "File AIS_2024_08_13.parquet pre-pulito\n",
      "File ais-2025-02-28.parquet pre-pulito\n",
      "File ais-2025-05-26.parquet pre-pulito\n",
      "File ais-2025-02-11.parquet pre-pulito\n",
      "File ais-2025-06-05.parquet pre-pulito\n",
      "File AIS_2024_11_08.parquet pre-pulito\n",
      "File ais-2025-02-13.parquet pre-pulito\n",
      "File AIS_2024_10_11.parquet pre-pulito\n",
      "File ais-2025-03-31.parquet pre-pulito\n",
      "File ais-2025-05-08.parquet pre-pulito\n",
      "File ais-2025-04-10.parquet pre-pulito\n",
      "File ais-2025-03-19.parquet pre-pulito\n",
      "File ais-2025-02-05.parquet pre-pulito\n",
      "File AIS_2024_08_08.parquet pre-pulito\n",
      "File AIS_2024_09_03.parquet pre-pulito\n",
      "File AIS_2024_07_06.parquet pre-pulito\n",
      "File AIS_2024_11_11.parquet pre-pulito\n",
      "File AIS_2024_11_13.parquet pre-pulito\n",
      "File ais-2025-03-21.parquet pre-pulito\n",
      "File ais-2025-03-04.parquet pre-pulito\n",
      "File ais-2025-06-09.parquet pre-pulito\n",
      "File ais-2025-05-27.parquet pre-pulito\n",
      "File ais-2025-02-19.parquet pre-pulito\n",
      "File ais-2025-01-08.parquet pre-pulito\n",
      "File AIS_2024_12_07.parquet pre-pulito\n",
      "File AIS_2024_09_23.parquet pre-pulito\n",
      "File AIS_2024_10_03.parquet pre-pulito\n",
      "File AIS_2024_11_06.parquet pre-pulito\n",
      "File ais-2025-04-27.parquet pre-pulito\n",
      "File ais-2025-05-30.parquet pre-pulito\n",
      "File AIS_2024_10_12.parquet pre-pulito\n",
      "File AIS_2024_10_07.parquet pre-pulito\n",
      "File ais-2025-06-27.parquet pre-pulito\n",
      "File ais-2025-02-14.parquet pre-pulito\n",
      "File ais-2025-03-27.parquet pre-pulito\n",
      "File ais-2025-01-29.parquet pre-pulito\n",
      "File AIS_2024_12_13.parquet pre-pulito\n",
      "File AIS_2024_08_04.parquet pre-pulito\n",
      "File AIS_2024_07_12.parquet pre-pulito\n",
      "File AIS_2024_07_18.parquet pre-pulito\n",
      "File AIS_2024_11_16.parquet pre-pulito\n",
      "File AIS_2024_07_28.parquet pre-pulito\n",
      "File ais-2025-06-20.parquet pre-pulito\n",
      "File AIS_2024_09_10.parquet pre-pulito\n",
      "File AIS_2024_12_04.parquet pre-pulito\n",
      "File ais-2025-06-19.parquet pre-pulito\n",
      "File ais-2025-01-20.parquet pre-pulito\n",
      "File ais-2025-03-17.parquet pre-pulito\n",
      "File ais-2025-04-04.parquet pre-pulito\n",
      "File AIS_2024_08_01.parquet pre-pulito\n",
      "File AIS_2024_09_02.parquet pre-pulito\n",
      "File AIS_2024_07_13.parquet pre-pulito\n",
      "File ais-2025-03-10.parquet pre-pulito\n",
      "File AIS_2024_07_15.parquet pre-pulito\n",
      "File ais-2025-04-08.parquet pre-pulito\n",
      "File AIS_2024_10_20.parquet pre-pulito\n",
      "File ais-2025-03-28.parquet pre-pulito\n",
      "File ais-2025-03-29.parquet pre-pulito\n",
      "File AIS_2024_09_05.parquet pre-pulito\n",
      "File AIS_2024_08_15.parquet pre-pulito\n",
      "File AIS_2024_09_12.parquet pre-pulito\n",
      "File ais-2025-02-21.parquet pre-pulito\n",
      "File ais-2025-03-11.parquet pre-pulito\n",
      "File AIS_2024_07_25.parquet pre-pulito\n",
      "File AIS_2024_10_21.parquet pre-pulito\n",
      "File AIS_2024_11_12.parquet pre-pulito\n",
      "File ais-2025-05-17.parquet pre-pulito\n",
      "File ais-2025-04-01.parquet pre-pulito\n",
      "File ais-2025-05-09.parquet pre-pulito\n",
      "File AIS_2024_11_27.parquet pre-pulito\n",
      "File ais-2025-01-01.parquet pre-pulito\n",
      "File AIS_2024_07_24.parquet pre-pulito\n",
      "File AIS_2024_09_28.parquet pre-pulito\n",
      "File ais-2025-01-25.parquet pre-pulito\n",
      "File ais-2025-04-28.parquet pre-pulito\n",
      "File AIS_2024_09_14.parquet pre-pulito\n",
      "File AIS_2024_12_25.parquet pre-pulito\n",
      "File ais-2025-04-16.parquet pre-pulito\n",
      "File ais-2025-05-28.parquet pre-pulito\n",
      "File AIS_2024_12_08.parquet pre-pulito\n",
      "File ais-2025-06-03.parquet pre-pulito\n",
      "File ais-2025-04-14.parquet pre-pulito\n",
      "File ais-2025-06-13.parquet pre-pulito\n",
      "File AIS_2024_07_26.parquet pre-pulito\n",
      "File AIS_2024_10_04.parquet pre-pulito\n",
      "File ais-2025-01-18.parquet pre-pulito\n",
      "File ais-2025-04-19.parquet pre-pulito\n",
      "File AIS_2024_11_03.parquet pre-pulito\n",
      "File AIS_2024_12_09.parquet pre-pulito\n",
      "File AIS_2024_07_31.parquet pre-pulito\n",
      "File ais-2025-04-12.parquet pre-pulito\n",
      "File AIS_2024_08_07.parquet pre-pulito\n",
      "File AIS_2024_10_13.parquet pre-pulito\n",
      "File AIS_2024_11_20.parquet pre-pulito\n",
      "File ais-2025-04-20.parquet pre-pulito\n",
      "File ais-2025-06-10.parquet pre-pulito\n",
      "File ais-2025-02-03.parquet pre-pulito\n",
      "File ais-2025-05-03.parquet pre-pulito\n",
      "File ais-2025-04-02.parquet pre-pulito\n",
      "File AIS_2024_07_29.parquet pre-pulito\n",
      "File ais-2025-05-22.parquet pre-pulito\n",
      "File ais-2025-05-02.parquet pre-pulito\n",
      "File AIS_2024_07_19.parquet pre-pulito\n",
      "File ais-2025-02-04.parquet pre-pulito\n",
      "File AIS_2024_07_21.parquet pre-pulito\n",
      "File ais-2025-01-28.parquet pre-pulito\n",
      "File AIS_2024_07_20.parquet pre-pulito\n",
      "File AIS_2024_08_17.parquet pre-pulito\n",
      "File ais-2025-05-24.parquet pre-pulito\n",
      "File ais-2025-01-06.parquet pre-pulito\n",
      "File ais-2025-02-12.parquet pre-pulito\n",
      "File ais-2025-06-30.parquet pre-pulito\n",
      "File AIS_2024_10_15.parquet pre-pulito\n",
      "File ais-2025-05-20.parquet pre-pulito\n",
      "File ais-2025-02-10.parquet pre-pulito\n",
      "File ais-2025-05-12.parquet pre-pulito\n",
      "File AIS_2024_07_05.parquet pre-pulito\n",
      "File ais-2025-03-12.parquet pre-pulito\n",
      "File ais-2025-04-09.parquet pre-pulito\n",
      "File ais-2025-03-06.parquet pre-pulito\n",
      "File ais-2025-01-03.parquet pre-pulito\n",
      "File ais-2025-03-15.parquet pre-pulito\n",
      "File ais-2025-04-17.parquet pre-pulito\n",
      "File AIS_2024_09_19.parquet pre-pulito\n",
      "File AIS_2024_10_02.parquet pre-pulito\n",
      "File AIS_2024_08_29.parquet pre-pulito\n",
      "File ais-2025-01-14.parquet pre-pulito\n",
      "File ais-2025-03-18.parquet pre-pulito\n",
      "File ais-2025-01-13.parquet pre-pulito\n",
      "File AIS_2024_07_17.parquet pre-pulito\n",
      "File AIS_2024_08_12.parquet pre-pulito\n",
      "File AIS_2024_10_10.parquet pre-pulito\n",
      "File AIS_2024_12_03.parquet pre-pulito\n",
      "File AIS_2024_11_26.parquet pre-pulito\n",
      "File AIS_2024_12_15.parquet pre-pulito\n",
      "File AIS_2024_11_02.parquet pre-pulito\n",
      "File AIS_2024_09_07.parquet pre-pulito\n",
      "File ais-2025-06-08.parquet pre-pulito\n",
      "File AIS_2024_07_10.parquet pre-pulito\n",
      "File AIS_2024_08_14.parquet pre-pulito\n",
      "File ais-2025-04-23.parquet pre-pulito\n",
      "File ais-2025-03-24.parquet pre-pulito\n",
      "File ais-2025-04-30.parquet pre-pulito\n",
      "File AIS_2024_07_04.parquet pre-pulito\n",
      "File ais-2025-01-24.parquet pre-pulito\n",
      "File AIS_2024_10_24.parquet pre-pulito\n",
      "File AIS_2024_12_01.parquet pre-pulito\n",
      "File ais-2025-06-07.parquet pre-pulito\n",
      "File ais-2025-06-02.parquet pre-pulito\n",
      "File AIS_2024_08_10.parquet pre-pulito\n",
      "File AIS_2024_09_27.parquet pre-pulito\n",
      "File ais-2025-03-20.parquet pre-pulito\n",
      "File AIS_2024_09_15.parquet pre-pulito\n",
      "File ais-2025-01-10.parquet pre-pulito\n",
      "File AIS_2024_07_08.parquet pre-pulito\n",
      "File ais-2025-06-01.parquet pre-pulito\n",
      "File AIS_2024_12_27.parquet pre-pulito\n",
      "File ais-2025-06-15.parquet pre-pulito\n",
      "File ais-2025-05-07.parquet pre-pulito\n",
      "File AIS_2024_08_24.parquet pre-pulito\n",
      "File ais-2025-02-02.parquet pre-pulito\n",
      "File ais-2025-06-06.parquet pre-pulito\n",
      "File AIS_2024_10_01.parquet pre-pulito\n",
      "File ais-2025-04-24.parquet pre-pulito\n",
      "File AIS_2024_10_19.parquet pre-pulito\n",
      "File ais-2025-02-08.parquet pre-pulito\n",
      "File AIS_2024_07_02.parquet pre-pulito\n",
      "File ais-2025-05-13.parquet pre-pulito\n",
      "File AIS_2024_10_28.parquet pre-pulito\n",
      "File AIS_2024_08_09.parquet pre-pulito\n",
      "File ais-2025-01-11.parquet pre-pulito\n",
      "File ais-2025-05-31.parquet pre-pulito\n",
      "File ais-2025-03-23.parquet pre-pulito\n",
      "File AIS_2024_11_07.parquet pre-pulito\n",
      "File AIS_2024_10_05.parquet pre-pulito\n",
      "File ais-2025-03-02.parquet pre-pulito\n",
      "File ais-2025-01-30.parquet pre-pulito\n",
      "File ais-2025-01-31.parquet pre-pulito\n",
      "File AIS_2024_07_23.parquet pre-pulito\n",
      "File ais-2025-03-22.parquet pre-pulito\n",
      "File ais-2025-01-19.parquet pre-pulito\n",
      "File AIS_2024_12_24.parquet pre-pulito\n",
      "File ais-2025-05-23.parquet pre-pulito\n",
      "File ais-2025-05-05.parquet pre-pulito\n",
      "File ais-2025-02-09.parquet pre-pulito\n",
      "File ais-2025-05-16.parquet pre-pulito\n",
      "File AIS_2024_10_14.parquet pre-pulito\n",
      "File AIS_2024_10_27.parquet pre-pulito\n",
      "File ais-2025-03-14.parquet pre-pulito\n",
      "File AIS_2024_12_17.parquet pre-pulito\n",
      "File AIS_2024_09_08.parquet pre-pulito\n",
      "File AIS_2024_10_08.parquet pre-pulito\n",
      "File AIS_2024_09_16.parquet pre-pulito\n",
      "File AIS_2024_08_03.parquet pre-pulito\n",
      "File AIS_2024_07_30.parquet pre-pulito\n",
      "File AIS_2024_10_23.parquet pre-pulito\n",
      "File AIS_2024_07_16.parquet pre-pulito\n",
      "File AIS_2024_11_18.parquet pre-pulito\n",
      "File ais-2025-03-09.parquet pre-pulito\n",
      "File AIS_2024_08_26.parquet pre-pulito\n",
      "File AIS_2024_10_17.parquet pre-pulito\n",
      "File AIS_2024_10_06.parquet pre-pulito\n",
      "File ais-2025-02-07.parquet pre-pulito\n",
      "File ais-2025-02-01.parquet pre-pulito\n",
      "File AIS_2024_10_29.parquet pre-pulito\n",
      "File ais-2025-05-14.parquet pre-pulito\n",
      "File AIS_2024_10_26.parquet pre-pulito\n",
      "File ais-2025-04-29.parquet pre-pulito\n",
      "File AIS_2024_11_24.parquet pre-pulito\n",
      "File AIS_2024_08_19.parquet pre-pulito\n",
      "File ais-2025-04-07.parquet pre-pulito\n",
      "File AIS_2024_11_23.parquet pre-pulito\n",
      "File AIS_2024_11_30.parquet pre-pulito\n",
      "File ais-2025-02-24.parquet pre-pulito\n",
      "File AIS_2024_11_25.parquet pre-pulito\n",
      "File ais-2025-06-26.parquet pre-pulito\n",
      "File ais-2025-03-13.parquet pre-pulito\n",
      "File AIS_2024_08_27.parquet pre-pulito\n",
      "File AIS_2024_09_09.parquet pre-pulito\n",
      "File AIS_2024_12_02.parquet pre-pulito\n",
      "File ais-2025-06-23.parquet pre-pulito\n",
      "File AIS_2024_07_09.parquet pre-pulito\n",
      "File AIS_2024_08_20.parquet pre-pulito\n",
      "File ais-2025-01-12.parquet pre-pulito\n",
      "File ais-2025-03-30.parquet pre-pulito\n",
      "File ais-2025-04-21.parquet pre-pulito\n",
      "File AIS_2024_08_02.parquet pre-pulito\n",
      "File AIS_2024_10_22.parquet pre-pulito\n",
      "File ais-2025-01-27.parquet pre-pulito\n",
      "File ais-2025-01-26.parquet pre-pulito\n",
      "File AIS_2024_09_20.parquet pre-pulito\n",
      "File AIS_2024_12_16.parquet pre-pulito\n",
      "File AIS_2024_09_04.parquet pre-pulito\n",
      "File ais-2025-05-19.parquet pre-pulito\n",
      "\n",
      "--- FASE 1 (Pre-Pulizia) completata. ---\n"
     ]
    }
   ],
   "source": [
    "MAPPING_2025 = {\n",
    "    'mmsi': 'MMSI', \n",
    "    'latitude': 'Latitude', \n",
    "    'longitude': 'Longitude', \n",
    "    'sog': 'SOG', \n",
    "    'cog': 'COG', \n",
    "    'base_date_time': 'Timestamp' \n",
    "}\n",
    "\n",
    "COLUMNS_2025 = list(MAPPING_2025.keys())\n",
    "\n",
    "MAPPING_2024 = {\n",
    "    'MMSI': 'MMSI',\n",
    "    'LAT': 'Latitude',\n",
    "    'LON': 'Longitude',\n",
    "    'SOG': 'SOG',\n",
    "    'COG': 'COG',\n",
    "    'BaseDateTime': 'Timestamp'\n",
    "}\n",
    "COLUMNS_2024 = list(MAPPING_2024.keys())\n",
    "\n",
    "for file_path in all_files:\n",
    "    df = None\n",
    "    mapping_usato = None\n",
    "\n",
    "    try:\n",
    "        \n",
    "        df = pd.read_parquet(\n",
    "            file_path, \n",
    "            columns=COLUMNS_2025,\n",
    "            engine='pyarrow' \n",
    "        )\n",
    "        df = df.rename(columns=MAPPING_2025)\n",
    "        mapping_usato = \"2025\"\n",
    "    \n",
    "    except Exception as e1:\n",
    "        try:\n",
    "            df = pd.read_parquet(\n",
    "                file_path, \n",
    "                columns=COLUMNS_2024,\n",
    "                engine='pyarrow' \n",
    "            )\n",
    "            df = df.rename(columns=MAPPING_2024)\n",
    "            mapping_usato = \"2024\"\n",
    "        \n",
    "        except Exception as e2:\n",
    "            print(f\"Errore IRRISOLVIBILE nel caricare {file_path}: Schema non riconosciuto.\")\n",
    "            continue\n",
    "\n",
    "    if df is not None:\n",
    "        try:\n",
    "            \n",
    "            df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "            \n",
    "            # Filtri cinematici e geografici\n",
    "            df = df[df['SOG'] > SOG_MIN_THRESHOLD]\n",
    "            df = df[df['COG'] != 511]\n",
    "            df = df[(df['Latitude'] >= -90) & (df['Latitude'] <= 90)]\n",
    "            df = df[(df['Longitude'] >= -180) & (df['Longitude'] <= 180)]\n",
    "            \n",
    "            # Filtri di integritÃ \n",
    "            df = df.dropna(subset=['MMSI', 'Latitude', 'Longitude', 'SOG', 'COG'])\n",
    "            df['MMSI'] = df['MMSI'].astype(str).str.replace(r'\\D', '', regex=True)\n",
    "            df = df[df['MMSI'].str.len() == 9]\n",
    "\n",
    "            if not df.empty:\n",
    "\n",
    "                output_filename = os.path.basename(file_path).lower()\n",
    "                output_file = os.path.join(OUTPUT_DIR, output_filename)\n",
    "                \n",
    "                df.to_parquet(output_file, index=False)\n",
    "                \n",
    "                print(f\"File {os.path.basename(file_path)} pre-pulito\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Errore nella FASE DI PULIZIA per il file {file_path}: {e}\")\n",
    "\n",
    "print(\"\\n--- FASE 1 (Pre-Pulizia) completata. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076e33fe",
   "metadata": {},
   "source": [
    "#### Test file pre-pulizia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b55661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MMSI</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>SOG</th>\n",
       "      <th>COG</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>368138010</td>\n",
       "      <td>40.47715</td>\n",
       "      <td>-73.84652</td>\n",
       "      <td>5.5</td>\n",
       "      <td>286.9</td>\n",
       "      <td>2025-01-01 00:00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>367188610</td>\n",
       "      <td>27.93936</td>\n",
       "      <td>-82.45703</td>\n",
       "      <td>2.2</td>\n",
       "      <td>147.6</td>\n",
       "      <td>2025-01-01 00:00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>366938780</td>\n",
       "      <td>46.04232</td>\n",
       "      <td>-83.93567</td>\n",
       "      <td>11.8</td>\n",
       "      <td>126.0</td>\n",
       "      <td>2025-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>316028554</td>\n",
       "      <td>49.28782</td>\n",
       "      <td>-123.10689</td>\n",
       "      <td>7.8</td>\n",
       "      <td>215.6</td>\n",
       "      <td>2025-01-01 00:00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>338122081</td>\n",
       "      <td>37.78262</td>\n",
       "      <td>-122.38452</td>\n",
       "      <td>3.7</td>\n",
       "      <td>196.6</td>\n",
       "      <td>2025-01-01 00:00:12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        MMSI  Latitude  Longitude   SOG    COG           Timestamp\n",
       "0  368138010  40.47715  -73.84652   5.5  286.9 2025-01-01 00:00:02\n",
       "1  367188610  27.93936  -82.45703   2.2  147.6 2025-01-01 00:00:04\n",
       "2  366938780  46.04232  -83.93567  11.8  126.0 2025-01-01 00:00:00\n",
       "3  316028554  49.28782 -123.10689   7.8  215.6 2025-01-01 00:00:06\n",
       "4  338122081  37.78262 -122.38452   3.7  196.6 2025-01-01 00:00:12"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRE_CLEANED_FILE_PATH_TEST = 'Dataset_Pre-Cleaned_AIS/ais-2025-01-01.parquet'\n",
    "COLUMNS_TO_READ_2025 = ['MMSI', 'Latitude', 'Longitude','SOG', 'COG', 'Timestamp']\n",
    "\n",
    "df = pd.read_parquet(\n",
    "        PRE_CLEANED_FILE_PATH_TEST, \n",
    "        columns=COLUMNS_TO_READ_2025,\n",
    "        engine='pyarrow' \n",
    "    )\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78c1631",
   "metadata": {},
   "source": [
    "#### Unificazione in blocchi di file da 15 giorni\n",
    "\n",
    "Questo raggruppamento serve per andare a diminuire quelli che sono i punti del problema di \"mezzanotte\". Questo problema chiamato cosÃ¬ da noi per indicare la situazione in cui ci siano traiettorie continue a cavallo di due differenti file. Con un'unica grande unificazione non ci sarebbe stato tale problema ma a causa di limiti Hardware non Ã¨ stato possibile consolidare tutto in un unico file. Si Ã¨ scelto quindi di procedere con un unificazione parziale del dataset totale dove ogni file racchiude 15 giorni.\n",
    "\n",
    "##### Segmentazione e Creazione delle Traiettorie\n",
    "\n",
    "Questa Ã¨ la fase finale prima del salvataggio dei nuovi blocchi, dove trasformiamo i dati puliti in sequenze coerenti (TrajectoryID).  \n",
    "Quello che andiamo a fare Ã¨ raggruppare i nostri dati prima per l'MMSI e poi per il TimeStamp. In questo modo abbiamo i dati ordinati ed  Ã¨ possibile delineare quelle che sono le traiettorie diverse per ogni nave. Viene aggiunta una nuova colonna al dataset che Ã¨ `TrajectoryID` che ha il compito di raggruppare tutti i dati di ogni singola nave che fanno riferimento ad un intero spostamento.  \n",
    "Gli spostamenti sono stati delineati assumendo che spostamenti diversi vengono caratterizzati da uno stato di navigazione non attiva di almeno 1 ora.  \n",
    "Questa fase Ã¨ essenziale perchÃ¨ i modelli che andremo ad addestrare, impareranno non dai singoli punti ma dalle intere sequenze.\n",
    "\n",
    "```\n",
    "df = df.sort_values(by=['MMSI', 'Timestamp']).reset_index(drop=True)\n",
    "df_blocco['TimeDiff'] = df_blocco.groupby('MMSI')['Timestamp'].diff()     \n",
    "df_blocco['IsNewTraj'] = (df_blocco['MMSI'] != df_blocco['MMSI'].shift(1)) | (df_blocco['TimeDiff'] > TIME_GAP_THRESHOLD)\n",
    "df_blocco['IsNewTraj_int'] = df_blocco['IsNewTraj'].astype(int)\n",
    "df_blocco['TrajectoryID'] = df_blocco['IsNewTraj_int'].cumsum() + max_trajectory_id_globale\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efbdc5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trovati 365 file, raggruppati in 25 blocchi da 15 giorni.\n",
      "\n",
      "--- Inizio elaborazione Blocco 1/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 2/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 3/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 4/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 5/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 6/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 7/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 8/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 9/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 10/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 11/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 12/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 13/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 14/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 15/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 16/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 17/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 18/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 19/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 20/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 21/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 22/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 23/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 24/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 25/25 ---\n",
      "Caricamento di 5 file...\n",
      "Inizio calcolo TrajectoryID...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "INPUT_DIR = 'Dataset_Pre-Cleaned_AIS' \n",
    "SCRIPT_DIR = os.getcwd()\n",
    "\n",
    "OUTPUT_DIR_NAME = 'Dataset_Segmentato_15Giorni' \n",
    "OUTPUT_DIR = os.path.join(SCRIPT_DIR, OUTPUT_DIR_NAME)\n",
    "\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    shutil.rmtree(OUTPUT_DIR)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "TIME_GAP_THRESHOLD = pd.Timedelta(hours=1)\n",
    "\n",
    "GIORNI_PER_BLOCCO = 15\n",
    "\n",
    "all_files = glob.glob(os.path.join(INPUT_DIR, '*.parquet'))\n",
    "all_files.sort() # Fondamentale per ordinare i giorni!\n",
    "\n",
    "num_blocchi = math.ceil(len(all_files) / GIORNI_PER_BLOCCO)\n",
    "print(f\"Trovati {len(all_files)} file, raggruppati in {num_blocchi} blocchi da 15 giorni.\")\n",
    "\n",
    "max_trajectory_id_globale = 0 \n",
    "\n",
    "for i in range(num_blocchi):\n",
    "    start_index = i * GIORNI_PER_BLOCCO\n",
    "    end_index = (i + 1) * GIORNI_PER_BLOCCO\n",
    "    \n",
    "    file_list_blocco = all_files[start_index:end_index]\n",
    "    \n",
    "    print(f\"\\n--- Inizio elaborazione Blocco {i+1}/{num_blocchi} ---\")\n",
    "    \n",
    "    try:\n",
    "        print(f\"Caricamento di {len(file_list_blocco)} file...\")\n",
    "        \n",
    "        df_list = [pd.read_parquet(f) for f in file_list_blocco]\n",
    "        df_blocco = pd.concat(df_list, ignore_index=True)\n",
    "        \n",
    "\n",
    "        df_blocco = df_blocco.sort_values(by=['MMSI', 'Timestamp']).reset_index(drop=True)\n",
    "        \n",
    "        print(\"Inizio calcolo TrajectoryID...\")\n",
    "        df_blocco['TimeDiff'] = df_blocco.groupby('MMSI')['Timestamp'].diff()\n",
    "        df_blocco['IsNewTraj'] = (df_blocco['MMSI'] != df_blocco['MMSI'].shift(1)) | (df_blocco['TimeDiff'] > TIME_GAP_THRESHOLD)\n",
    "        df_blocco['IsNewTraj_int'] = df_blocco['IsNewTraj'].astype(int)\n",
    "        df_blocco['TrajectoryID'] = df_blocco['IsNewTraj_int'].cumsum() + max_trajectory_id_globale\n",
    "        \n",
    "        df_blocco = df_blocco.drop(columns=['TimeDiff', 'IsNewTraj', 'IsNewTraj_int'])\n",
    "        \n",
    "        max_trajectory_id_globale = df_blocco['TrajectoryID'].max()\n",
    "        \n",
    "        output_file = os.path.join(OUTPUT_DIR, f\"blocco_{i:03d}-segmentato.parquet\")\n",
    "        df_blocco.to_parquet(output_file, index=False, engine='pyarrow', compression='snappy')\n",
    "        \n",
    "    except MemoryError:\n",
    "        print(f\"--- âŒ ERRORE DI MEMORIA: Blocco {i+1} (15 giorni) Ã¨ ancora troppo grande! ---\")\n",
    "        break \n",
    "    except Exception as e:\n",
    "        print(f\"--- âŒ ERRORE SCONOSCIUTO nel blocco {i+1}: {e} ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75beca3",
   "metadata": {},
   "source": [
    "#### Individuazione e Applicazione dell 'algoritmo di cucitura\n",
    "\n",
    "Per ottenere un'effettiva coerenza dei **TrajectorID** dobbiamo andare ad individuare la presenza di incoerenza nei relativi punti di mezzanotte. CiÃ² sta a significare che nonostante siano stati diminuiti questi punti cruciali non Ã¨ possibile lasciare che traiettorie continue (nuovo spostamento in meno di 1 ora) venga considerato come una nuova traiettoria solo perchÃ¨ vi Ã¨ un cambio di file .parquet.\n",
    "\n",
    "Per gestire tale situazione Ã¨ stato implementato un algoritmo :\n",
    "\n",
    "- Viene letto il primo file insieme al secondo e viene analizzato ogni spostamento durante la \"Mezzanotte\"\n",
    "- Se ci sono spostamenti che non superano l'ora, viene modificato il **TrajectorID** per mantenere coerenza tra i due file\n",
    "- Successivamente viene tolto dalla memoria il primo file e si analizza interamente il secondo per propagare i cambiamenti\n",
    "- In conlcusione viene preso il terzo file e si ripete tutta la procedura\n",
    "\n",
    "Questi passaggi vengono attuati a tutti i blocchi in ordine in modo da poter propagare la correzione all'intero dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6398b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FASE B: Stitching Sequenziale avviato ---\n",
      "Lettura blocchi da: Dataset_Segmentato_15Giorni\n",
      "Salvataggio finale in: /home/al3th3ia/Scrivania/Cybersecurity/Detecting-Trajectory-Spoofing-Attacks-on-AIS/Progetto/Pre-Elaborazione Dati/Dataset_Stitched_Finale\n",
      "\n",
      "Blocco 0 (blocco_000-segmentato.parquet) copiato, nessuna correzione necessaria.\n",
      "\n",
      "--- Inizio cucitura: blocco_000-segmentato.parquet -> blocco_001-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 3127 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_001-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_001-segmentato.parquet -> blocco_002-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2913 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_002-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_002-segmentato.parquet -> blocco_003-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 3666 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_003-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_003-segmentato.parquet -> blocco_004-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 3450 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_004-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_004-segmentato.parquet -> blocco_005-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 3353 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_005-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_005-segmentato.parquet -> blocco_006-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2838 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_006-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_006-segmentato.parquet -> blocco_007-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 3095 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_007-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_007-segmentato.parquet -> blocco_008-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2501 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_008-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_008-segmentato.parquet -> blocco_009-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2540 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_009-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_009-segmentato.parquet -> blocco_010-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2657 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_010-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_010-segmentato.parquet -> blocco_011-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2115 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_011-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_011-segmentato.parquet -> blocco_012-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2019 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_012-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_012-segmentato.parquet -> blocco_013-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2353 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_013-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_013-segmentato.parquet -> blocco_014-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 1890 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_014-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_014-segmentato.parquet -> blocco_015-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2154 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_015-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_015-segmentato.parquet -> blocco_016-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2295 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_016-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_016-segmentato.parquet -> blocco_017-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2602 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_017-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_017-segmentato.parquet -> blocco_018-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2559 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_018-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_018-segmentato.parquet -> blocco_019-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2847 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_019-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_019-segmentato.parquet -> blocco_020-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2463 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_020-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_020-segmentato.parquet -> blocco_021-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2449 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_021-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_021-segmentato.parquet -> blocco_022-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 3181 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_022-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_022-segmentato.parquet -> blocco_023-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 3053 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_023-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_023-segmentato.parquet -> blocco_024-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 3396 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_024-segmentato.parquet corretto e salvato.\n",
      "Dataset perfetto salvato in: /home/al3th3ia/Scrivania/Cybersecurity/Detecting-Trajectory-Spoofing-Attacks-on-AIS/Progetto/Pre-Elaborazione Dati/Dataset_Stitched_Finale\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIR = 'Dataset_Segmentato_15Giorni' \n",
    "OUTPUT_DIR_NAME = 'Dataset_Stitched_Finale' \n",
    "OUTPUT_DIR = os.path.join(os.getcwd(), OUTPUT_DIR_NAME)\n",
    "\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    shutil.rmtree(OUTPUT_DIR)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "TIME_GAP_THRESHOLD = pd.Timedelta(hours=1)\n",
    "\n",
    "print(f\"--- FASE B: Stitching Sequenziale avviato ---\")\n",
    "print(f\"Lettura blocchi da: {INPUT_DIR}\")\n",
    "print(f\"Salvataggio finale in: {OUTPUT_DIR}\\n\")\n",
    "\n",
    "all_blocks = sorted(glob.glob(os.path.join(INPUT_DIR, '*.parquet')))\n",
    "\n",
    "if not all_blocks:\n",
    "    print(f\"ERRORE: Nessun file blocco trovato in {INPUT_DIR}.\")\n",
    "else:\n",
    "        \n",
    "    path_A_corretto = all_blocks[0]\n",
    "    output_path_A = os.path.join(OUTPUT_DIR, os.path.basename(path_A_corretto))\n",
    "    shutil.copy(path_A_corretto, output_path_A)\n",
    "    print(f\"Blocco 0 ({os.path.basename(path_A_corretto)}) copiato, nessuna correzione necessaria.\")\n",
    "\n",
    "    # Si passa al controllo sui due blocchi consecutivi    \n",
    "    for i in range(len(all_blocks) - 1):\n",
    "                \n",
    "        path_A_corretto = os.path.join(OUTPUT_DIR, os.path.basename(all_blocks[i]))\n",
    "        path_B_grezzo = all_blocks[i+1]\n",
    "        \n",
    "        print(f\"\\n--- Inizio cucitura: {os.path.basename(path_A_corretto)} -> {os.path.basename(path_B_grezzo)} ---\")\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            df_A = pd.read_parquet(path_A_corretto)\n",
    "            df_B = pd.read_parquet(path_B_grezzo)\n",
    "            \n",
    "            \n",
    "            print(\"  Trovati confini, calcolo mappa...\")\n",
    "            last_records_A = df_A.loc[df_A.groupby('MMSI')['Timestamp'].idxmax()]\n",
    "            last_records_A = last_records_A[['MMSI', 'Timestamp', 'TrajectoryID']].rename(\n",
    "                columns={'Timestamp': 'Last_Timestamp', 'TrajectoryID': 'Correct_ID'}\n",
    "            )   # Prende l'ultimo record di ogni MMSI in A\n",
    "\n",
    "            first_records_B = df_B.loc[df_B.groupby('MMSI')['Timestamp'].idxmin()]\n",
    "            first_records_B = first_records_B[['MMSI', 'Timestamp', 'TrajectoryID']].rename(\n",
    "                columns={'Timestamp': 'First_Timestamp', 'TrajectoryID': 'Old_ID'}\n",
    "            )   # Prende il primo record di ogni MMSI in B\n",
    "\n",
    "            \n",
    "            boundary_check = pd.merge(last_records_A, first_records_B, on='MMSI')\n",
    "            boundary_check['TimeDiff'] = boundary_check['First_Timestamp'] - boundary_check['Last_Timestamp']\n",
    "            stitch_candidates = boundary_check[boundary_check['TimeDiff'] <= TIME_GAP_THRESHOLD]\n",
    "        \n",
    "            # Creazione della mappa di correzione\n",
    "            local_fix_map = stitch_candidates.set_index('Old_ID')['Correct_ID'].to_dict()\n",
    "            print(f\"  -> Trovate {len(local_fix_map)} cuciture da applicare.\")\n",
    "\n",
    "            print(\"  Rilascio memoria Blocco A...\")\n",
    "            del df_A, last_records_A, first_records_B, boundary_check, stitch_candidates\n",
    "            gc.collect()\n",
    "\n",
    "            print(\"  Applicazione correzioni a Blocco B...\")\n",
    "            df_B['TrajectoryID'] = df_B['TrajectoryID'].map(local_fix_map).fillna(df_B['TrajectoryID']).astype(int)\n",
    "\n",
    "            output_path_B = os.path.join(OUTPUT_DIR, os.path.basename(path_B_grezzo))\n",
    "            df_B.to_parquet(output_path_B, index=False, engine='pyarrow', compression='snappy')\n",
    "            print(f\"  -> Blocco {os.path.basename(output_path_B)} corretto e salvato.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  -> âŒ ERRORE durante la cucitura: {e}\")\n",
    "            break \n",
    "        finally:\n",
    "            if 'df_A' in locals(): del df_A\n",
    "            if 'df_B' in locals(): del df_B\n",
    "            gc.collect()\n",
    "\n",
    "    print(f\"Dataset perfetto salvato in: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3731d31a",
   "metadata": {},
   "source": [
    "#### Verifica dell'algoritmo di cucitura\n",
    "\n",
    "In questa sezione viene integrata una verifica delle cuciture appena svolte. Questo viene fatto analizzando tutti i gap temporali inferiori ad 1 ora e successivamente si vede se tra questi c'Ã¨ differenza di **TrajectorID**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea250b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inizio scansione...\n",
      "Confine 1: Trovati 3127 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 2: Trovati 2913 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 3: Trovati 3666 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 4: Trovati 3450 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 5: Trovati 3353 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 6: Trovati 2838 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 7: Trovati 3095 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 8: Trovati 2501 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 9: Trovati 2540 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 10: Trovati 2657 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 11: Trovati 2115 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 12: Trovati 2019 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 13: Trovati 2353 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 14: Trovati 1890 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 15: Trovati 2154 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 16: Trovati 2295 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 17: Trovati 2602 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 18: Trovati 2559 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 19: Trovati 2847 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 20: Trovati 2463 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 21: Trovati 2449 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 22: Trovati 3181 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 23: Trovati 3053 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 24: Trovati 3396 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "RISULTATO FINALE: Trovate 0 cuciture mancate.\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIR = 'Dataset_Stitched_Finale' \n",
    "TIME_GAP_THRESHOLD = pd.Timedelta(hours=1)\n",
    "\n",
    "all_blocks = sorted(glob.glob(os.path.join(INPUT_DIR, '*.parquet')))\n",
    "\n",
    "if not all_blocks:\n",
    "    print(f\"ERRORE: Nessun file blocco trovato in {INPUT_DIR}.\")\n",
    "else:\n",
    "    total_missed_stitches = 0\n",
    "\n",
    "    print(\"Inizio scansione...\")\n",
    "\n",
    "    for i in range(len(all_blocks) - 1):\n",
    "        path_A = all_blocks[i]\n",
    "        path_B = all_blocks[i+1]\n",
    "        \n",
    "        try:\n",
    "            df_A = pd.read_parquet(path_A)\n",
    "            df_B = pd.read_parquet(path_B)\n",
    "\n",
    "            \n",
    "            last_records_A = df_A.loc[df_A.groupby('MMSI')['Timestamp'].idxmax()]\n",
    "            last_records_A = last_records_A[['MMSI', 'Timestamp', 'TrajectoryID']].rename(\n",
    "                columns={'Timestamp': 'Last_Timestamp', 'TrajectoryID': 'ID_A'}\n",
    "            )\n",
    "\n",
    "            \n",
    "            first_records_B = df_B.loc[df_B.groupby('MMSI')['Timestamp'].idxmin()]\n",
    "            first_records_B = first_records_B[['MMSI', 'Timestamp', 'TrajectoryID']].rename(\n",
    "                columns={'Timestamp': 'First_Timestamp', 'TrajectoryID': 'ID_B'}\n",
    "            )\n",
    "\n",
    "            \n",
    "            boundary_check = pd.merge(last_records_A, first_records_B, on='MMSI')\n",
    "\n",
    "            \n",
    "            boundary_check['TimeDiff'] = boundary_check['First_Timestamp'] - boundary_check['Last_Timestamp']\n",
    "\n",
    "            \n",
    "            stitchable_gaps = boundary_check[boundary_check['TimeDiff'] <= TIME_GAP_THRESHOLD]\n",
    "            \n",
    "            if not stitchable_gaps.empty:\n",
    "                \n",
    "                missed_stitches = stitchable_gaps[stitchable_gaps['ID_A'] != stitchable_gaps['ID_B']]\n",
    "                \n",
    "                local_missed_count = len(missed_stitches)\n",
    "                total_missed_stitches += local_missed_count\n",
    "                \n",
    "                print(f\"Confine {i+1}: Trovati {len(stitchable_gaps)} gap (<= 1h). Di questi, {local_missed_count} cuciture mancate.\")\n",
    "            else:\n",
    "                print(f\"Confine {i+1}: Nessun gap (<= 1h) trovato.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  -> ERRORE durante il controllo del confine {i+1}: {e}\")\n",
    "            \n",
    "        finally:\n",
    "            del df_A, df_B, last_records_A, first_records_B, boundary_check, stitchable_gaps\n",
    "            if 'missed_stitches' in locals(): del missed_stitches\n",
    "            gc.collect()\n",
    "\n",
    "    print(f\"RISULTATO FINALE: Trovate {total_missed_stitches} cuciture mancate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6c7807",
   "metadata": {},
   "source": [
    "#### Verifica presenza dei duplicati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be38478e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inizio scansione\n",
      "Controllo: blocco_000-segmentato.parquet...\n",
      "Trovate 7527 righe duplicate.\n",
      "Controllo: blocco_001-segmentato.parquet...\n",
      "Trovate 1245 righe duplicate.\n",
      "Controllo: blocco_002-segmentato.parquet...\n",
      "Trovate 1329 righe duplicate.\n",
      "Controllo: blocco_003-segmentato.parquet...\n",
      "Trovate 1064 righe duplicate.\n",
      "Controllo: blocco_004-segmentato.parquet...\n",
      "Trovate 2699 righe duplicate.\n",
      "Controllo: blocco_005-segmentato.parquet...\n",
      "Trovate 8737 righe duplicate.\n",
      "Controllo: blocco_006-segmentato.parquet...\n",
      "Trovate 5302 righe duplicate.\n",
      "Controllo: blocco_007-segmentato.parquet...\n",
      "Trovate 6240 righe duplicate.\n",
      "Controllo: blocco_008-segmentato.parquet...\n",
      "Trovate 5287 righe duplicate.\n",
      "Controllo: blocco_009-segmentato.parquet...\n",
      "Trovate 6058 righe duplicate.\n",
      "Controllo: blocco_010-segmentato.parquet...\n",
      "Trovate 6137 righe duplicate.\n",
      "Controllo: blocco_011-segmentato.parquet...\n",
      "Trovate 4072 righe duplicate.\n",
      "Controllo: blocco_012-segmentato.parquet...\n",
      "Trovate 4855 righe duplicate.\n",
      "Controllo: blocco_013-segmentato.parquet...\n",
      "Trovate 6558 righe duplicate.\n",
      "Controllo: blocco_014-segmentato.parquet...\n",
      "Trovate 7355 righe duplicate.\n",
      "Controllo: blocco_015-segmentato.parquet...\n",
      "Trovate 7089 righe duplicate.\n",
      "Controllo: blocco_016-segmentato.parquet...\n",
      "Trovate 9960 righe duplicate.\n",
      "Controllo: blocco_017-segmentato.parquet...\n",
      "Trovate 6473 righe duplicate.\n",
      "Controllo: blocco_018-segmentato.parquet...\n",
      "Trovate 8105 righe duplicate.\n",
      "Controllo: blocco_019-segmentato.parquet...\n",
      "Trovate 6077 righe duplicate.\n",
      "Controllo: blocco_020-segmentato.parquet...\n",
      "Trovate 12409 righe duplicate.\n",
      "Controllo: blocco_021-segmentato.parquet...\n",
      "Trovate 9882 righe duplicate.\n",
      "Controllo: blocco_022-segmentato.parquet...\n",
      "Trovate 8453 righe duplicate.\n",
      "Controllo: blocco_023-segmentato.parquet...\n",
      "Trovate 14830 righe duplicate.\n",
      "Controllo: blocco_024-segmentato.parquet...\n",
      "Trovate 3525 righe duplicate.\n",
      "Sono state trovate 161,268 righe duplicate in totale.\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIR = 'Dataset_Stitched_Finale' \n",
    "\n",
    "all_blocks = sorted(glob.glob(os.path.join(INPUT_DIR, '*.parquet')))\n",
    "\n",
    "if not all_blocks:\n",
    "    print(f\"ERRORE: Nessun file blocco trovato in {INPUT_DIR}.\")\n",
    "else:\n",
    "    total_duplicates_found = 0\n",
    "    print(\"Inizio scansione\")\n",
    "\n",
    " \n",
    "    for block_path in all_blocks:\n",
    "        try:\n",
    "            print(f\"Controllo: {os.path.basename(block_path)}...\")\n",
    "    \n",
    "            df_block = pd.read_parquet(block_path)\n",
    "            \n",
    "            local_duplicates = df_block.duplicated().sum()   \n",
    "            \n",
    "            if local_duplicates > 0:\n",
    "                print(f\"Trovate {local_duplicates} righe duplicate.\")\n",
    "                total_duplicates_found += local_duplicates\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERRORE durante il controllo del blocco: {e}\")\n",
    "        \n",
    "        finally:\n",
    "            if 'df_block' in locals():\n",
    "                del df_block\n",
    "            gc.collect()\n",
    "\n",
    "    print(f\"Sono state trovate {total_duplicates_found:,} righe duplicate in totale.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57e261e",
   "metadata": {},
   "source": [
    "#### Pulizia dei duplicati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b444f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lettura blocchi da: Dataset_Stitched_Finale\n",
      "Salvataggio in: /home/al3th3ia/Scrivania/Cybersecurity/Detecting-Trajectory-Spoofing-Attacks-on-AIS/Progetto/Pre-Elaborazione Dati/Dataset\n",
      "\n",
      "Inizio scansione e pulizia (MMSI Specifici + Duplicati)...\n",
      "Regole MMSI: Rimozione di zeri, '123456789' e cifre ripetute.\n",
      "Blocco blocco_000-segmentato.parquet: Rimosse 1915 righe (MMSI) | Rimosse 7526 righe (Duplicati)\n",
      "Blocco blocco_001-segmentato.parquet: Rimosse 1014 righe (MMSI) | Rimosse 1245 righe (Duplicati)\n",
      "Blocco blocco_002-segmentato.parquet: Rimosse 1555 righe (MMSI) | Rimosse 1329 righe (Duplicati)\n",
      "Blocco blocco_003-segmentato.parquet: Rimosse 1234 righe (MMSI) | Rimosse 1064 righe (Duplicati)\n",
      "Blocco blocco_004-segmentato.parquet: Rimosse 996 righe (MMSI) | Rimosse 2699 righe (Duplicati)\n",
      "Blocco blocco_005-segmentato.parquet: Rimosse 1144 righe (MMSI) | Rimosse 8737 righe (Duplicati)\n",
      "Blocco blocco_006-segmentato.parquet: Rimosse 272 righe (MMSI) | Rimosse 5302 righe (Duplicati)\n",
      "Blocco blocco_007-segmentato.parquet: Rimosse 3767 righe (MMSI) | Rimosse 6240 righe (Duplicati)\n",
      "Blocco blocco_008-segmentato.parquet: Rimosse 3625 righe (MMSI) | Rimosse 5287 righe (Duplicati)\n",
      "Blocco blocco_009-segmentato.parquet: Rimosse 1791 righe (MMSI) | Rimosse 6058 righe (Duplicati)\n",
      "Blocco blocco_010-segmentato.parquet: Rimosse 1605 righe (MMSI) | Rimosse 6137 righe (Duplicati)\n",
      "Blocco blocco_011-segmentato.parquet: Rimosse 687 righe (MMSI) | Rimosse 4072 righe (Duplicati)\n",
      "Blocco blocco_012-segmentato.parquet: Rimosse 463 righe (MMSI) | Rimosse 4854 righe (Duplicati)\n",
      "Blocco blocco_013-segmentato.parquet: Rimosse 854 righe (MMSI) | Rimosse 6558 righe (Duplicati)\n",
      "Blocco blocco_014-segmentato.parquet: Rimosse 1098 righe (MMSI) | Rimosse 7355 righe (Duplicati)\n",
      "Blocco blocco_015-segmentato.parquet: Rimosse 2270 righe (MMSI) | Rimosse 7089 righe (Duplicati)\n",
      "Blocco blocco_016-segmentato.parquet: Rimosse 1308 righe (MMSI) | Rimosse 9960 righe (Duplicati)\n",
      "Blocco blocco_017-segmentato.parquet: Rimosse 3837 righe (MMSI) | Rimosse 6472 righe (Duplicati)\n",
      "Blocco blocco_018-segmentato.parquet: Rimosse 3552 righe (MMSI) | Rimosse 8105 righe (Duplicati)\n",
      "Blocco blocco_019-segmentato.parquet: Rimosse 1797 righe (MMSI) | Rimosse 6077 righe (Duplicati)\n",
      "Blocco blocco_020-segmentato.parquet: Rimosse 6677 righe (MMSI) | Rimosse 12408 righe (Duplicati)\n",
      "Blocco blocco_021-segmentato.parquet: Rimosse 4321 righe (MMSI) | Rimosse 9882 righe (Duplicati)\n",
      "Blocco blocco_022-segmentato.parquet: Rimosse 1070 righe (MMSI) | Rimosse 8452 righe (Duplicati)\n",
      "Blocco blocco_023-segmentato.parquet: Rimosse 908 righe (MMSI) | Rimosse 14830 righe (Duplicati)\n",
      "Blocco blocco_024-segmentato.parquet: Rimosse 339 righe (MMSI) | Rimosse 3525 righe (Duplicati)\n",
      "\n",
      "--- PULIZIA COMPLETATA ---\n",
      "Totale righe rimosse per MMSI non valido: 48099\n",
      "Totale righe rimosse per duplicati: 161263\n",
      "Il dataset finale Ã¨ in: /home/al3th3ia/Scrivania/Cybersecurity/Detecting-Trajectory-Spoofing-Attacks-on-AIS/Progetto/Pre-Elaborazione Dati/Dataset\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIR = 'Dataset_Stitched_Finale' \n",
    "OUTPUT_DIR_NAME = 'Dataset' \n",
    "OUTPUT_DIR = os.path.join(os.getcwd(), OUTPUT_DIR_NAME)\n",
    "NOME_COLONNA_MMSI = \"MMSI\" \n",
    "\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    shutil.rmtree(OUTPUT_DIR)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Lettura blocchi da: {INPUT_DIR}\")\n",
    "print(f\"Salvataggio in: {OUTPUT_DIR}\\n\")\n",
    "\n",
    "all_blocks = sorted(glob.glob(os.path.join(INPUT_DIR, '*.parquet')))\n",
    "\n",
    "if not all_blocks:\n",
    "    print(f\"ERRORE: Nessun file blocco trovato in {INPUT_DIR}.\")\n",
    "else:\n",
    "    total_duplicates_removed = 0\n",
    "    total_mmsi_removed = 0  \n",
    "    \n",
    "    print(\"Inizio scansione e pulizia (MMSI Specifici + Duplicati)...\")\n",
    "    print(\"Regole MMSI: Rimozione di zeri, '123456789' e cifre ripetute.\")\n",
    "\n",
    "    for block_path in all_blocks:\n",
    "        block_name = os.path.basename(block_path)\n",
    "        df_block = None \n",
    "        \n",
    "        try:\n",
    "            df_block = pd.read_parquet(block_path)\n",
    "            rows_before_total = len(df_block)\n",
    "            \n",
    "            if rows_before_total == 0:\n",
    "                print(f\"Blocco {block_name}: Vuoto. Saltato.\")\n",
    "                continue\n",
    "                \n",
    "            if NOME_COLONNA_MMSI not in df_block.columns:\n",
    "                print(f\"ERRORE: Colonna '{NOME_COLONNA_MMSI}' non trovata in {block_name}. File saltato.\")\n",
    "                continue\n",
    "            \n",
    "            df_block['MMSI_str'] = df_block[NOME_COLONNA_MMSI].astype(str).str.strip()\n",
    "\n",
    "            filtro_lunghezza = df_block['MMSI_str'].str.len() == 9\n",
    "            filtro_numerico = df_block['MMSI_str'].str.isdigit()\n",
    "            filtro_non_zeri = df_block['MMSI_str'] != '000000000'\n",
    "            filtro_non_seq = df_block['MMSI_str'] != '123456789'\n",
    "            filtro_non_ripetuti = df_block['MMSI_str'].apply(lambda x: len(set(x)) > 1)\n",
    "\n",
    "            df_block = df_block[\n",
    "                filtro_lunghezza & \n",
    "                filtro_numerico & \n",
    "                filtro_non_zeri & \n",
    "                filtro_non_seq & \n",
    "                filtro_non_ripetuti\n",
    "            ]\n",
    "            \n",
    "            df_block.drop(columns=['MMSI_str'], inplace=True)\n",
    "            \n",
    "            rows_after_mmsi = len(df_block)\n",
    "            local_mmsi_removed = rows_before_total - rows_after_mmsi\n",
    "            total_mmsi_removed += local_mmsi_removed\n",
    "            \n",
    "\n",
    "            rows_before_duplicates = rows_after_mmsi\n",
    "            df_block.drop_duplicates(inplace=True)\n",
    "            \n",
    "            rows_after_duplicates = len(df_block)\n",
    "            local_duplicates_removed = rows_before_duplicates - rows_after_duplicates\n",
    "            total_duplicates_removed += local_duplicates_removed\n",
    "\n",
    "            print(f\"Blocco {block_name}: Rimosse {local_mmsi_removed} righe (MMSI) | Rimosse {local_duplicates_removed} righe (Duplicati)\")\n",
    "\n",
    "            output_file = os.path.join(OUTPUT_DIR, block_name)\n",
    "            df_block.to_parquet(output_file, index=False, engine='pyarrow', compression='snappy')\n",
    "            \n",
    "        except MemoryError:\n",
    "            print(f\"ERRORE DI MEMORIA: Il blocco {block_name} Ã¨ troppo grande per essere processato.\")\n",
    "            break \n",
    "        except Exception as e:\n",
    "            print(f\"ERRORE durante la pulizia del blocco {block_name}: {e}\")\n",
    "\n",
    "        finally:\n",
    "            if df_block is not None:\n",
    "                del df_block\n",
    "            gc.collect() \n",
    "\n",
    "    print(\"\\n--- PULIZIA COMPLETATA ---\")\n",
    "    print(f\"Totale righe rimosse per MMSI non valido: {total_mmsi_removed}\")\n",
    "    print(f\"Totale righe rimosse per duplicati: {total_duplicates_removed}\")\n",
    "    print(f\"Il dataset finale Ã¨ in: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6e6541",
   "metadata": {},
   "source": [
    "#### Conteggio delle traiettorie uniche con fine valutativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1518358e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando blocco_000-segmentato.parquet...\n",
      "Processando blocco_001-segmentato.parquet...\n",
      "Processando blocco_002-segmentato.parquet...\n",
      "Processando blocco_003-segmentato.parquet...\n",
      "Processando blocco_004-segmentato.parquet...\n",
      "Processando blocco_005-segmentato.parquet...\n",
      "Processando blocco_006-segmentato.parquet...\n",
      "Processando blocco_007-segmentato.parquet...\n",
      "Processando blocco_008-segmentato.parquet...\n",
      "Processando blocco_009-segmentato.parquet...\n",
      "Processando blocco_010-segmentato.parquet...\n",
      "Processando blocco_011-segmentato.parquet...\n",
      "Processando blocco_012-segmentato.parquet...\n",
      "Processando blocco_013-segmentato.parquet...\n",
      "Processando blocco_014-segmentato.parquet...\n",
      "Processando blocco_015-segmentato.parquet...\n",
      "Processando blocco_016-segmentato.parquet...\n",
      "Processando blocco_017-segmentato.parquet...\n",
      "Processando blocco_018-segmentato.parquet...\n",
      "Processando blocco_019-segmentato.parquet...\n",
      "Processando blocco_020-segmentato.parquet...\n",
      "Processando blocco_021-segmentato.parquet...\n",
      "Processando blocco_022-segmentato.parquet...\n",
      "Processando blocco_023-segmentato.parquet...\n",
      "Processando blocco_024-segmentato.parquet...\n",
      "Il tuo dataset finale contiene 6,744,164 traiettorie uniche totali.\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIR = 'Dataset' \n",
    "\n",
    "global_unique_ids = set()  # Utilizziamo il set perchÃ¨ memorizza solo valori unici\n",
    "\n",
    "try:\n",
    "    all_blocks = sorted(glob.glob(os.path.join(INPUT_DIR, '*.parquet')))\n",
    "    \n",
    "    if not all_blocks:\n",
    "        print(f\"ERRORE: Nessun file blocco trovato in {INPUT_DIR}.\")\n",
    "    else:\n",
    "        \n",
    "        for block_path in all_blocks:\n",
    "            print(f\"Processando {os.path.basename(block_path)}...\")\n",
    "            \n",
    "            df = pd.read_parquet(block_path, columns=['TrajectoryID'])\n",
    "            \n",
    "            local_uniques = set(df['TrajectoryID'].unique())\n",
    "            \n",
    "            global_unique_ids.update(local_uniques)\n",
    "            \n",
    "            del df, local_uniques\n",
    "            gc.collect()\n",
    "\n",
    "        numero_totale_traiettorie = len(global_unique_ids)\n",
    "        \n",
    "        print(f\"Il tuo dataset finale contiene {numero_totale_traiettorie:,} traiettorie uniche totali.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERRORE\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f10aa62",
   "metadata": {},
   "source": [
    "#### Controllo finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93148560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MMSI</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>SOG</th>\n",
       "      <th>COG</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>TrajectoryID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.92981</td>\n",
       "      <td>-124.62096</td>\n",
       "      <td>5.6</td>\n",
       "      <td>180.3</td>\n",
       "      <td>2024-07-09 15:37:13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.93671</td>\n",
       "      <td>-124.61779</td>\n",
       "      <td>4.7</td>\n",
       "      <td>21.5</td>\n",
       "      <td>2024-07-09 20:39:09</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.93372</td>\n",
       "      <td>-124.61674</td>\n",
       "      <td>3.0</td>\n",
       "      <td>176.4</td>\n",
       "      <td>2024-07-09 20:56:43</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.93272</td>\n",
       "      <td>-124.61994</td>\n",
       "      <td>3.0</td>\n",
       "      <td>246.4</td>\n",
       "      <td>2024-07-09 20:58:02</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.95354</td>\n",
       "      <td>-124.62340</td>\n",
       "      <td>6.3</td>\n",
       "      <td>10.7</td>\n",
       "      <td>2024-07-09 23:49:54</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.95859</td>\n",
       "      <td>-124.62075</td>\n",
       "      <td>6.4</td>\n",
       "      <td>20.6</td>\n",
       "      <td>2024-07-09 23:53:08</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.96356</td>\n",
       "      <td>-124.61822</td>\n",
       "      <td>6.3</td>\n",
       "      <td>20.1</td>\n",
       "      <td>2024-07-09 23:56:14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.96884</td>\n",
       "      <td>-124.61549</td>\n",
       "      <td>6.7</td>\n",
       "      <td>20.4</td>\n",
       "      <td>2024-07-09 23:59:25</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.97417</td>\n",
       "      <td>-124.61268</td>\n",
       "      <td>6.8</td>\n",
       "      <td>20.8</td>\n",
       "      <td>2024-07-10 00:02:39</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.97894</td>\n",
       "      <td>-124.60952</td>\n",
       "      <td>6.3</td>\n",
       "      <td>25.4</td>\n",
       "      <td>2024-07-10 00:05:46</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.98712</td>\n",
       "      <td>-124.60252</td>\n",
       "      <td>7.4</td>\n",
       "      <td>173.0</td>\n",
       "      <td>2024-07-10 01:38:10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.95275</td>\n",
       "      <td>-124.61481</td>\n",
       "      <td>6.3</td>\n",
       "      <td>213.6</td>\n",
       "      <td>2024-07-10 01:57:13</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.94992</td>\n",
       "      <td>-124.62244</td>\n",
       "      <td>7.4</td>\n",
       "      <td>242.7</td>\n",
       "      <td>2024-07-10 02:00:24</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.94751</td>\n",
       "      <td>-124.63030</td>\n",
       "      <td>7.3</td>\n",
       "      <td>246.9</td>\n",
       "      <td>2024-07-10 02:03:36</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.94453</td>\n",
       "      <td>-124.63737</td>\n",
       "      <td>7.0</td>\n",
       "      <td>239.6</td>\n",
       "      <td>2024-07-10 02:06:44</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.93983</td>\n",
       "      <td>-124.64311</td>\n",
       "      <td>7.5</td>\n",
       "      <td>221.3</td>\n",
       "      <td>2024-07-10 02:09:59</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.93617</td>\n",
       "      <td>-124.64938</td>\n",
       "      <td>6.9</td>\n",
       "      <td>231.0</td>\n",
       "      <td>2024-07-10 02:13:06</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.93387</td>\n",
       "      <td>-124.65702</td>\n",
       "      <td>7.1</td>\n",
       "      <td>247.2</td>\n",
       "      <td>2024-07-10 02:16:19</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.93172</td>\n",
       "      <td>-124.66430</td>\n",
       "      <td>6.8</td>\n",
       "      <td>247.7</td>\n",
       "      <td>2024-07-10 02:19:30</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.92863</td>\n",
       "      <td>-124.66380</td>\n",
       "      <td>5.3</td>\n",
       "      <td>165.0</td>\n",
       "      <td>2024-07-10 02:25:56</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.91717</td>\n",
       "      <td>-124.66266</td>\n",
       "      <td>3.7</td>\n",
       "      <td>210.1</td>\n",
       "      <td>2024-07-10 02:35:32</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.91949</td>\n",
       "      <td>-124.67176</td>\n",
       "      <td>4.6</td>\n",
       "      <td>322.5</td>\n",
       "      <td>2024-07-10 02:41:49</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.93281</td>\n",
       "      <td>-124.68434</td>\n",
       "      <td>4.3</td>\n",
       "      <td>340.3</td>\n",
       "      <td>2024-07-10 02:54:32</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.91386</td>\n",
       "      <td>-124.65799</td>\n",
       "      <td>4.0</td>\n",
       "      <td>352.9</td>\n",
       "      <td>2024-07-10 08:43:43</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.91753</td>\n",
       "      <td>-124.66124</td>\n",
       "      <td>5.2</td>\n",
       "      <td>327.4</td>\n",
       "      <td>2024-07-10 08:46:48</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.92642</td>\n",
       "      <td>-124.66605</td>\n",
       "      <td>6.3</td>\n",
       "      <td>347.4</td>\n",
       "      <td>2024-07-10 08:53:10</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.90848</td>\n",
       "      <td>-124.65206</td>\n",
       "      <td>6.7</td>\n",
       "      <td>292.4</td>\n",
       "      <td>2024-07-10 11:22:27</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.90829</td>\n",
       "      <td>-124.66014</td>\n",
       "      <td>6.9</td>\n",
       "      <td>268.1</td>\n",
       "      <td>2024-07-10 11:25:33</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.90810</td>\n",
       "      <td>-124.66814</td>\n",
       "      <td>6.9</td>\n",
       "      <td>268.0</td>\n",
       "      <td>2024-07-10 11:28:39</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.90783</td>\n",
       "      <td>-124.67614</td>\n",
       "      <td>6.9</td>\n",
       "      <td>267.4</td>\n",
       "      <td>2024-07-10 11:31:45</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.90856</td>\n",
       "      <td>-124.68434</td>\n",
       "      <td>7.1</td>\n",
       "      <td>276.9</td>\n",
       "      <td>2024-07-10 11:34:56</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.90870</td>\n",
       "      <td>-124.69276</td>\n",
       "      <td>7.2</td>\n",
       "      <td>271.3</td>\n",
       "      <td>2024-07-10 11:38:08</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.90776</td>\n",
       "      <td>-124.70126</td>\n",
       "      <td>7.4</td>\n",
       "      <td>261.2</td>\n",
       "      <td>2024-07-10 11:41:18</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.90700</td>\n",
       "      <td>-124.70975</td>\n",
       "      <td>7.4</td>\n",
       "      <td>262.9</td>\n",
       "      <td>2024-07-10 11:44:28</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.90649</td>\n",
       "      <td>-124.71862</td>\n",
       "      <td>7.7</td>\n",
       "      <td>265.4</td>\n",
       "      <td>2024-07-10 11:47:40</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.90629</td>\n",
       "      <td>-124.72702</td>\n",
       "      <td>7.2</td>\n",
       "      <td>268.1</td>\n",
       "      <td>2024-07-10 11:50:47</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.90651</td>\n",
       "      <td>-124.73548</td>\n",
       "      <td>7.3</td>\n",
       "      <td>272.0</td>\n",
       "      <td>2024-07-10 11:54:00</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.90658</td>\n",
       "      <td>-124.74356</td>\n",
       "      <td>6.9</td>\n",
       "      <td>270.7</td>\n",
       "      <td>2024-07-10 11:57:13</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.90678</td>\n",
       "      <td>-124.75932</td>\n",
       "      <td>6.9</td>\n",
       "      <td>273.3</td>\n",
       "      <td>2024-07-10 12:03:32</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.90762</td>\n",
       "      <td>-124.76682</td>\n",
       "      <td>6.5</td>\n",
       "      <td>278.7</td>\n",
       "      <td>2024-07-10 12:06:40</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.90810</td>\n",
       "      <td>-124.77497</td>\n",
       "      <td>7.0</td>\n",
       "      <td>274.6</td>\n",
       "      <td>2024-07-10 12:09:51</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.90783</td>\n",
       "      <td>-124.78283</td>\n",
       "      <td>6.8</td>\n",
       "      <td>267.3</td>\n",
       "      <td>2024-07-10 12:12:59</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.90619</td>\n",
       "      <td>-124.79094</td>\n",
       "      <td>7.2</td>\n",
       "      <td>254.3</td>\n",
       "      <td>2024-07-10 12:16:08</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.90754</td>\n",
       "      <td>-124.79542</td>\n",
       "      <td>4.2</td>\n",
       "      <td>292.6</td>\n",
       "      <td>2024-07-10 12:19:21</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.91332</td>\n",
       "      <td>-124.87436</td>\n",
       "      <td>7.0</td>\n",
       "      <td>306.1</td>\n",
       "      <td>2024-07-10 13:10:01</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.93039</td>\n",
       "      <td>-124.87463</td>\n",
       "      <td>6.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2024-07-10 13:19:31</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.92585</td>\n",
       "      <td>-124.89269</td>\n",
       "      <td>6.1</td>\n",
       "      <td>287.9</td>\n",
       "      <td>2024-07-10 13:51:15</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.93127</td>\n",
       "      <td>-124.89259</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2024-07-10 13:54:25</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.93655</td>\n",
       "      <td>-124.89223</td>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2024-07-10 13:57:37</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.94198</td>\n",
       "      <td>-124.89179</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3.3</td>\n",
       "      <td>2024-07-10 14:00:56</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         MMSI  Latitude  Longitude  SOG    COG           Timestamp  \\\n",
       "0   100011758  43.92981 -124.62096  5.6  180.3 2024-07-09 15:37:13   \n",
       "1   100011758  43.93671 -124.61779  4.7   21.5 2024-07-09 20:39:09   \n",
       "2   100011758  43.93372 -124.61674  3.0  176.4 2024-07-09 20:56:43   \n",
       "3   100011758  43.93272 -124.61994  3.0  246.4 2024-07-09 20:58:02   \n",
       "4   100011758  43.95354 -124.62340  6.3   10.7 2024-07-09 23:49:54   \n",
       "5   100011758  43.95859 -124.62075  6.4   20.6 2024-07-09 23:53:08   \n",
       "6   100011758  43.96356 -124.61822  6.3   20.1 2024-07-09 23:56:14   \n",
       "7   100011758  43.96884 -124.61549  6.7   20.4 2024-07-09 23:59:25   \n",
       "8   100011758  43.97417 -124.61268  6.8   20.8 2024-07-10 00:02:39   \n",
       "9   100011758  43.97894 -124.60952  6.3   25.4 2024-07-10 00:05:46   \n",
       "10  100011758  43.98712 -124.60252  7.4  173.0 2024-07-10 01:38:10   \n",
       "11  100011758  43.95275 -124.61481  6.3  213.6 2024-07-10 01:57:13   \n",
       "12  100011758  43.94992 -124.62244  7.4  242.7 2024-07-10 02:00:24   \n",
       "13  100011758  43.94751 -124.63030  7.3  246.9 2024-07-10 02:03:36   \n",
       "14  100011758  43.94453 -124.63737  7.0  239.6 2024-07-10 02:06:44   \n",
       "15  100011758  43.93983 -124.64311  7.5  221.3 2024-07-10 02:09:59   \n",
       "16  100011758  43.93617 -124.64938  6.9  231.0 2024-07-10 02:13:06   \n",
       "17  100011758  43.93387 -124.65702  7.1  247.2 2024-07-10 02:16:19   \n",
       "18  100011758  43.93172 -124.66430  6.8  247.7 2024-07-10 02:19:30   \n",
       "19  100011758  43.92863 -124.66380  5.3  165.0 2024-07-10 02:25:56   \n",
       "20  100011758  43.91717 -124.66266  3.7  210.1 2024-07-10 02:35:32   \n",
       "21  100011758  43.91949 -124.67176  4.6  322.5 2024-07-10 02:41:49   \n",
       "22  100011758  43.93281 -124.68434  4.3  340.3 2024-07-10 02:54:32   \n",
       "23  100011758  43.91386 -124.65799  4.0  352.9 2024-07-10 08:43:43   \n",
       "24  100011758  43.91753 -124.66124  5.2  327.4 2024-07-10 08:46:48   \n",
       "25  100011758  43.92642 -124.66605  6.3  347.4 2024-07-10 08:53:10   \n",
       "26  100011758  43.90848 -124.65206  6.7  292.4 2024-07-10 11:22:27   \n",
       "27  100011758  43.90829 -124.66014  6.9  268.1 2024-07-10 11:25:33   \n",
       "28  100011758  43.90810 -124.66814  6.9  268.0 2024-07-10 11:28:39   \n",
       "29  100011758  43.90783 -124.67614  6.9  267.4 2024-07-10 11:31:45   \n",
       "30  100011758  43.90856 -124.68434  7.1  276.9 2024-07-10 11:34:56   \n",
       "31  100011758  43.90870 -124.69276  7.2  271.3 2024-07-10 11:38:08   \n",
       "32  100011758  43.90776 -124.70126  7.4  261.2 2024-07-10 11:41:18   \n",
       "33  100011758  43.90700 -124.70975  7.4  262.9 2024-07-10 11:44:28   \n",
       "34  100011758  43.90649 -124.71862  7.7  265.4 2024-07-10 11:47:40   \n",
       "35  100011758  43.90629 -124.72702  7.2  268.1 2024-07-10 11:50:47   \n",
       "36  100011758  43.90651 -124.73548  7.3  272.0 2024-07-10 11:54:00   \n",
       "37  100011758  43.90658 -124.74356  6.9  270.7 2024-07-10 11:57:13   \n",
       "38  100011758  43.90678 -124.75932  6.9  273.3 2024-07-10 12:03:32   \n",
       "39  100011758  43.90762 -124.76682  6.5  278.7 2024-07-10 12:06:40   \n",
       "40  100011758  43.90810 -124.77497  7.0  274.6 2024-07-10 12:09:51   \n",
       "41  100011758  43.90783 -124.78283  6.8  267.3 2024-07-10 12:12:59   \n",
       "42  100011758  43.90619 -124.79094  7.2  254.3 2024-07-10 12:16:08   \n",
       "43  100011758  43.90754 -124.79542  4.2  292.6 2024-07-10 12:19:21   \n",
       "44  100011758  43.91332 -124.87436  7.0  306.1 2024-07-10 13:10:01   \n",
       "45  100011758  43.93039 -124.87463  6.4    1.6 2024-07-10 13:19:31   \n",
       "46  100011758  43.92585 -124.89269  6.1  287.9 2024-07-10 13:51:15   \n",
       "47  100011758  43.93127 -124.89259  6.4    0.7 2024-07-10 13:54:25   \n",
       "48  100011758  43.93655 -124.89223  6.3    2.8 2024-07-10 13:57:37   \n",
       "49  100011758  43.94198 -124.89179  6.5    3.3 2024-07-10 14:00:56   \n",
       "\n",
       "    TrajectoryID  \n",
       "0              1  \n",
       "1              2  \n",
       "2              2  \n",
       "3              2  \n",
       "4              3  \n",
       "5              3  \n",
       "6              3  \n",
       "7              3  \n",
       "8              3  \n",
       "9              3  \n",
       "10             4  \n",
       "11             4  \n",
       "12             4  \n",
       "13             4  \n",
       "14             4  \n",
       "15             4  \n",
       "16             4  \n",
       "17             4  \n",
       "18             4  \n",
       "19             4  \n",
       "20             4  \n",
       "21             4  \n",
       "22             4  \n",
       "23             5  \n",
       "24             5  \n",
       "25             5  \n",
       "26             6  \n",
       "27             6  \n",
       "28             6  \n",
       "29             6  \n",
       "30             6  \n",
       "31             6  \n",
       "32             6  \n",
       "33             6  \n",
       "34             6  \n",
       "35             6  \n",
       "36             6  \n",
       "37             6  \n",
       "38             6  \n",
       "39             6  \n",
       "40             6  \n",
       "41             6  \n",
       "42             6  \n",
       "43             6  \n",
       "44             6  \n",
       "45             6  \n",
       "46             6  \n",
       "47             6  \n",
       "48             6  \n",
       "49             6  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FILE_PATH_TEST = 'Dataset/blocco_000-segmentato.parquet'\n",
    "COLUMNS_TO_READ = ['MMSI', 'Latitude', 'Longitude','SOG', 'COG', 'Timestamp','TrajectoryID']\n",
    "\n",
    "df = pd.read_parquet(\n",
    "        FILE_PATH_TEST, \n",
    "        columns=COLUMNS_TO_READ,\n",
    "        engine='pyarrow' \n",
    "    )\n",
    "\n",
    "df.head(50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08c88e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di righe: 39959464, Numero di colonne: 7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows,columns = df.shape\n",
    "print(f\"Numero di righe: {rows}, Numero di colonne: {columns}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad86942",
   "metadata": {},
   "source": [
    "##### Diagnosi del dataset per identificare gap all'interno delle traiettorie > 10 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40d81490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” AVVIO DIAGNOSI DATASET: Dataset\n",
      "   > Regola Gap Max: 10.0 min\n",
      "   > Regola Lunghezza Min: 30 righe\n",
      "------------------------------------------------------------\n",
      "[25/25] Analizzato blocco_024-segmentato.parquet\n",
      "============================================================\n",
      "ðŸ“Š RISULTATI ANALISI GLOBALE\n",
      "Totale Traiettorie Uniche: 6,743,290\n",
      "------------------------------\n",
      "âŒ Scartate per Gap > 10min:   3,634,344 (53.9%)\n",
      "âŒ Scartate per Righe < 30:    2,124,181 (31.5%)\n",
      "------------------------------\n",
      "âœ… TRAIETTORIE VALIDE (Target): 984,765 (14.6%)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# --- CONFIGURAZIONE ---\n",
    "INPUT_DIR = 'Dataset'  # La cartella finale del tuo notebook\n",
    "GAP_LIMIT_SECONDS = 10 * 60  # 10 minuti\n",
    "MIN_LENGTH = 30  # Filtro rete neurale\n",
    "\n",
    "print(f\"ðŸ” AVVIO DIAGNOSI DATASET: {INPUT_DIR}\")\n",
    "print(f\"   > Regola Gap Max: {GAP_LIMIT_SECONDS/60} min\")\n",
    "print(f\"   > Regola Lunghezza Min: {MIN_LENGTH} righe\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "all_files = sorted(glob.glob(os.path.join(INPUT_DIR, '*.parquet')))\n",
    "\n",
    "# Contatori Globali\n",
    "total_trajectories = 0\n",
    "valid_trajectories = 0\n",
    "dropped_gap_10m = 0\n",
    "dropped_too_short = 0\n",
    "\n",
    "# Dizionario per tracciare ID che attraversano i file (per statistiche corrette)\n",
    "# Mappa: ID -> {max_gap_found: bool, row_count: int}\n",
    "global_stats = {}\n",
    "\n",
    "for i, file_path in enumerate(all_files):\n",
    "    try:\n",
    "        # Leggiamo solo le colonne necessarie per la diagnosi\n",
    "        df = pl.read_parquet(file_path).select(['TrajectoryID', 'Timestamp'])\n",
    "        \n",
    "        # Casting e Ordinamento\n",
    "        df = df.with_columns(pl.col(\"Timestamp\").cast(pl.Datetime))\n",
    "        df = df.sort([\"TrajectoryID\", \"Timestamp\"])\n",
    "        \n",
    "        # Calcolo Gap Temporali (Delta tra righe consecutive dello stesso ID)\n",
    "        df = df.with_columns([\n",
    "            pl.col(\"Timestamp\").diff().dt.total_seconds().over(\"TrajectoryID\").alias(\"delta_sec\")\n",
    "        ])\n",
    "        \n",
    "        # Aggregazione Statistica per ID in questo file\n",
    "        stats = df.group_by(\"TrajectoryID\").agg([\n",
    "            pl.col(\"delta_sec\").max().alias(\"max_gap\"),\n",
    "            pl.len().alias(\"count\")\n",
    "        ])\n",
    "        \n",
    "        # Aggiornamento Statistiche Globali\n",
    "        # Nota: PoichÃ© un ID puÃ² essere spalmato su piÃ¹ file, dobbiamo accumulare\n",
    "        for row in stats.iter_rows(named=True):\n",
    "            tid = row['TrajectoryID']\n",
    "            gap = row['max_gap'] if row['max_gap'] is not None else 0\n",
    "            cnt = row['count']\n",
    "            \n",
    "            if tid not in global_stats:\n",
    "                global_stats[tid] = {'max_gap': gap, 'total_rows': cnt}\n",
    "            else:\n",
    "                # Aggiorniamo il gap massimo visto finora e sommiamo le righe\n",
    "                global_stats[tid]['max_gap'] = max(global_stats[tid]['max_gap'], gap)\n",
    "                global_stats[tid]['total_rows'] += cnt\n",
    "        \n",
    "        print(f\"[{i+1}/{len(all_files)}] Analizzato {os.path.basename(file_path)}\", end='\\r')\n",
    "        \n",
    "        del df, stats\n",
    "        gc.collect()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Errore su {file_path}: {e}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ðŸ“Š RISULTATI ANALISI GLOBALE\")\n",
    "\n",
    "# Elaborazione finale dai dati accumulati\n",
    "for tid, data in global_stats.items():\n",
    "    total_trajectories += 1\n",
    "    \n",
    "    # Verifica Regole\n",
    "    is_gap_ok = data['max_gap'] <= GAP_LIMIT_SECONDS\n",
    "    is_len_ok = data['total_rows'] >= MIN_LENGTH\n",
    "    \n",
    "    if not is_gap_ok:\n",
    "        dropped_gap_10m += 1\n",
    "    elif not is_len_ok: # Se il gap Ã¨ ok, controlliamo la lunghezza\n",
    "        dropped_too_short += 1\n",
    "    else:\n",
    "        valid_trajectories += 1\n",
    "\n",
    "print(f\"Totale Traiettorie Uniche: {total_trajectories:,}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"âŒ Scartate per Gap > 10min:   {dropped_gap_10m:,} ({(dropped_gap_10m/total_trajectories)*100:.1f}%)\")\n",
    "print(f\"âŒ Scartate per Righe < 30:    {dropped_too_short:,} ({(dropped_too_short/total_trajectories)*100:.1f}%)\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"âœ… TRAIETTORIE VALIDE (Target): {valid_trajectories:,} ({(valid_trajectories/total_trajectories)*100:.1f}%)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8bdd50",
   "metadata": {},
   "source": [
    "##### Algoritmo di pulizia salti temporali e interpolazione per l'intero dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a4cc458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ­ AVVIO V11: PRODUZIONE FINALE (FULL DATASET)\n",
      "   > Input: 25 file\n",
      "   > Output: Dataset_Ready_For_AI_FINAL\n",
      "   > Batch Size: 250 (Safe Mode)\n",
      "------------------------------------------------------------\n",
      "[1/25] blocco_000-segmentato.parquet: COMPLETATO.            \n",
      "[2/25] blocco_001-segmentato.parquet: COMPLETATO.            \n",
      "[3/25] blocco_002-segmentato.parquet: COMPLETATO.            \n",
      "[4/25] blocco_003-segmentato.parquet: COMPLETATO.            \n",
      "[5/25] blocco_004-segmentato.parquet: COMPLETATO.            \n",
      "[6/25] blocco_005-segmentato.parquet: COMPLETATO.            \n",
      "[7/25] blocco_006-segmentato.parquet: COMPLETATO.            \n",
      "[8/25] blocco_007-segmentato.parquet: COMPLETATO.            \n",
      "[9/25] blocco_008-segmentato.parquet: COMPLETATO.            \n",
      "[10/25] blocco_009-segmentato.parquet: COMPLETATO.            \n",
      "[11/25] blocco_010-segmentato.parquet: COMPLETATO.            \n",
      "[12/25] blocco_011-segmentato.parquet: COMPLETATO.            \n",
      "[13/25] blocco_012-segmentato.parquet: COMPLETATO.            \n",
      "[14/25] blocco_013-segmentato.parquet: COMPLETATO.            \n",
      "[15/25] blocco_014-segmentato.parquet: COMPLETATO.            \n",
      "[16/25] blocco_015-segmentato.parquet: COMPLETATO.            \n",
      "[17/25] blocco_016-segmentato.parquet: COMPLETATO.            \n",
      "[18/25] blocco_017-segmentato.parquet: COMPLETATO.            \n",
      "[19/25] blocco_018-segmentato.parquet: COMPLETATO.            \n",
      "[20/25] blocco_019-segmentato.parquet: COMPLETATO.            \n",
      "[21/25] blocco_020-segmentato.parquet: COMPLETATO.            \n",
      "[22/25] blocco_021-segmentato.parquet: COMPLETATO.            \n",
      "[23/25] blocco_022-segmentato.parquet: COMPLETATO.            \n",
      "[24/25] blocco_023-segmentato.parquet: COMPLETATO.            \n",
      "[25/25] blocco_024-segmentato.parquet: COMPLETATO.            \n",
      "\n",
      "============================================================\n",
      "ðŸŽ‰ PRODUZIONE COMPLETATA\n",
      "   Output: Dataset_Ready_For_AI_FINAL\n",
      "   Totale Righe Stimate: 215,538,298\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import gc\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "# --- CONFIGURAZIONE ---\n",
    "INPUT_DIR = 'Dataset'\n",
    "OUTPUT_DIR = 'Dataset_Ready_For_AI_FINAL'\n",
    "\n",
    "# Parametri Blindati\n",
    "GAP_LIMIT_MINUTES = 10      # Se buco > 10 min sui dati grezzi, scarta ID\n",
    "MIN_LENGTH_ROWS = 30        # Minima lunghezza traiettoria finale\n",
    "BATCH_SIZE = 250            # Aumentato leggermente per velocitÃ , ma SAFE per RAM\n",
    "MAX_DURATION_DAYS = 20      # Safety Valve per date corrotte\n",
    "\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    shutil.rmtree(OUTPUT_DIR)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "COLS = ['MMSI', 'Latitude', 'Longitude', 'SOG', 'COG', 'Timestamp', 'TrajectoryID']\n",
    "all_files = sorted(glob.glob(os.path.join(INPUT_DIR, '*.parquet')))\n",
    "\n",
    "print(f\"ðŸ­ AVVIO V11: PRODUZIONE FINALE (FULL DATASET)\")\n",
    "print(f\"   > Input: {len(all_files)} file\")\n",
    "print(f\"   > Output: {OUTPUT_DIR}\")\n",
    "print(f\"   > Batch Size: {BATCH_SIZE} (Safe Mode)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "previous_tail_df = None\n",
    "total_rows_final = 0\n",
    "\n",
    "for i, file_path in enumerate(all_files):\n",
    "    out_name = os.path.basename(file_path)\n",
    "    \n",
    "    # Feedback visivo semplice\n",
    "    print(f\"[{i+1}/{len(all_files)}] Elaborazione {out_name}...\", end='\\r')\n",
    "    \n",
    "    try:\n",
    "        # 1. CARICAMENTO\n",
    "        df = pl.read_parquet(file_path).select(COLS)\n",
    "        df = df.with_columns(pl.col(\"Timestamp\").cast(pl.Datetime(\"us\")))\n",
    "        \n",
    "        # 2. STITCHING (Buffer)\n",
    "        if previous_tail_df is not None:\n",
    "            previous_tail_df = previous_tail_df.select(COLS)\n",
    "            df = pl.concat([previous_tail_df, df], how=\"vertical\")\n",
    "        \n",
    "        df = df.sort([\"TrajectoryID\", \"Timestamp\"])\n",
    "        current_tail = df.group_by(\"TrajectoryID\", maintain_order=True).last()\n",
    "\n",
    "        # 3. KILLER FILTER (Pre-Kill Gaps > 10m)\n",
    "        df = df.with_columns([\n",
    "            pl.col(\"Timestamp\").diff().dt.total_seconds().over(\"TrajectoryID\").fill_null(0).alias(\"delta_sec\")\n",
    "        ])\n",
    "        \n",
    "        bad_gap_ids = df.filter(pl.col(\"delta_sec\") > (GAP_LIMIT_MINUTES * 60))[\"TrajectoryID\"].unique().to_list()\n",
    "        \n",
    "        if len(bad_gap_ids) > 0:\n",
    "            df = df.filter(~pl.col(\"TrajectoryID\").is_in(bad_gap_ids))\n",
    "            \n",
    "        df = df.drop(\"delta_sec\")\n",
    "\n",
    "        if df.height == 0:\n",
    "            previous_tail_df = None\n",
    "            continue\n",
    "\n",
    "        # 4. PREPARAZIONE VETTORIALE (Snap-to-Grid)\n",
    "        df = df.with_columns([\n",
    "            pl.col(\"Timestamp\").dt.round(\"1m\").alias(\"GridTime\"),\n",
    "            (pl.col(\"COG\") * np.pi / 180).sin().alias(\"cog_sin\"),\n",
    "            (pl.col(\"COG\") * np.pi / 180).cos().alias(\"cog_cos\")\n",
    "        ])\n",
    "\n",
    "        # 5. CONSOLIDAMENTO\n",
    "        data_consolidated = (\n",
    "            df.group_by([\"TrajectoryID\", \"GridTime\"])\n",
    "            .agg([\n",
    "                pl.col(\"Latitude\").mean(),\n",
    "                pl.col(\"Longitude\").mean(),\n",
    "                pl.col(\"SOG\").mean(),\n",
    "                pl.col(\"cog_sin\").mean(),\n",
    "                pl.col(\"cog_cos\").mean(),\n",
    "                pl.col(\"MMSI\").first()\n",
    "            ])\n",
    "            .sort([\"TrajectoryID\", \"GridTime\"])\n",
    "        )\n",
    "        \n",
    "        # Free RAM\n",
    "        del df\n",
    "        gc.collect()\n",
    "\n",
    "        # 6. MICRO-BATCHING\n",
    "        unique_ids = data_consolidated[\"TrajectoryID\"].unique(maintain_order=True).to_list()\n",
    "        processed_chunks = []\n",
    "        \n",
    "        # Loop batch\n",
    "        for k_idx, k in enumerate(range(0, len(unique_ids), BATCH_SIZE)):\n",
    "            batch_ids = unique_ids[k : k + BATCH_SIZE]\n",
    "            batch_data = data_consolidated.filter(pl.col(\"TrajectoryID\").is_in(batch_ids))\n",
    "            \n",
    "            # Calcolo Range\n",
    "            ranges = (\n",
    "                batch_data.group_by(\"TrajectoryID\")\n",
    "                .agg([\n",
    "                    pl.col(\"GridTime\").min().alias(\"start\"),\n",
    "                    pl.col(\"GridTime\").max().alias(\"end\")\n",
    "                ])\n",
    "                .with_columns([\n",
    "                    (pl.col(\"end\") - pl.col(\"start\")).dt.total_days().alias(\"duration_days\")\n",
    "                ])\n",
    "            )\n",
    "            \n",
    "            valid_ranges = ranges.filter(pl.col(\"duration_days\") <= MAX_DURATION_DAYS)\n",
    "            \n",
    "            if valid_ranges.height > 0:\n",
    "                # Creazione Scheletro (Fix V10 applicato)\n",
    "                skeleton = (\n",
    "                    valid_ranges.select([\n",
    "                        \"TrajectoryID\",\n",
    "                        pl.datetime_ranges(\n",
    "                            pl.col(\"start\"), \n",
    "                            pl.col(\"end\"), \n",
    "                            interval=\"1m\", \n",
    "                            closed=\"both\"\n",
    "                        ).alias(\"GridTime\")\n",
    "                    ])\n",
    "                    .explode(\"GridTime\")\n",
    "                )\n",
    "                \n",
    "                joined = skeleton.join(batch_data, on=[\"TrajectoryID\", \"GridTime\"], how=\"left\")\n",
    "                \n",
    "                # Interpolazione Coerente\n",
    "                final_batch = joined.with_columns([\n",
    "                    pl.col(\"Latitude\").interpolate().over(\"TrajectoryID\"),\n",
    "                    pl.col(\"Longitude\").interpolate().over(\"TrajectoryID\"),\n",
    "                    pl.col(\"SOG\").interpolate().clip(0, 200).over(\"TrajectoryID\"),\n",
    "                    pl.col(\"cog_sin\").interpolate().over(\"TrajectoryID\"),\n",
    "                    pl.col(\"cog_cos\").interpolate().over(\"TrajectoryID\"),\n",
    "                    pl.col(\"MMSI\").forward_fill().backward_fill().over(\"TrajectoryID\")\n",
    "                ])\n",
    "                \n",
    "                # Ricostruzione COG\n",
    "                final_batch = final_batch.with_columns([\n",
    "                    (np.arctan2(pl.col(\"cog_sin\"), pl.col(\"cog_cos\")) * 180 / np.pi).alias(\"COG_new\")\n",
    "                ]).with_columns([\n",
    "                    ((pl.col(\"COG_new\") + 360) % 360).alias(\"COG\")\n",
    "                ]).rename({\"GridTime\": \"Timestamp\"})\n",
    "                \n",
    "                # Filtro Lunghezza\n",
    "                id_counts = final_batch.group_by(\"TrajectoryID\").len()\n",
    "                valid_len_list = id_counts.filter(pl.col(\"len\") >= MIN_LENGTH_ROWS)[\"TrajectoryID\"].to_list()\n",
    "                final_batch = final_batch.filter(pl.col(\"TrajectoryID\").is_in(valid_len_list))\n",
    "                \n",
    "                # Deduplica Buffer e Selezione Colonne\n",
    "                final_batch = final_batch.select(COLS).unique(subset=['TrajectoryID', 'Timestamp'], keep='last', maintain_order=True)\n",
    "                \n",
    "                processed_chunks.append(final_batch)\n",
    "            \n",
    "            # Pulizia RAM aggressiva intra-batch\n",
    "            del batch_data, ranges, valid_ranges\n",
    "            if 'skeleton' in locals(): del skeleton\n",
    "            if 'joined' in locals(): del joined\n",
    "            if 'final_batch' in locals(): del final_batch\n",
    "            \n",
    "            # Feedback ogni 20 batch per non intasare il log\n",
    "            if k_idx % 20 == 0:\n",
    "                print(f\"[{i+1}/{len(all_files)}] Batch {k_idx} processato...\", end='\\r')\n",
    "\n",
    "        # Salvataggio\n",
    "        if processed_chunks:\n",
    "            final_df = pl.concat(processed_chunks).sort([\"TrajectoryID\", \"Timestamp\"])\n",
    "            out_path = os.path.join(OUTPUT_DIR, out_name)\n",
    "            final_df.write_parquet(out_path, compression='zstd')\n",
    "            \n",
    "            total_rows_final += final_df.height\n",
    "            del final_df\n",
    "        \n",
    "        previous_tail_df = current_tail\n",
    "        del data_consolidated, processed_chunks, unique_ids\n",
    "        gc.collect()\n",
    "        \n",
    "        # Conferma completamento file\n",
    "        print(f\"[{i+1}/{len(all_files)}] {out_name}: COMPLETATO.            \")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâš ï¸ ERRORE {file_path}: {e}\")\n",
    "        previous_tail_df = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"ðŸŽ‰ PRODUZIONE COMPLETATA\")\n",
    "print(f\"   Output: {OUTPUT_DIR}\")\n",
    "print(f\"   Totale Righe Stimate: {total_rows_final:,}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770a8a3e",
   "metadata": {},
   "source": [
    "##### Controllo integritÃ  temporale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19fa5aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•µï¸ VERIFICA INTEGRITÃ€ TEST 3 FILE\n",
      "âœ… blocco_000-segmentato.parquet: OK (Tutti delta = 60s)\n",
      "âœ… blocco_001-segmentato.parquet: OK (Tutti delta = 60s)\n",
      "âœ… blocco_002-segmentato.parquet: OK (Tutti delta = 60s)\n",
      "âœ… blocco_003-segmentato.parquet: OK (Tutti delta = 60s)\n",
      "âœ… blocco_004-segmentato.parquet: OK (Tutti delta = 60s)\n",
      "âœ… blocco_005-segmentato.parquet: OK (Tutti delta = 60s)\n",
      "âœ… blocco_006-segmentato.parquet: OK (Tutti delta = 60s)\n",
      "âœ… blocco_007-segmentato.parquet: OK (Tutti delta = 60s)\n",
      "âœ… blocco_008-segmentato.parquet: OK (Tutti delta = 60s)\n",
      "âœ… blocco_009-segmentato.parquet: OK (Tutti delta = 60s)\n",
      "âœ… blocco_010-segmentato.parquet: OK (Tutti delta = 60s)\n",
      "âœ… blocco_011-segmentato.parquet: OK (Tutti delta = 60s)\n",
      "âœ… blocco_012-segmentato.parquet: OK (Tutti delta = 60s)\n",
      "âœ… blocco_013-segmentato.parquet: OK (Tutti delta = 60s)\n",
      "âœ… blocco_014-segmentato.parquet: OK (Tutti delta = 60s)\n",
      "âœ… blocco_015-segmentato.parquet: OK (Tutti delta = 60s)\n",
      "âœ… blocco_016-segmentato.parquet: OK (Tutti delta = 60s)\n",
      "âœ… blocco_017-segmentato.parquet: OK (Tutti delta = 60s)\n",
      "âœ… blocco_018-segmentato.parquet: OK (Tutti delta = 60s)\n",
      "âœ… blocco_019-segmentato.parquet: OK (Tutti delta = 60s)\n",
      "âœ… blocco_020-segmentato.parquet: OK (Tutti delta = 60s)\n",
      "âœ… blocco_021-segmentato.parquet: OK (Tutti delta = 60s)\n",
      "âœ… blocco_022-segmentato.parquet: OK (Tutti delta = 60s)\n",
      "âœ… blocco_023-segmentato.parquet: OK (Tutti delta = 60s)\n",
      "âœ… blocco_024-segmentato.parquet: OK (Tutti delta = 60s)\n",
      "------------------------------\n",
      "ðŸŽ‰ SUCCESSO TOTALE: L'algoritmo funziona ed Ã¨ sicuro.\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import glob\n",
    "import os\n",
    "\n",
    "INPUT_DIR = 'Dataset_Ready_For_AI_FINAL'  # La cartella di test\n",
    "\n",
    "print(\"ðŸ•µï¸ VERIFICA INTEGRITÃ€ TEST 3 FILE\")\n",
    "all_files = sorted(glob.glob(os.path.join(INPUT_DIR, '*.parquet')))\n",
    "\n",
    "total_broken = 0\n",
    "\n",
    "for f in all_files:\n",
    "    df = pl.read_parquet(f).select(['TrajectoryID', 'Timestamp'])\n",
    "    \n",
    "    # Calcolo delta\n",
    "    df = df.with_columns([\n",
    "        pl.col(\"Timestamp\").diff().dt.total_seconds().over(\"TrajectoryID\").alias(\"delta\")\n",
    "    ])\n",
    "    \n",
    "    # Cerchiamo errori (delta != 60s e non null)\n",
    "    # Tolleranza 0.1s\n",
    "    errors = df.filter(\n",
    "        (pl.col(\"delta\").is_not_null()) & \n",
    "        ((pl.col(\"delta\") - 60).abs() > 0.1)\n",
    "    )\n",
    "    \n",
    "    count = errors.height\n",
    "    total_broken += count\n",
    "    \n",
    "    if count == 0:\n",
    "        print(f\"âœ… {os.path.basename(f)}: OK (Tutti delta = 60s)\")\n",
    "    else:\n",
    "        print(f\"âŒ {os.path.basename(f)}: {count} errori trovati!\")\n",
    "        print(errors.head())\n",
    "\n",
    "print(\"-\" * 30)\n",
    "if total_broken == 0:\n",
    "    print(\"ðŸŽ‰ SUCCESSO TOTALE: L'algoritmo funziona ed Ã¨ sicuro.\")\n",
    "else:\n",
    "    print(\"âš ï¸ Ci sono ancora problemi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e922abf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•µï¸  AVVIO DIAGNOSI INTEGRITÃ€ TEMPORALE\n",
      "    Target: Dataset_Ready_For_AI_FINAL\n",
      "------------------------------------------------------------\n",
      "[25/25] blocco_024-segmentato.parquet: âœ… IntegritÃ  OK.\n",
      "============================================================\n",
      "ðŸ“Š REPORT DIAGNOSTICO\n",
      "============================================================\n",
      "Totale Traiettorie: 1,232,449\n",
      "Traiettorie Guaste: 0\n",
      "Percentuale Danno:  0.00%\n",
      "------------------------------------------------------------\n",
      "ðŸŸ¢ IL DATASET Ãˆ PERFETTO. Nessuna azione necessaria.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# --- CONFIGURAZIONE ---\n",
    "INPUT_DIR = 'Dataset_Ready_For_AI_FINAL'  # La cartella da controllare\n",
    "EXPECTED_DELTA_SEC = 60\n",
    "TOLERANCE_SEC = 0.1  # Tolleranza minima per i float\n",
    "\n",
    "all_files = sorted(glob.glob(os.path.join(INPUT_DIR, '*.parquet')))\n",
    "\n",
    "print(f\"ðŸ•µï¸  AVVIO DIAGNOSI INTEGRITÃ€ TEMPORALE\")\n",
    "print(f\"    Target: {INPUT_DIR}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "total_ids_set = set()\n",
    "bad_ids_set = set()\n",
    "bad_files_map = {} # Per sapere in quali file sono gli errori\n",
    "\n",
    "for i, file_path in enumerate(all_files):\n",
    "    fname = os.path.basename(file_path)\n",
    "    try:\n",
    "        # Leggiamo solo ID e Timestamp per velocitÃ \n",
    "        df = pl.read_parquet(file_path).select(['TrajectoryID', 'Timestamp'])\n",
    "        \n",
    "        # Ordiniamo per sicurezza\n",
    "        df = df.sort(['TrajectoryID', 'Timestamp'])\n",
    "\n",
    "        # Calcolo Delta\n",
    "        df = df.with_columns([\n",
    "            pl.col(\"Timestamp\").diff().dt.total_seconds().over(\"TrajectoryID\").alias(\"delta\")\n",
    "        ])\n",
    "\n",
    "        # Troviamo ID con delta != 60 (escludendo i null che sono i primi punti di ogni traccia)\n",
    "        # Un delta di 120s significa che manca 1 riga.\n",
    "        bad_rows = df.filter(\n",
    "            (pl.col(\"delta\").is_not_null()) & \n",
    "            ((pl.col(\"delta\") - EXPECTED_DELTA_SEC).abs() > TOLERANCE_SEC)\n",
    "        )\n",
    "        \n",
    "        # Estraiamo gli ID unici problematici in questo file\n",
    "        current_bad_ids = set(bad_rows[\"TrajectoryID\"].unique().to_list())\n",
    "        current_total_ids = set(df[\"TrajectoryID\"].unique().to_list())\n",
    "        \n",
    "        # Aggiorniamo i contatori globali\n",
    "        bad_ids_set.update(current_bad_ids)\n",
    "        total_ids_set.update(current_total_ids)\n",
    "        \n",
    "        if len(current_bad_ids) > 0:\n",
    "            bad_files_map[fname] = len(current_bad_ids)\n",
    "            print(f\"[{i+1}/{len(all_files)}] {fname}: âš ï¸ TROVATE {len(current_bad_ids)} traiettorie rotte.\", end='\\r')\n",
    "        else:\n",
    "            print(f\"[{i+1}/{len(all_files)}] {fname}: âœ… IntegritÃ  OK.\", end='\\r')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ ERRORE LETTURA {fname}: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“Š REPORT DIAGNOSTICO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "num_total = len(total_ids_set)\n",
    "num_bad = len(bad_ids_set)\n",
    "percent_bad = (num_bad / num_total * 100) if num_total > 0 else 0\n",
    "\n",
    "print(f\"Totale Traiettorie: {num_total:,}\")\n",
    "print(f\"Traiettorie Guaste: {num_bad:,}\")\n",
    "print(f\"Percentuale Danno:  {percent_bad:.2f}%\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if percent_bad == 0:\n",
    "    print(\"ðŸŸ¢ IL DATASET Ãˆ PERFETTO. Nessuna azione necessaria.\")\n",
    "elif percent_bad < 5:\n",
    "    print(\"ðŸŸ¡ DANNO LIEVE. Consiglio: ELIMINA le traiettorie guaste.\")\n",
    "else:\n",
    "    print(\"ðŸ”´ DANNO ESTESO. Consiglio: Bisogna capire perchÃ© l'interpolazione fallisce cosÃ¬ tanto.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cac9ba59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š AVVIO AUDIT DEL DATASET (COLUMN NAME FIX)\n",
      "   Target: Dataset_Ready_For_AI_FINAL\n",
      "------------------------------------------------------------\n",
      "[25/25] Analizzato blocco_024-segmentato.parquet (EXTRA)\n",
      "============================================================\n",
      "ðŸ“„ DATASET PASSPORT (REPORT FINALE)\n",
      "============================================================\n",
      "\n",
      "1. DISTRIBUZIONE DATI (Split)\n",
      "   â€¢ TRAIN (0-15):  136,786,489 righe\n",
      "   â€¢ VAL   (16-19): 31,860,181 righe\n",
      "   â€¢ TEST  (20-23): 42,379,530 righe\n",
      "   â€¢ EXTRA (24+):   4,512,098 righe\n",
      "   --------------------------------------------------\n",
      "   â€¢ TOTALE REALE:  215,538,298 righe\n",
      "   â€¢ NAVI UNICHE:   1,232,449 (ID univoci globali)\n",
      "\n",
      "2. COERENZA TEMPORALE (La Griglia)\n",
      "   â€¢ Delta 60s (Regolari): 214,299,754\n",
      "   â€¢ Delta Anomali (BUCHI): 0  <-- DEVE ESSERE 0\n",
      "   âœ… VERIFICA TEMPORALE SUPERATA: Nessun buco interno.\n",
      "\n",
      "3. INTEGRITÃ€ CUCITURE (Passaggio tra file)\n",
      "   â€¢ Errori di continuitÃ : 0\n",
      "   âœ… CUCITURE PERFETTE.\n",
      "\n",
      "4. FISICA E DENSITÃ€\n",
      "   â€¢ Lat: 0.0085 / 80.8467\n",
      "   â€¢ Lon: -179.5020 / 158.9307\n",
      "   â€¢ SOG: Min 2.10, Max 102.30\n",
      "   â€¢ Durata Traj: Min 30m, Max 15829m, Media 174.0m\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# --- CONFIGURAZIONE ---\n",
    "INPUT_DIR = 'Dataset_Ready_For_AI_FINAL'\n",
    "\n",
    "# Definizione Split\n",
    "SPLITS = {\n",
    "    \"TRAIN\": range(0, 16),\n",
    "    \"VAL\":   range(16, 20),\n",
    "    \"TEST\":  range(20, 24),\n",
    "    \"EXTRA\": range(24, 25) \n",
    "}\n",
    "\n",
    "EXPECTED_DELTA = 60.0\n",
    "TOLERANCE = 0.001 \n",
    "\n",
    "print(f\"ðŸ“Š AVVIO AUDIT DEL DATASET (COLUMN NAME FIX)\")\n",
    "print(f\"   Target: {INPUT_DIR}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "all_files = sorted(glob.glob(os.path.join(INPUT_DIR, '*.parquet')))\n",
    "if not all_files:\n",
    "    print(\"âŒ Nessun file trovato.\")\n",
    "    sys.exit()\n",
    "\n",
    "# --- ACCUMULATORI ---\n",
    "global_stats = {\n",
    "    \"total_rows\": 0,\n",
    "    \"unique_ids\": set(),\n",
    "    \"geo_bounds\": {\"min_lat\": 90, \"max_lat\": -90, \"min_lon\": 180, \"max_lon\": -180},\n",
    "    \"sog_stats\":  {\"min\": 1000, \"max\": -1},\n",
    "    \"time_gaps\":  {\"ok_60s\": 0, \"overlap_0s\": 0, \"errors\": 0},\n",
    "    \"traj_lens\":  [], \n",
    "    \"split_counts\": {\"TRAIN\": 0, \"VAL\": 0, \"TEST\": 0, \"EXTRA\": 0}\n",
    "}\n",
    "\n",
    "def get_split_name(idx):\n",
    "    for name, rng in SPLITS.items():\n",
    "        if idx in rng: return name\n",
    "    return \"EXTRA\"\n",
    "\n",
    "previous_file_last_rows = None\n",
    "stitching_errors = 0\n",
    "\n",
    "# --- ELABORAZIONE ---\n",
    "for i, file_path in enumerate(all_files):\n",
    "    fname = os.path.basename(file_path)\n",
    "    split = get_split_name(i)\n",
    "    \n",
    "    try:\n",
    "        # Caricamento\n",
    "        df = pl.read_parquet(file_path)\n",
    "        \n",
    "        # 1. VOLUMETRIA\n",
    "        rows = df.height\n",
    "        ids = df[\"TrajectoryID\"].unique().to_list()\n",
    "        \n",
    "        global_stats[\"total_rows\"] += rows\n",
    "        global_stats[\"unique_ids\"].update(ids)\n",
    "        global_stats[\"split_counts\"][split] += rows\n",
    "        \n",
    "        # 2. FISICA\n",
    "        global_stats[\"geo_bounds\"][\"min_lat\"] = min(global_stats[\"geo_bounds\"][\"min_lat\"], df[\"Latitude\"].min())\n",
    "        global_stats[\"geo_bounds\"][\"max_lat\"] = max(global_stats[\"geo_bounds\"][\"max_lat\"], df[\"Latitude\"].max())\n",
    "        global_stats[\"geo_bounds\"][\"min_lon\"] = min(global_stats[\"geo_bounds\"][\"min_lon\"], df[\"Longitude\"].min())\n",
    "        global_stats[\"geo_bounds\"][\"max_lon\"] = max(global_stats[\"geo_bounds\"][\"max_lon\"], df[\"Longitude\"].max())\n",
    "        \n",
    "        global_stats[\"sog_stats\"][\"min\"] = min(global_stats[\"sog_stats\"][\"min\"], df[\"SOG\"].min())\n",
    "        global_stats[\"sog_stats\"][\"max\"] = max(global_stats[\"sog_stats\"][\"max\"], df[\"SOG\"].max())\n",
    "        \n",
    "        # 3. ANALISI TEMPORALE\n",
    "        df = df.sort([\"TrajectoryID\", \"Timestamp\"])\n",
    "        \n",
    "        df = df.with_columns([\n",
    "            pl.col(\"Timestamp\").diff().dt.total_seconds().over(\"TrajectoryID\").fill_null(0).alias(\"delta\")\n",
    "        ])\n",
    "        \n",
    "        n_60 = df.filter((pl.col(\"delta\") - 60).abs() < TOLERANCE).height\n",
    "        n_0 = df.filter(pl.col(\"delta\").abs() < TOLERANCE).height\n",
    "        n_errors = df.filter(\n",
    "            ((pl.col(\"delta\") - 60).abs() >= TOLERANCE) & \n",
    "            (pl.col(\"delta\").abs() >= TOLERANCE)\n",
    "        ).height\n",
    "        \n",
    "        global_stats[\"time_gaps\"][\"ok_60s\"] += n_60\n",
    "        global_stats[\"time_gaps\"][\"overlap_0s\"] += n_0 \n",
    "        global_stats[\"time_gaps\"][\"errors\"] += n_errors\n",
    "        \n",
    "        # 4. LUNGHEZZE\n",
    "        lengths = df.group_by(\"TrajectoryID\").len()[\"len\"]\n",
    "        global_stats[\"traj_lens\"].append(lengths)\n",
    "\n",
    "        # 5. CUCITURE (FIXED - Selezione Esplicita)\n",
    "        # Selezioniamo SOLO le colonne che ci servono per evitare errori di indice\n",
    "        current_starts = (\n",
    "            df.group_by(\"TrajectoryID\", maintain_order=True)\n",
    "            .first()\n",
    "            .select([\"TrajectoryID\", \"Timestamp\"]) # <--- FIX: Garantisce ordine colonne\n",
    "        )\n",
    "        \n",
    "        if previous_file_last_rows is not None:\n",
    "            prev_dict = {row[0]: row[1] for row in previous_file_last_rows.iter_rows()}\n",
    "            \n",
    "            for row in current_starts.iter_rows():\n",
    "                # Ora siamo sicuri: 0=ID, 1=Timestamp\n",
    "                tid = row[0] \n",
    "                curr_time = row[1] \n",
    "                \n",
    "                if tid in prev_dict:\n",
    "                    prev_time = prev_dict[tid]\n",
    "                    diff = (curr_time - prev_time).total_seconds()\n",
    "                    \n",
    "                    if not (abs(diff - 60) < TOLERANCE or abs(diff) < TOLERANCE):\n",
    "                        stitching_errors += 1\n",
    "\n",
    "        # Salva coda per prossimo giro (Selezione Esplicita)\n",
    "        last_rows = (\n",
    "            df.group_by(\"TrajectoryID\", maintain_order=True)\n",
    "            .last()\n",
    "            .select([\"TrajectoryID\", \"Timestamp\"]) # <--- FIX\n",
    "        )\n",
    "        previous_file_last_rows = last_rows\n",
    "        \n",
    "        print(f\"[{i+1}/{len(all_files)}] Analizzato {fname} ({split})\", end='\\r')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ ERRORE CRITICO su {fname}: {e}\")\n",
    "\n",
    "# --- REPORT ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“„ DATASET PASSPORT (REPORT FINALE)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_lens = pl.concat(global_stats[\"traj_lens\"])\n",
    "len_stats = {\n",
    "    \"min\": all_lens.min(),\n",
    "    \"max\": all_lens.max(),\n",
    "    \"mean\": all_lens.mean(),\n",
    "    \"median\": all_lens.median()\n",
    "}\n",
    "\n",
    "print(f\"\\n1. DISTRIBUZIONE DATI (Split)\")\n",
    "print(f\"   â€¢ TRAIN (0-15):  {global_stats['split_counts']['TRAIN']:,} righe\")\n",
    "print(f\"   â€¢ VAL   (16-19): {global_stats['split_counts']['VAL']:,} righe\")\n",
    "print(f\"   â€¢ TEST  (20-23): {global_stats['split_counts']['TEST']:,} righe\")\n",
    "print(f\"   â€¢ EXTRA (24+):   {global_stats['split_counts']['EXTRA']:,} righe\")\n",
    "print(f\"   --------------------------------------------------\")\n",
    "print(f\"   â€¢ TOTALE REALE:  {global_stats['total_rows']:,} righe\")\n",
    "print(f\"   â€¢ NAVI UNICHE:   {len(global_stats['unique_ids']):,} (ID univoci globali)\")\n",
    "\n",
    "print(f\"\\n2. COERENZA TEMPORALE (La Griglia)\")\n",
    "print(f\"   â€¢ Delta 60s (Regolari): {global_stats['time_gaps']['ok_60s']:,}\")\n",
    "print(f\"   â€¢ Delta Anomali (BUCHI): {global_stats['time_gaps']['errors']}  <-- DEVE ESSERE 0\")\n",
    "\n",
    "if global_stats[\"time_gaps\"][\"errors\"] == 0:\n",
    "    print(\"   âœ… VERIFICA TEMPORALE SUPERATA: Nessun buco interno.\")\n",
    "else:\n",
    "    print(\"   âŒ ATTENZIONE: Trovati buchi temporali interni ai file.\")\n",
    "\n",
    "print(f\"\\n3. INTEGRITÃ€ CUCITURE (Passaggio tra file)\")\n",
    "print(f\"   â€¢ Errori di continuitÃ : {stitching_errors}\")\n",
    "if stitching_errors == 0:\n",
    "    print(\"   âœ… CUCITURE PERFETTE.\")\n",
    "else:\n",
    "    print(f\"   âš ï¸ NOTA: {stitching_errors} sovrapposizioni o gap ai bordi.\")\n",
    "\n",
    "print(f\"\\n4. FISICA E DENSITÃ€\")\n",
    "print(f\"   â€¢ Lat: {global_stats['geo_bounds']['min_lat']:.4f} / {global_stats['geo_bounds']['max_lat']:.4f}\")\n",
    "print(f\"   â€¢ Lon: {global_stats['geo_bounds']['min_lon']:.4f} / {global_stats['geo_bounds']['max_lon']:.4f}\")\n",
    "print(f\"   â€¢ SOG: Min {global_stats['sog_stats']['min']:.2f}, Max {global_stats['sog_stats']['max']:.2f}\")\n",
    "print(f\"   â€¢ Durata Traj: Min {len_stats['min']}m, Max {len_stats['max']}m, Media {len_stats['mean']:.1f}m\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
