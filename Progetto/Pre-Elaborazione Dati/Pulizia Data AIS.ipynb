{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67cbf103",
   "metadata": {},
   "source": [
    "# Pre-Elaborazione dei Dati (Dataset di riferimento da Luglio 2024 a Giugno 2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ec05d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import shutil\n",
    "import re \n",
    "import gc\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbbdcf0",
   "metadata": {},
   "source": [
    "### Delineamo l'ambiente di lavoro\n",
    "\n",
    "In questa sezione vengono definite le directory di lavoro e tutti quei parametri per cui andiamo a filtrare i nostri dati.\n",
    "\n",
    "SOG_MIN --> Impostiamo il parametro a 2.0, questo ci serve per poi andare a scartare tutte le navi ferme.\n",
    "\n",
    "TIME_GAP --> Questa Ã¨ una soglia di tempo massima arbitraria permessa all'interno di una singola traiettoria. Se tra due messaggi consecutivi della stessa nave passano piÃ¹ di 60 minuti, assumiamo che la rotta sia stata interrotta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbf94fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trovati 365 file Parquet da processare.\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIR = '../../../Dataset'\n",
    "SCRIPT_DIR = os.getcwd()                                # Restituisce la directory di lavoro corrente\n",
    "\n",
    "OUTPUT_DIR_NAME = 'Dataset_Pre-Cleaned_AIS' \n",
    "OUTPUT_DIR = os.path.join(SCRIPT_DIR, OUTPUT_DIR_NAME)\n",
    "\n",
    "SOG_MIN_THRESHOLD = 2.0\n",
    "TIME_GAP_THRESHOLD = pd.Timedelta(hours=1)\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "all_files = glob.glob(os.path.join(INPUT_DIR, '*.parquet'))\n",
    "\n",
    "all_clean_data = []\n",
    "\n",
    "print(f\"Trovati {len(all_files)} file Parquet da processare.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3397dad",
   "metadata": {},
   "source": [
    "#### Test\n",
    "\n",
    "- Proviamo a verificare la lettura di un file parquet e della corretta formattazione dei dati.  \n",
    "- Oltre a questo andiamo ad estrarre il numero di colonne per verificare se sono state selezionate le colonne corrette.  \n",
    "- Viene aggiunto anche un controllo sulle righe per vedere dopo la pulizia la percentuale di pulizia per ogni file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b38aa3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ðŸ” DEBUG: Dati iniziali dal file ais-2025-01-01.parquet ---\n",
      "\n",
      "Head del DataFrame:\n",
      "        MMSI  Latitude  Longitude  SOG    COG           Timestamp\n",
      "0  671087100  18.46281  -66.10297  0.0  176.7 2025-01-01 00:00:00\n",
      "1  367733950  48.48503 -122.60927  0.0  215.5 2025-01-01 00:00:00\n",
      "2  368138010  40.47715  -73.84652  5.5  286.9 2025-01-01 00:00:02\n",
      "3  367637210  29.12033  -90.21215  0.0  227.6 2025-01-01 00:00:03\n",
      "4  368050000  41.27196  -72.46934  0.0  107.1 2025-01-01 00:00:03\n",
      "\n",
      "Tipi di Dati (Dtypes) dopo la conversione Timestamp:\n",
      "MMSI                  int64\n",
      "Latitude            float64\n",
      "Longitude           float64\n",
      "SOG                 float64\n",
      "COG                 float64\n",
      "Timestamp    datetime64[ns]\n",
      "dtype: object\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Numero di righe: 7337208, Numero di colonne: 6\n",
      "\n",
      "--- ðŸ” DEBUG: Dati iniziali dal file AIS_2024_12_31.parquet ---\n",
      "\n",
      "Head del DataFrame:\n",
      "        MMSI  Latitude  Longitude   SOG    COG           Timestamp\n",
      "0  367776660  21.19308 -157.72342   8.0  112.1 2024-12-31 00:00:08\n",
      "1  368095340  29.76995  -95.07893   0.0  185.5 2024-12-31 00:00:05\n",
      "2  366847780  29.96697  -93.85909   0.1  186.2 2024-12-31 00:00:00\n",
      "3  367481310  27.68242  -82.58073  11.5   57.6 2024-12-31 00:00:04\n",
      "4  248669000  29.85743  -93.94083   3.1  220.9 2024-12-31 00:00:06\n",
      "\n",
      "Tipi di Dati (Dtypes) dopo la conversione Timestamp:\n",
      "MMSI                  int64\n",
      "Latitude            float64\n",
      "Longitude           float64\n",
      "SOG                 float64\n",
      "COG                 float64\n",
      "Timestamp    datetime64[ns]\n",
      "dtype: object\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Numero di righe: 7588976, Numero di colonne: 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = '../../../Dataset/'\n",
    "\n",
    "FILE_PATH_TEST = os.path.join(BASE_PATH, 'ais-2025-01-01.parquet')\n",
    "FILE_PATH_TEST2 = os.path.join(BASE_PATH, 'AIS_2024_12_31.parquet')\n",
    "\n",
    "COLUMN_MAPPING2025 = {\n",
    "    'mmsi': 'MMSI', \n",
    "    'latitude': 'Latitude', \n",
    "    'longitude': 'Longitude', \n",
    "    'sog': 'SOG', \n",
    "    'cog': 'COG', \n",
    "    'base_date_time': 'Timestamp' \n",
    "}\n",
    "COLUMNS_TO_READ_2025 = list(COLUMN_MAPPING2025.keys())\n",
    "\n",
    "try:\n",
    "    df = pd.read_parquet(\n",
    "        FILE_PATH_TEST, \n",
    "        columns=COLUMNS_TO_READ_2025,\n",
    "        engine='pyarrow' \n",
    "    )\n",
    "\n",
    "    df = df.rename(columns=COLUMN_MAPPING2025)\n",
    "\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "    \n",
    "    \n",
    "    print(f\"--- ðŸ” DEBUG: Dati iniziali dal file {os.path.basename(FILE_PATH_TEST)} ---\")\n",
    "    print(\"\\nHead del DataFrame:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nTipi di Dati (Dtypes) dopo la conversione Timestamp:\")\n",
    "    print(df.dtypes)\n",
    "    print(\"----------------------------------------------------------------\\n\")\n",
    "    rows,columns = df.shape\n",
    "    print(f\"Numero di righe: {rows}, Numero di colonne: {columns}\\n\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Errore nel processare il file {FILE_PATH_TEST}: {e}\")\n",
    "\n",
    "\n",
    "COLUMN_MAPPING2024 = {\n",
    "    'MMSI': 'MMSI',\n",
    "    'LAT': 'Latitude',\n",
    "    'LON': 'Longitude',\n",
    "    'SOG': 'SOG',\n",
    "    'COG': 'COG',\n",
    "    'BaseDateTime': 'Timestamp' \n",
    "}\n",
    "COLUMNS_TO_READ_2024 = list(COLUMN_MAPPING2024.keys())\n",
    "\n",
    "try:\n",
    "    df = pd.read_parquet(\n",
    "        FILE_PATH_TEST2, \n",
    "        columns=COLUMNS_TO_READ_2024,\n",
    "        engine='pyarrow' \n",
    "    )\n",
    "\n",
    "    df = df.rename(columns=COLUMN_MAPPING2024)\n",
    "\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "\n",
    "\n",
    "    print(f\"--- ðŸ” DEBUG: Dati iniziali dal file {os.path.basename(FILE_PATH_TEST2)} ---\")\n",
    "    print(\"\\nHead del DataFrame:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nTipi di Dati (Dtypes) dopo la conversione Timestamp:\")\n",
    "    print(df.dtypes)\n",
    "    print(\"----------------------------------------------------------------\\n\")\n",
    "    rows,columns = df.shape\n",
    "    print(f\"Numero di righe: {rows}, Numero di colonne: {columns}\\n\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Errore nel processare il file {FILE_PATH_TEST2}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d6f751",
   "metadata": {},
   "source": [
    "### Pulizia dei dati\n",
    " \n",
    "In questa sezione, iteriamo su ogni file del nostro dataset ed eseguiamo la pulizia vera e propria, applicando dei filtri. Il primo filtro filtro applicato Ã¨ sulla lettura delle colonne `COLUMNS_TO_READ` prima di caricare i dati. Ãˆ il modo piÃ¹ efficiente per scartare le colonne inutili e riduce drasticamente l'utilizzo della RAM velocizzando l'intero processo.\n",
    "\n",
    "##### Filtri Navigazione Attiva e di ValiditÃ \n",
    "  \n",
    "Vengono applicati una serie di filtri per lasciare all'interno del dataset solo valori validi e di navigazione attiva:\n",
    "1. Applichiamo il filtro `df = df[df['SOG'] > SOG_MIN_THRESHOLD`, eliminando i dati statici come deciso sopra.\n",
    "2. Applichiamo il filtro `df[df['COG'] != 511]`,rimuovendo i record dove il COG (Course Over Ground) Ã¨ $511$. Questo Ã¨ un codice standard AIS che significa \"Dato Non Disponibile\". Senza una rotta (COG), l'informazione cinematica Ã¨ incompleta e inutile per il modello.\n",
    "3. Applichiamo il filtro `Filtro Lat/Lon (>= -90, <= 90, etc.)`, eliminiamo i record con coordinate geografiche errate (fuori dal globo). Questi sono errori di trasmissione o del sensore che inquinerebbero il dataset.\n",
    "4. Utilizziamo il metodo `df.dopna(...)` per rimuovere qualsiasi riga che abbia valori mancanti. Questo perchÃ¨ i modelli LSTM/LNN richiedono input completi per funzionare correttamente.\n",
    "5. Infine l'ultimo filtro Ã¨ `df['MMSI'].str.len()==9` per rimuovere i record con l'identificativo della nave non corretto. Questo perchÃ¨ l'MMSI deve essere di 9 cifre e questo ci garantisce che ogni traiettoria sia attribuita ad una nave valida.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "445983d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ais-2025-02-15.parquet pre-pulito\n",
      "File ais-2025-06-22.parquet pre-pulito\n",
      "File ais-2025-04-26.parquet pre-pulito\n",
      "File AIS_2024_08_21.parquet pre-pulito\n",
      "File ais-2025-01-22.parquet pre-pulito\n",
      "File ais-2025-02-27.parquet pre-pulito\n",
      "File ais-2025-06-18.parquet pre-pulito\n",
      "File ais-2025-06-14.parquet pre-pulito\n",
      "File AIS_2024_08_11.parquet pre-pulito\n",
      "File AIS_2024_08_06.parquet pre-pulito\n",
      "File AIS_2024_08_22.parquet pre-pulito\n",
      "File ais-2025-06-21.parquet pre-pulito\n",
      "File AIS_2024_11_05.parquet pre-pulito\n",
      "File AIS_2024_10_30.parquet pre-pulito\n",
      "File AIS_2024_10_16.parquet pre-pulito\n",
      "File ais-2025-01-23.parquet pre-pulito\n",
      "File ais-2025-03-26.parquet pre-pulito\n",
      "File AIS_2024_09_24.parquet pre-pulito\n",
      "File ais-2025-03-25.parquet pre-pulito\n",
      "File AIS_2024_07_01.parquet pre-pulito\n",
      "File AIS_2024_09_11.parquet pre-pulito\n",
      "File ais-2025-03-08.parquet pre-pulito\n",
      "File AIS_2024_11_21.parquet pre-pulito\n",
      "File ais-2025-04-25.parquet pre-pulito\n",
      "File AIS_2024_08_18.parquet pre-pulito\n",
      "File AIS_2024_12_19.parquet pre-pulito\n",
      "File AIS_2024_12_29.parquet pre-pulito\n",
      "File ais-2025-01-21.parquet pre-pulito\n",
      "File ais-2025-04-03.parquet pre-pulito\n",
      "File ais-2025-04-22.parquet pre-pulito\n",
      "File ais-2025-01-04.parquet pre-pulito\n",
      "File ais-2025-04-11.parquet pre-pulito\n",
      "File AIS_2024_12_05.parquet pre-pulito\n",
      "File AIS_2024_09_30.parquet pre-pulito\n",
      "File AIS_2024_12_28.parquet pre-pulito\n",
      "File AIS_2024_12_26.parquet pre-pulito\n",
      "File AIS_2024_09_21.parquet pre-pulito\n",
      "File ais-2025-05-10.parquet pre-pulito\n",
      "File AIS_2024_12_21.parquet pre-pulito\n",
      "File AIS_2024_07_14.parquet pre-pulito\n",
      "File ais-2025-04-05.parquet pre-pulito\n",
      "File AIS_2024_08_23.parquet pre-pulito\n",
      "File AIS_2024_08_30.parquet pre-pulito\n",
      "File ais-2025-05-01.parquet pre-pulito\n",
      "File AIS_2024_12_31.parquet pre-pulito\n",
      "File AIS_2024_07_11.parquet pre-pulito\n",
      "File AIS_2024_09_25.parquet pre-pulito\n",
      "File AIS_2024_11_17.parquet pre-pulito\n",
      "File AIS_2024_09_29.parquet pre-pulito\n",
      "File AIS_2024_07_27.parquet pre-pulito\n",
      "File ais-2025-06-25.parquet pre-pulito\n",
      "File ais-2025-04-15.parquet pre-pulito\n",
      "File ais-2025-02-20.parquet pre-pulito\n",
      "File AIS_2024_11_15.parquet pre-pulito\n",
      "File ais-2025-05-06.parquet pre-pulito\n",
      "File AIS_2024_09_18.parquet pre-pulito\n",
      "File AIS_2024_12_12.parquet pre-pulito\n",
      "File ais-2025-05-25.parquet pre-pulito\n",
      "File AIS_2024_10_09.parquet pre-pulito\n",
      "File ais-2025-04-18.parquet pre-pulito\n",
      "File ais-2025-02-18.parquet pre-pulito\n",
      "File AIS_2024_11_19.parquet pre-pulito\n",
      "File AIS_2024_11_14.parquet pre-pulito\n",
      "File ais-2025-02-17.parquet pre-pulito\n",
      "File ais-2025-03-16.parquet pre-pulito\n",
      "File ais-2025-06-24.parquet pre-pulito\n",
      "File ais-2025-06-17.parquet pre-pulito\n",
      "File AIS_2024_11_09.parquet pre-pulito\n",
      "File ais-2025-03-07.parquet pre-pulito\n",
      "File AIS_2024_09_17.parquet pre-pulito\n",
      "File AIS_2024_08_28.parquet pre-pulito\n",
      "File AIS_2024_09_01.parquet pre-pulito\n",
      "File ais-2025-01-16.parquet pre-pulito\n",
      "File AIS_2024_11_28.parquet pre-pulito\n",
      "File ais-2025-03-05.parquet pre-pulito\n",
      "File AIS_2024_08_05.parquet pre-pulito\n",
      "File AIS_2024_12_06.parquet pre-pulito\n",
      "File AIS_2024_11_04.parquet pre-pulito\n",
      "File ais-2025-06-28.parquet pre-pulito\n",
      "File ais-2025-05-29.parquet pre-pulito\n",
      "File ais-2025-01-07.parquet pre-pulito\n",
      "File AIS_2024_08_16.parquet pre-pulito\n",
      "File AIS_2024_12_20.parquet pre-pulito\n",
      "File AIS_2024_07_22.parquet pre-pulito\n",
      "File AIS_2024_11_22.parquet pre-pulito\n",
      "File AIS_2024_11_10.parquet pre-pulito\n",
      "File ais-2025-02-06.parquet pre-pulito\n",
      "File ais-2025-05-11.parquet pre-pulito\n",
      "File AIS_2024_12_30.parquet pre-pulito\n",
      "File ais-2025-05-21.parquet pre-pulito\n",
      "File ais-2025-04-06.parquet pre-pulito\n",
      "File AIS_2024_08_25.parquet pre-pulito\n",
      "File AIS_2024_07_07.parquet pre-pulito\n",
      "File AIS_2024_10_25.parquet pre-pulito\n",
      "File AIS_2024_09_06.parquet pre-pulito\n",
      "File ais-2025-05-04.parquet pre-pulito\n",
      "File ais-2025-05-18.parquet pre-pulito\n",
      "File AIS_2024_08_31.parquet pre-pulito\n",
      "File AIS_2024_12_10.parquet pre-pulito\n",
      "File ais-2025-06-11.parquet pre-pulito\n",
      "File AIS_2024_12_22.parquet pre-pulito\n",
      "File AIS_2024_09_13.parquet pre-pulito\n",
      "File AIS_2024_12_23.parquet pre-pulito\n",
      "File AIS_2024_11_01.parquet pre-pulito\n",
      "File ais-2025-06-29.parquet pre-pulito\n",
      "File ais-2025-02-26.parquet pre-pulito\n",
      "File AIS_2024_09_26.parquet pre-pulito\n",
      "File ais-2025-02-25.parquet pre-pulito\n",
      "File ais-2025-01-15.parquet pre-pulito\n",
      "File ais-2025-02-16.parquet pre-pulito\n",
      "File ais-2025-02-23.parquet pre-pulito\n",
      "File AIS_2024_09_22.parquet pre-pulito\n",
      "File ais-2025-05-15.parquet pre-pulito\n",
      "File AIS_2024_12_18.parquet pre-pulito\n",
      "File AIS_2024_10_31.parquet pre-pulito\n",
      "File ais-2025-04-13.parquet pre-pulito\n",
      "File ais-2025-03-03.parquet pre-pulito\n",
      "File ais-2025-03-01.parquet pre-pulito\n",
      "File ais-2025-01-05.parquet pre-pulito\n",
      "File ais-2025-06-16.parquet pre-pulito\n",
      "File ais-2025-02-22.parquet pre-pulito\n",
      "File ais-2025-01-09.parquet pre-pulito\n",
      "File AIS_2024_07_03.parquet pre-pulito\n",
      "File ais-2025-06-12.parquet pre-pulito\n",
      "File AIS_2024_12_11.parquet pre-pulito\n",
      "File AIS_2024_10_18.parquet pre-pulito\n",
      "File ais-2025-01-02.parquet pre-pulito\n",
      "File ais-2025-06-04.parquet pre-pulito\n",
      "File AIS_2024_12_14.parquet pre-pulito\n",
      "File AIS_2024_11_29.parquet pre-pulito\n",
      "File ais-2025-01-17.parquet pre-pulito\n",
      "File AIS_2024_08_13.parquet pre-pulito\n",
      "File ais-2025-02-28.parquet pre-pulito\n",
      "File ais-2025-05-26.parquet pre-pulito\n",
      "File ais-2025-02-11.parquet pre-pulito\n",
      "File ais-2025-06-05.parquet pre-pulito\n",
      "File AIS_2024_11_08.parquet pre-pulito\n",
      "File ais-2025-02-13.parquet pre-pulito\n",
      "File AIS_2024_10_11.parquet pre-pulito\n",
      "File ais-2025-03-31.parquet pre-pulito\n",
      "File ais-2025-05-08.parquet pre-pulito\n",
      "File ais-2025-04-10.parquet pre-pulito\n",
      "File ais-2025-03-19.parquet pre-pulito\n",
      "File ais-2025-02-05.parquet pre-pulito\n",
      "File AIS_2024_08_08.parquet pre-pulito\n",
      "File AIS_2024_09_03.parquet pre-pulito\n",
      "File AIS_2024_07_06.parquet pre-pulito\n",
      "File AIS_2024_11_11.parquet pre-pulito\n",
      "File AIS_2024_11_13.parquet pre-pulito\n",
      "File ais-2025-03-21.parquet pre-pulito\n",
      "File ais-2025-03-04.parquet pre-pulito\n",
      "File ais-2025-06-09.parquet pre-pulito\n",
      "File ais-2025-05-27.parquet pre-pulito\n",
      "File ais-2025-02-19.parquet pre-pulito\n",
      "File ais-2025-01-08.parquet pre-pulito\n",
      "File AIS_2024_12_07.parquet pre-pulito\n",
      "File AIS_2024_09_23.parquet pre-pulito\n",
      "File AIS_2024_10_03.parquet pre-pulito\n",
      "File AIS_2024_11_06.parquet pre-pulito\n",
      "File ais-2025-04-27.parquet pre-pulito\n",
      "File ais-2025-05-30.parquet pre-pulito\n",
      "File AIS_2024_10_12.parquet pre-pulito\n",
      "File AIS_2024_10_07.parquet pre-pulito\n",
      "File ais-2025-06-27.parquet pre-pulito\n",
      "File ais-2025-02-14.parquet pre-pulito\n",
      "File ais-2025-03-27.parquet pre-pulito\n",
      "File ais-2025-01-29.parquet pre-pulito\n",
      "File AIS_2024_12_13.parquet pre-pulito\n",
      "File AIS_2024_08_04.parquet pre-pulito\n",
      "File AIS_2024_07_12.parquet pre-pulito\n",
      "File AIS_2024_07_18.parquet pre-pulito\n",
      "File AIS_2024_11_16.parquet pre-pulito\n",
      "File AIS_2024_07_28.parquet pre-pulito\n",
      "File ais-2025-06-20.parquet pre-pulito\n",
      "File AIS_2024_09_10.parquet pre-pulito\n",
      "File AIS_2024_12_04.parquet pre-pulito\n",
      "File ais-2025-06-19.parquet pre-pulito\n",
      "File ais-2025-01-20.parquet pre-pulito\n",
      "File ais-2025-03-17.parquet pre-pulito\n",
      "File ais-2025-04-04.parquet pre-pulito\n",
      "File AIS_2024_08_01.parquet pre-pulito\n",
      "File AIS_2024_09_02.parquet pre-pulito\n",
      "File AIS_2024_07_13.parquet pre-pulito\n",
      "File ais-2025-03-10.parquet pre-pulito\n",
      "File AIS_2024_07_15.parquet pre-pulito\n",
      "File ais-2025-04-08.parquet pre-pulito\n",
      "File AIS_2024_10_20.parquet pre-pulito\n",
      "File ais-2025-03-28.parquet pre-pulito\n",
      "File ais-2025-03-29.parquet pre-pulito\n",
      "File AIS_2024_09_05.parquet pre-pulito\n",
      "File AIS_2024_08_15.parquet pre-pulito\n",
      "File AIS_2024_09_12.parquet pre-pulito\n",
      "File ais-2025-02-21.parquet pre-pulito\n",
      "File ais-2025-03-11.parquet pre-pulito\n",
      "File AIS_2024_07_25.parquet pre-pulito\n",
      "File AIS_2024_10_21.parquet pre-pulito\n",
      "File AIS_2024_11_12.parquet pre-pulito\n",
      "File ais-2025-05-17.parquet pre-pulito\n",
      "File ais-2025-04-01.parquet pre-pulito\n",
      "File ais-2025-05-09.parquet pre-pulito\n",
      "File AIS_2024_11_27.parquet pre-pulito\n",
      "File ais-2025-01-01.parquet pre-pulito\n",
      "File AIS_2024_07_24.parquet pre-pulito\n",
      "File AIS_2024_09_28.parquet pre-pulito\n",
      "File ais-2025-01-25.parquet pre-pulito\n",
      "File ais-2025-04-28.parquet pre-pulito\n",
      "File AIS_2024_09_14.parquet pre-pulito\n",
      "File AIS_2024_12_25.parquet pre-pulito\n",
      "File ais-2025-04-16.parquet pre-pulito\n",
      "File ais-2025-05-28.parquet pre-pulito\n",
      "File AIS_2024_12_08.parquet pre-pulito\n",
      "File ais-2025-06-03.parquet pre-pulito\n",
      "File ais-2025-04-14.parquet pre-pulito\n",
      "File ais-2025-06-13.parquet pre-pulito\n",
      "File AIS_2024_07_26.parquet pre-pulito\n",
      "File AIS_2024_10_04.parquet pre-pulito\n",
      "File ais-2025-01-18.parquet pre-pulito\n",
      "File ais-2025-04-19.parquet pre-pulito\n",
      "File AIS_2024_11_03.parquet pre-pulito\n",
      "File AIS_2024_12_09.parquet pre-pulito\n",
      "File AIS_2024_07_31.parquet pre-pulito\n",
      "File ais-2025-04-12.parquet pre-pulito\n",
      "File AIS_2024_08_07.parquet pre-pulito\n",
      "File AIS_2024_10_13.parquet pre-pulito\n",
      "File AIS_2024_11_20.parquet pre-pulito\n",
      "File ais-2025-04-20.parquet pre-pulito\n",
      "File ais-2025-06-10.parquet pre-pulito\n",
      "File ais-2025-02-03.parquet pre-pulito\n",
      "File ais-2025-05-03.parquet pre-pulito\n",
      "File ais-2025-04-02.parquet pre-pulito\n",
      "File AIS_2024_07_29.parquet pre-pulito\n",
      "File ais-2025-05-22.parquet pre-pulito\n",
      "File ais-2025-05-02.parquet pre-pulito\n",
      "File AIS_2024_07_19.parquet pre-pulito\n",
      "File ais-2025-02-04.parquet pre-pulito\n",
      "File AIS_2024_07_21.parquet pre-pulito\n",
      "File ais-2025-01-28.parquet pre-pulito\n",
      "File AIS_2024_07_20.parquet pre-pulito\n",
      "File AIS_2024_08_17.parquet pre-pulito\n",
      "File ais-2025-05-24.parquet pre-pulito\n",
      "File ais-2025-01-06.parquet pre-pulito\n",
      "File ais-2025-02-12.parquet pre-pulito\n",
      "File ais-2025-06-30.parquet pre-pulito\n",
      "File AIS_2024_10_15.parquet pre-pulito\n",
      "File ais-2025-05-20.parquet pre-pulito\n",
      "File ais-2025-02-10.parquet pre-pulito\n",
      "File ais-2025-05-12.parquet pre-pulito\n",
      "File AIS_2024_07_05.parquet pre-pulito\n",
      "File ais-2025-03-12.parquet pre-pulito\n",
      "File ais-2025-04-09.parquet pre-pulito\n",
      "File ais-2025-03-06.parquet pre-pulito\n",
      "File ais-2025-01-03.parquet pre-pulito\n",
      "File ais-2025-03-15.parquet pre-pulito\n",
      "File ais-2025-04-17.parquet pre-pulito\n",
      "File AIS_2024_09_19.parquet pre-pulito\n",
      "File AIS_2024_10_02.parquet pre-pulito\n",
      "File AIS_2024_08_29.parquet pre-pulito\n",
      "File ais-2025-01-14.parquet pre-pulito\n",
      "File ais-2025-03-18.parquet pre-pulito\n",
      "File ais-2025-01-13.parquet pre-pulito\n",
      "File AIS_2024_07_17.parquet pre-pulito\n",
      "File AIS_2024_08_12.parquet pre-pulito\n",
      "File AIS_2024_10_10.parquet pre-pulito\n",
      "File AIS_2024_12_03.parquet pre-pulito\n",
      "File AIS_2024_11_26.parquet pre-pulito\n",
      "File AIS_2024_12_15.parquet pre-pulito\n",
      "File AIS_2024_11_02.parquet pre-pulito\n",
      "File AIS_2024_09_07.parquet pre-pulito\n",
      "File ais-2025-06-08.parquet pre-pulito\n",
      "File AIS_2024_07_10.parquet pre-pulito\n",
      "File AIS_2024_08_14.parquet pre-pulito\n",
      "File ais-2025-04-23.parquet pre-pulito\n",
      "File ais-2025-03-24.parquet pre-pulito\n",
      "File ais-2025-04-30.parquet pre-pulito\n",
      "File AIS_2024_07_04.parquet pre-pulito\n",
      "File ais-2025-01-24.parquet pre-pulito\n",
      "File AIS_2024_10_24.parquet pre-pulito\n",
      "File AIS_2024_12_01.parquet pre-pulito\n",
      "File ais-2025-06-07.parquet pre-pulito\n",
      "File ais-2025-06-02.parquet pre-pulito\n",
      "File AIS_2024_08_10.parquet pre-pulito\n",
      "File AIS_2024_09_27.parquet pre-pulito\n",
      "File ais-2025-03-20.parquet pre-pulito\n",
      "File AIS_2024_09_15.parquet pre-pulito\n",
      "File ais-2025-01-10.parquet pre-pulito\n",
      "File AIS_2024_07_08.parquet pre-pulito\n",
      "File ais-2025-06-01.parquet pre-pulito\n",
      "File AIS_2024_12_27.parquet pre-pulito\n",
      "File ais-2025-06-15.parquet pre-pulito\n",
      "File ais-2025-05-07.parquet pre-pulito\n",
      "File AIS_2024_08_24.parquet pre-pulito\n",
      "File ais-2025-02-02.parquet pre-pulito\n",
      "File ais-2025-06-06.parquet pre-pulito\n",
      "File AIS_2024_10_01.parquet pre-pulito\n",
      "File ais-2025-04-24.parquet pre-pulito\n",
      "File AIS_2024_10_19.parquet pre-pulito\n",
      "File ais-2025-02-08.parquet pre-pulito\n",
      "File AIS_2024_07_02.parquet pre-pulito\n",
      "File ais-2025-05-13.parquet pre-pulito\n",
      "File AIS_2024_10_28.parquet pre-pulito\n",
      "File AIS_2024_08_09.parquet pre-pulito\n",
      "File ais-2025-01-11.parquet pre-pulito\n",
      "File ais-2025-05-31.parquet pre-pulito\n",
      "File ais-2025-03-23.parquet pre-pulito\n",
      "File AIS_2024_11_07.parquet pre-pulito\n",
      "File AIS_2024_10_05.parquet pre-pulito\n",
      "File ais-2025-03-02.parquet pre-pulito\n",
      "File ais-2025-01-30.parquet pre-pulito\n",
      "File ais-2025-01-31.parquet pre-pulito\n",
      "File AIS_2024_07_23.parquet pre-pulito\n",
      "File ais-2025-03-22.parquet pre-pulito\n",
      "File ais-2025-01-19.parquet pre-pulito\n",
      "File AIS_2024_12_24.parquet pre-pulito\n",
      "File ais-2025-05-23.parquet pre-pulito\n",
      "File ais-2025-05-05.parquet pre-pulito\n",
      "File ais-2025-02-09.parquet pre-pulito\n",
      "File ais-2025-05-16.parquet pre-pulito\n",
      "File AIS_2024_10_14.parquet pre-pulito\n",
      "File AIS_2024_10_27.parquet pre-pulito\n",
      "File ais-2025-03-14.parquet pre-pulito\n",
      "File AIS_2024_12_17.parquet pre-pulito\n",
      "File AIS_2024_09_08.parquet pre-pulito\n",
      "File AIS_2024_10_08.parquet pre-pulito\n",
      "File AIS_2024_09_16.parquet pre-pulito\n",
      "File AIS_2024_08_03.parquet pre-pulito\n",
      "File AIS_2024_07_30.parquet pre-pulito\n",
      "File AIS_2024_10_23.parquet pre-pulito\n",
      "File AIS_2024_07_16.parquet pre-pulito\n",
      "File AIS_2024_11_18.parquet pre-pulito\n",
      "File ais-2025-03-09.parquet pre-pulito\n",
      "File AIS_2024_08_26.parquet pre-pulito\n",
      "File AIS_2024_10_17.parquet pre-pulito\n",
      "File AIS_2024_10_06.parquet pre-pulito\n",
      "File ais-2025-02-07.parquet pre-pulito\n",
      "File ais-2025-02-01.parquet pre-pulito\n",
      "File AIS_2024_10_29.parquet pre-pulito\n",
      "File ais-2025-05-14.parquet pre-pulito\n",
      "File AIS_2024_10_26.parquet pre-pulito\n",
      "File ais-2025-04-29.parquet pre-pulito\n",
      "File AIS_2024_11_24.parquet pre-pulito\n",
      "File AIS_2024_08_19.parquet pre-pulito\n",
      "File ais-2025-04-07.parquet pre-pulito\n",
      "File AIS_2024_11_23.parquet pre-pulito\n",
      "File AIS_2024_11_30.parquet pre-pulito\n",
      "File ais-2025-02-24.parquet pre-pulito\n",
      "File AIS_2024_11_25.parquet pre-pulito\n",
      "File ais-2025-06-26.parquet pre-pulito\n",
      "File ais-2025-03-13.parquet pre-pulito\n",
      "File AIS_2024_08_27.parquet pre-pulito\n",
      "File AIS_2024_09_09.parquet pre-pulito\n",
      "File AIS_2024_12_02.parquet pre-pulito\n",
      "File ais-2025-06-23.parquet pre-pulito\n",
      "File AIS_2024_07_09.parquet pre-pulito\n",
      "File AIS_2024_08_20.parquet pre-pulito\n",
      "File ais-2025-01-12.parquet pre-pulito\n",
      "File ais-2025-03-30.parquet pre-pulito\n",
      "File ais-2025-04-21.parquet pre-pulito\n",
      "File AIS_2024_08_02.parquet pre-pulito\n",
      "File AIS_2024_10_22.parquet pre-pulito\n",
      "File ais-2025-01-27.parquet pre-pulito\n",
      "File ais-2025-01-26.parquet pre-pulito\n",
      "File AIS_2024_09_20.parquet pre-pulito\n",
      "File AIS_2024_12_16.parquet pre-pulito\n",
      "File AIS_2024_09_04.parquet pre-pulito\n",
      "File ais-2025-05-19.parquet pre-pulito\n",
      "\n",
      "--- FASE 1 (Pre-Pulizia) completata. ---\n"
     ]
    }
   ],
   "source": [
    "MAPPING_2025 = {\n",
    "    'mmsi': 'MMSI', \n",
    "    'latitude': 'Latitude', \n",
    "    'longitude': 'Longitude', \n",
    "    'sog': 'SOG', \n",
    "    'cog': 'COG', \n",
    "    'base_date_time': 'Timestamp' \n",
    "}\n",
    "\n",
    "COLUMNS_2025 = list(MAPPING_2025.keys())\n",
    "\n",
    "MAPPING_2024 = {\n",
    "    'MMSI': 'MMSI',\n",
    "    'LAT': 'Latitude',\n",
    "    'LON': 'Longitude',\n",
    "    'SOG': 'SOG',\n",
    "    'COG': 'COG',\n",
    "    'BaseDateTime': 'Timestamp'\n",
    "}\n",
    "COLUMNS_2024 = list(MAPPING_2024.keys())\n",
    "\n",
    "for file_path in all_files:\n",
    "    df = None\n",
    "    mapping_usato = None\n",
    "\n",
    "    try:\n",
    "        \n",
    "        df = pd.read_parquet(\n",
    "            file_path, \n",
    "            columns=COLUMNS_2025,\n",
    "            engine='pyarrow' \n",
    "        )\n",
    "        df = df.rename(columns=MAPPING_2025)\n",
    "        mapping_usato = \"2025\"\n",
    "    \n",
    "    except Exception as e1:\n",
    "        try:\n",
    "            df = pd.read_parquet(\n",
    "                file_path, \n",
    "                columns=COLUMNS_2024,\n",
    "                engine='pyarrow' \n",
    "            )\n",
    "            df = df.rename(columns=MAPPING_2024)\n",
    "            mapping_usato = \"2024\"\n",
    "        \n",
    "        except Exception as e2:\n",
    "            print(f\"Errore IRRISOLVIBILE nel caricare {file_path}: Schema non riconosciuto.\")\n",
    "            continue\n",
    "\n",
    "    if df is not None:\n",
    "        try:\n",
    "            \n",
    "            df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "            \n",
    "            # Filtri cinematici e geografici\n",
    "            df = df[df['SOG'] > SOG_MIN_THRESHOLD]\n",
    "            df = df[df['COG'] != 511]\n",
    "            df = df[(df['Latitude'] >= -90) & (df['Latitude'] <= 90)]\n",
    "            df = df[(df['Longitude'] >= -180) & (df['Longitude'] <= 180)]\n",
    "            \n",
    "            # Filtri di integritÃ \n",
    "            df = df.dropna(subset=['MMSI', 'Latitude', 'Longitude', 'SOG', 'COG'])\n",
    "            df['MMSI'] = df['MMSI'].astype(str).str.replace(r'\\D', '', regex=True)\n",
    "            df = df[df['MMSI'].str.len() == 9]\n",
    "\n",
    "            if not df.empty:\n",
    "\n",
    "                output_filename = os.path.basename(file_path).lower()\n",
    "                output_file = os.path.join(OUTPUT_DIR, output_filename)\n",
    "                \n",
    "                df.to_parquet(output_file, index=False)\n",
    "                \n",
    "                print(f\"File {os.path.basename(file_path)} pre-pulito\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Errore nella FASE DI PULIZIA per il file {file_path}: {e}\")\n",
    "\n",
    "print(\"\\n--- FASE 1 (Pre-Pulizia) completata. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076e33fe",
   "metadata": {},
   "source": [
    "#### Test file pre-pulizia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b55661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MMSI</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>SOG</th>\n",
       "      <th>COG</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>368138010</td>\n",
       "      <td>40.47715</td>\n",
       "      <td>-73.84652</td>\n",
       "      <td>5.5</td>\n",
       "      <td>286.9</td>\n",
       "      <td>2025-01-01 00:00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>367188610</td>\n",
       "      <td>27.93936</td>\n",
       "      <td>-82.45703</td>\n",
       "      <td>2.2</td>\n",
       "      <td>147.6</td>\n",
       "      <td>2025-01-01 00:00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>366938780</td>\n",
       "      <td>46.04232</td>\n",
       "      <td>-83.93567</td>\n",
       "      <td>11.8</td>\n",
       "      <td>126.0</td>\n",
       "      <td>2025-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>316028554</td>\n",
       "      <td>49.28782</td>\n",
       "      <td>-123.10689</td>\n",
       "      <td>7.8</td>\n",
       "      <td>215.6</td>\n",
       "      <td>2025-01-01 00:00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>338122081</td>\n",
       "      <td>37.78262</td>\n",
       "      <td>-122.38452</td>\n",
       "      <td>3.7</td>\n",
       "      <td>196.6</td>\n",
       "      <td>2025-01-01 00:00:12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        MMSI  Latitude  Longitude   SOG    COG           Timestamp\n",
       "0  368138010  40.47715  -73.84652   5.5  286.9 2025-01-01 00:00:02\n",
       "1  367188610  27.93936  -82.45703   2.2  147.6 2025-01-01 00:00:04\n",
       "2  366938780  46.04232  -83.93567  11.8  126.0 2025-01-01 00:00:00\n",
       "3  316028554  49.28782 -123.10689   7.8  215.6 2025-01-01 00:00:06\n",
       "4  338122081  37.78262 -122.38452   3.7  196.6 2025-01-01 00:00:12"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRE_CLEANED_FILE_PATH_TEST = 'Dataset_Pre-Cleaned_AIS/ais-2025-01-01.parquet'\n",
    "COLUMNS_TO_READ_2025 = ['MMSI', 'Latitude', 'Longitude','SOG', 'COG', 'Timestamp']\n",
    "\n",
    "df = pd.read_parquet(\n",
    "        PRE_CLEANED_FILE_PATH_TEST, \n",
    "        columns=COLUMNS_TO_READ_2025,\n",
    "        engine='pyarrow' \n",
    "    )\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78c1631",
   "metadata": {},
   "source": [
    "#### Unificazione in blocchi di file da 15 giorni\n",
    "\n",
    "Questo raggruppamento serve per andare a diminuire quelli che sono i punti del problema di \"mezzanotte\". Questo problema chiamato cosÃ¬ da noi per indicare la situazione in cui ci siano traiettorie continue a cavallo di due differenti file. Con un'unica grande unificazione non ci sarebbe stato tale problema ma a causa di limiti Hardware non Ã¨ stato possibile consolidare tutto in un unico file. Si Ã¨ scelto quindi di procedere con un unificazione parziale del dataset totale dove ogni file racchiude 15 giorni.\n",
    "\n",
    "##### Segmentazione e Creazione delle Traiettorie\n",
    "\n",
    "Questa Ã¨ la fase finale prima del salvataggio dei nuovi blocchi, dove trasformiamo i dati puliti in sequenze coerenti (TrajectoryID).  \n",
    "Quello che andiamo a fare Ã¨ raggruppare i nostri dati prima per l'MMSI e poi per il TimeStamp. In questo modo abbiamo i dati ordinati ed  Ã¨ possibile delineare quelle che sono le traiettorie diverse per ogni nave. Viene aggiunta una nuova colonna al dataset che Ã¨ `TrajectoryID` che ha il compito di raggruppare tutti i dati di ogni singola nave che fanno riferimento ad un intero spostamento.  \n",
    "Gli spostamenti sono stati delineati assumendo che spostamenti diversi vengono caratterizzati da uno stato di navigazione non attiva di almeno 1 ora.  \n",
    "Questa fase Ã¨ essenziale perchÃ¨ i modelli che andremo ad addestrare, impareranno non dai singoli punti ma dalle intere sequenze.\n",
    "\n",
    "```\n",
    "df = df.sort_values(by=['MMSI', 'Timestamp']).reset_index(drop=True)\n",
    "df_blocco['TimeDiff'] = df_blocco.groupby('MMSI')['Timestamp'].diff()     \n",
    "df_blocco['IsNewTraj'] = (df_blocco['MMSI'] != df_blocco['MMSI'].shift(1)) | (df_blocco['TimeDiff'] > TIME_GAP_THRESHOLD)\n",
    "df_blocco['IsNewTraj_int'] = df_blocco['IsNewTraj'].astype(int)\n",
    "df_blocco['TrajectoryID'] = df_blocco['IsNewTraj_int'].cumsum() + max_trajectory_id_globale\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efbdc5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trovati 365 file, raggruppati in 25 blocchi da 15 giorni.\n",
      "\n",
      "--- Inizio elaborazione Blocco 1/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 2/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 3/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 4/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 5/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 6/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 7/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 8/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 9/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 10/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 11/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 12/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 13/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 14/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 15/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 16/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 17/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 18/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 19/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 20/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 21/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 22/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 23/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 24/25 ---\n",
      "Caricamento di 15 file...\n",
      "Inizio calcolo TrajectoryID...\n",
      "\n",
      "--- Inizio elaborazione Blocco 25/25 ---\n",
      "Caricamento di 5 file...\n",
      "Inizio calcolo TrajectoryID...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "INPUT_DIR = 'Dataset_Pre-Cleaned_AIS' \n",
    "SCRIPT_DIR = os.getcwd()\n",
    "\n",
    "OUTPUT_DIR_NAME = 'Dataset_Segmentato_15Giorni' \n",
    "OUTPUT_DIR = os.path.join(SCRIPT_DIR, OUTPUT_DIR_NAME)\n",
    "\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    shutil.rmtree(OUTPUT_DIR)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "TIME_GAP_THRESHOLD = pd.Timedelta(hours=1)\n",
    "\n",
    "GIORNI_PER_BLOCCO = 15\n",
    "\n",
    "all_files = glob.glob(os.path.join(INPUT_DIR, '*.parquet'))\n",
    "all_files.sort() # Fondamentale per ordinare i giorni!\n",
    "\n",
    "num_blocchi = math.ceil(len(all_files) / GIORNI_PER_BLOCCO)\n",
    "print(f\"Trovati {len(all_files)} file, raggruppati in {num_blocchi} blocchi da 15 giorni.\")\n",
    "\n",
    "max_trajectory_id_globale = 0 \n",
    "\n",
    "for i in range(num_blocchi):\n",
    "    start_index = i * GIORNI_PER_BLOCCO\n",
    "    end_index = (i + 1) * GIORNI_PER_BLOCCO\n",
    "    \n",
    "    file_list_blocco = all_files[start_index:end_index]\n",
    "    \n",
    "    print(f\"\\n--- Inizio elaborazione Blocco {i+1}/{num_blocchi} ---\")\n",
    "    \n",
    "    try:\n",
    "        print(f\"Caricamento di {len(file_list_blocco)} file...\")\n",
    "        \n",
    "        df_list = [pd.read_parquet(f) for f in file_list_blocco]\n",
    "        df_blocco = pd.concat(df_list, ignore_index=True)\n",
    "        \n",
    "\n",
    "        df_blocco = df_blocco.sort_values(by=['MMSI', 'Timestamp']).reset_index(drop=True)\n",
    "        \n",
    "        print(\"Inizio calcolo TrajectoryID...\")\n",
    "        df_blocco['TimeDiff'] = df_blocco.groupby('MMSI')['Timestamp'].diff()\n",
    "        df_blocco['IsNewTraj'] = (df_blocco['MMSI'] != df_blocco['MMSI'].shift(1)) | (df_blocco['TimeDiff'] > TIME_GAP_THRESHOLD)\n",
    "        df_blocco['IsNewTraj_int'] = df_blocco['IsNewTraj'].astype(int)\n",
    "        df_blocco['TrajectoryID'] = df_blocco['IsNewTraj_int'].cumsum() + max_trajectory_id_globale\n",
    "        \n",
    "        df_blocco = df_blocco.drop(columns=['TimeDiff', 'IsNewTraj', 'IsNewTraj_int'])\n",
    "        \n",
    "        max_trajectory_id_globale = df_blocco['TrajectoryID'].max()\n",
    "        \n",
    "        output_file = os.path.join(OUTPUT_DIR, f\"blocco_{i:03d}-segmentato.parquet\")\n",
    "        df_blocco.to_parquet(output_file, index=False, engine='pyarrow', compression='snappy')\n",
    "        \n",
    "    except MemoryError:\n",
    "        print(f\"--- âŒ ERRORE DI MEMORIA: Blocco {i+1} (15 giorni) Ã¨ ancora troppo grande! ---\")\n",
    "        break \n",
    "    except Exception as e:\n",
    "        print(f\"--- âŒ ERRORE SCONOSCIUTO nel blocco {i+1}: {e} ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75beca3",
   "metadata": {},
   "source": [
    "#### Individuazione e Applicazione dell 'algoritmo di cucitura\n",
    "\n",
    "Per ottenere un'effettiva coerenza dei **TrajectorID** dobbiamo andare ad individuare la presenza di incoerenza nei relativi punti di mezzanotte. CiÃ² sta a significare che nonostante siano stati diminuiti questi punti cruciali non Ã¨ possibile lasciare che traiettorie continue (nuovo spostamento in meno di 1 ora) venga considerato come una nuova traiettoria solo perchÃ¨ vi Ã¨ un cambio di file .parquet.\n",
    "\n",
    "Per gestire tale situazione Ã¨ stato implementato un algoritmo :\n",
    "\n",
    "- Viene letto il primo file insieme al secondo e viene analizzato ogni spostamento durante la \"Mezzanotte\"\n",
    "- Se ci sono spostamenti che non superano l'ora, viene modificato il **TrajectorID** per mantenere coerenza tra i due file\n",
    "- Successivamente viene tolto dalla memoria il primo file e si analizza interamente il secondo per propagare i cambiamenti\n",
    "- In conlcusione viene preso il terzo file e si ripete tutta la procedura\n",
    "\n",
    "Questi passaggi vengono attuati a tutti i blocchi in ordine in modo da poter propagare la correzione all'intero dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6398b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FASE B: Stitching Sequenziale avviato ---\n",
      "Lettura blocchi da: Dataset_Segmentato_15Giorni\n",
      "Salvataggio finale in: /home/al3th3ia/Scrivania/Cybersecurity/Detecting-Trajectory-Spoofing-Attacks-on-AIS/Progetto/Pre-Elaborazione Dati/Dataset_Stitched_Finale\n",
      "\n",
      "Blocco 0 (blocco_000-segmentato.parquet) copiato, nessuna correzione necessaria.\n",
      "\n",
      "--- Inizio cucitura: blocco_000-segmentato.parquet -> blocco_001-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 3127 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_001-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_001-segmentato.parquet -> blocco_002-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2913 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_002-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_002-segmentato.parquet -> blocco_003-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 3666 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_003-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_003-segmentato.parquet -> blocco_004-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 3450 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_004-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_004-segmentato.parquet -> blocco_005-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 3353 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_005-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_005-segmentato.parquet -> blocco_006-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2838 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_006-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_006-segmentato.parquet -> blocco_007-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 3095 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_007-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_007-segmentato.parquet -> blocco_008-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2501 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_008-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_008-segmentato.parquet -> blocco_009-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2540 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_009-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_009-segmentato.parquet -> blocco_010-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2657 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_010-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_010-segmentato.parquet -> blocco_011-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2115 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_011-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_011-segmentato.parquet -> blocco_012-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2019 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_012-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_012-segmentato.parquet -> blocco_013-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2353 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_013-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_013-segmentato.parquet -> blocco_014-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 1890 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_014-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_014-segmentato.parquet -> blocco_015-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2154 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_015-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_015-segmentato.parquet -> blocco_016-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2295 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_016-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_016-segmentato.parquet -> blocco_017-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2602 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_017-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_017-segmentato.parquet -> blocco_018-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2559 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_018-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_018-segmentato.parquet -> blocco_019-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2847 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_019-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_019-segmentato.parquet -> blocco_020-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2463 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_020-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_020-segmentato.parquet -> blocco_021-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 2449 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_021-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_021-segmentato.parquet -> blocco_022-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 3181 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_022-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_022-segmentato.parquet -> blocco_023-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 3053 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_023-segmentato.parquet corretto e salvato.\n",
      "\n",
      "--- Inizio cucitura: blocco_023-segmentato.parquet -> blocco_024-segmentato.parquet ---\n",
      "  Trovati confini, calcolo mappa...\n",
      "  -> Trovate 3396 cuciture da applicare.\n",
      "  Rilascio memoria Blocco A...\n",
      "  Applicazione correzioni a Blocco B...\n",
      "  -> Blocco blocco_024-segmentato.parquet corretto e salvato.\n",
      "Dataset perfetto salvato in: /home/al3th3ia/Scrivania/Cybersecurity/Detecting-Trajectory-Spoofing-Attacks-on-AIS/Progetto/Pre-Elaborazione Dati/Dataset_Stitched_Finale\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIR = 'Dataset_Segmentato_15Giorni' \n",
    "OUTPUT_DIR_NAME = 'Dataset_Stitched_Finale' \n",
    "OUTPUT_DIR = os.path.join(os.getcwd(), OUTPUT_DIR_NAME)\n",
    "\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    shutil.rmtree(OUTPUT_DIR)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "TIME_GAP_THRESHOLD = pd.Timedelta(hours=1)\n",
    "\n",
    "print(f\"--- FASE B: Stitching Sequenziale avviato ---\")\n",
    "print(f\"Lettura blocchi da: {INPUT_DIR}\")\n",
    "print(f\"Salvataggio finale in: {OUTPUT_DIR}\\n\")\n",
    "\n",
    "all_blocks = sorted(glob.glob(os.path.join(INPUT_DIR, '*.parquet')))\n",
    "\n",
    "if not all_blocks:\n",
    "    print(f\"ERRORE: Nessun file blocco trovato in {INPUT_DIR}.\")\n",
    "else:\n",
    "        \n",
    "    path_A_corretto = all_blocks[0]\n",
    "    output_path_A = os.path.join(OUTPUT_DIR, os.path.basename(path_A_corretto))\n",
    "    shutil.copy(path_A_corretto, output_path_A)\n",
    "    print(f\"Blocco 0 ({os.path.basename(path_A_corretto)}) copiato, nessuna correzione necessaria.\")\n",
    "\n",
    "    # Si passa al controllo sui due blocchi consecutivi    \n",
    "    for i in range(len(all_blocks) - 1):\n",
    "                \n",
    "        path_A_corretto = os.path.join(OUTPUT_DIR, os.path.basename(all_blocks[i]))\n",
    "        path_B_grezzo = all_blocks[i+1]\n",
    "        \n",
    "        print(f\"\\n--- Inizio cucitura: {os.path.basename(path_A_corretto)} -> {os.path.basename(path_B_grezzo)} ---\")\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            df_A = pd.read_parquet(path_A_corretto)\n",
    "            df_B = pd.read_parquet(path_B_grezzo)\n",
    "            \n",
    "            \n",
    "            print(\"  Trovati confini, calcolo mappa...\")\n",
    "            last_records_A = df_A.loc[df_A.groupby('MMSI')['Timestamp'].idxmax()]\n",
    "            last_records_A = last_records_A[['MMSI', 'Timestamp', 'TrajectoryID']].rename(\n",
    "                columns={'Timestamp': 'Last_Timestamp', 'TrajectoryID': 'Correct_ID'}\n",
    "            )   # Prende l'ultimo record di ogni MMSI in A\n",
    "\n",
    "            first_records_B = df_B.loc[df_B.groupby('MMSI')['Timestamp'].idxmin()]\n",
    "            first_records_B = first_records_B[['MMSI', 'Timestamp', 'TrajectoryID']].rename(\n",
    "                columns={'Timestamp': 'First_Timestamp', 'TrajectoryID': 'Old_ID'}\n",
    "            )   # Prende il primo record di ogni MMSI in B\n",
    "\n",
    "            \n",
    "            boundary_check = pd.merge(last_records_A, first_records_B, on='MMSI')\n",
    "            boundary_check['TimeDiff'] = boundary_check['First_Timestamp'] - boundary_check['Last_Timestamp']\n",
    "            stitch_candidates = boundary_check[boundary_check['TimeDiff'] <= TIME_GAP_THRESHOLD]\n",
    "        \n",
    "            # Creazione della mappa di correzione\n",
    "            local_fix_map = stitch_candidates.set_index('Old_ID')['Correct_ID'].to_dict()\n",
    "            print(f\"  -> Trovate {len(local_fix_map)} cuciture da applicare.\")\n",
    "\n",
    "            print(\"  Rilascio memoria Blocco A...\")\n",
    "            del df_A, last_records_A, first_records_B, boundary_check, stitch_candidates\n",
    "            gc.collect()\n",
    "\n",
    "            print(\"  Applicazione correzioni a Blocco B...\")\n",
    "            df_B['TrajectoryID'] = df_B['TrajectoryID'].map(local_fix_map).fillna(df_B['TrajectoryID']).astype(int)\n",
    "\n",
    "            output_path_B = os.path.join(OUTPUT_DIR, os.path.basename(path_B_grezzo))\n",
    "            df_B.to_parquet(output_path_B, index=False, engine='pyarrow', compression='snappy')\n",
    "            print(f\"  -> Blocco {os.path.basename(output_path_B)} corretto e salvato.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  -> âŒ ERRORE durante la cucitura: {e}\")\n",
    "            break \n",
    "        finally:\n",
    "            if 'df_A' in locals(): del df_A\n",
    "            if 'df_B' in locals(): del df_B\n",
    "            gc.collect()\n",
    "\n",
    "    print(f\"Dataset perfetto salvato in: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3731d31a",
   "metadata": {},
   "source": [
    "#### Verifica dell'algoritmo di cucitura\n",
    "\n",
    "In questa sezione viene integrata una verifica delle cuciture appena svolte. Questo viene fatto analizzando tutti i gap temporali inferiori ad 1 ora e successivamente si vede se tra questi c'Ã¨ differenza di **TrajectorID**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea250b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inizio scansione...\n",
      "Confine 1: Trovati 3127 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 2: Trovati 2913 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 3: Trovati 3666 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 4: Trovati 3450 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 5: Trovati 3353 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 6: Trovati 2838 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 7: Trovati 3095 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 8: Trovati 2501 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 9: Trovati 2540 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 10: Trovati 2657 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 11: Trovati 2115 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 12: Trovati 2019 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 13: Trovati 2353 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 14: Trovati 1890 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 15: Trovati 2154 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 16: Trovati 2295 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 17: Trovati 2602 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 18: Trovati 2559 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 19: Trovati 2847 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 20: Trovati 2463 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 21: Trovati 2449 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 22: Trovati 3181 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 23: Trovati 3053 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "Confine 24: Trovati 3396 gap (<= 1h). Di questi, 0 cuciture mancate.\n",
      "RISULTATO FINALE: Trovate 0 cuciture mancate.\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIR = 'Dataset_Stitched_Finale' \n",
    "TIME_GAP_THRESHOLD = pd.Timedelta(hours=1)\n",
    "\n",
    "all_blocks = sorted(glob.glob(os.path.join(INPUT_DIR, '*.parquet')))\n",
    "\n",
    "if not all_blocks:\n",
    "    print(f\"ERRORE: Nessun file blocco trovato in {INPUT_DIR}.\")\n",
    "else:\n",
    "    total_missed_stitches = 0\n",
    "\n",
    "    print(\"Inizio scansione...\")\n",
    "\n",
    "    for i in range(len(all_blocks) - 1):\n",
    "        path_A = all_blocks[i]\n",
    "        path_B = all_blocks[i+1]\n",
    "        \n",
    "        try:\n",
    "            df_A = pd.read_parquet(path_A)\n",
    "            df_B = pd.read_parquet(path_B)\n",
    "\n",
    "            \n",
    "            last_records_A = df_A.loc[df_A.groupby('MMSI')['Timestamp'].idxmax()]\n",
    "            last_records_A = last_records_A[['MMSI', 'Timestamp', 'TrajectoryID']].rename(\n",
    "                columns={'Timestamp': 'Last_Timestamp', 'TrajectoryID': 'ID_A'}\n",
    "            )\n",
    "\n",
    "            \n",
    "            first_records_B = df_B.loc[df_B.groupby('MMSI')['Timestamp'].idxmin()]\n",
    "            first_records_B = first_records_B[['MMSI', 'Timestamp', 'TrajectoryID']].rename(\n",
    "                columns={'Timestamp': 'First_Timestamp', 'TrajectoryID': 'ID_B'}\n",
    "            )\n",
    "\n",
    "            \n",
    "            boundary_check = pd.merge(last_records_A, first_records_B, on='MMSI')\n",
    "\n",
    "            \n",
    "            boundary_check['TimeDiff'] = boundary_check['First_Timestamp'] - boundary_check['Last_Timestamp']\n",
    "\n",
    "            \n",
    "            stitchable_gaps = boundary_check[boundary_check['TimeDiff'] <= TIME_GAP_THRESHOLD]\n",
    "            \n",
    "            if not stitchable_gaps.empty:\n",
    "                \n",
    "                missed_stitches = stitchable_gaps[stitchable_gaps['ID_A'] != stitchable_gaps['ID_B']]\n",
    "                \n",
    "                local_missed_count = len(missed_stitches)\n",
    "                total_missed_stitches += local_missed_count\n",
    "                \n",
    "                print(f\"Confine {i+1}: Trovati {len(stitchable_gaps)} gap (<= 1h). Di questi, {local_missed_count} cuciture mancate.\")\n",
    "            else:\n",
    "                print(f\"Confine {i+1}: Nessun gap (<= 1h) trovato.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  -> ERRORE durante il controllo del confine {i+1}: {e}\")\n",
    "            \n",
    "        finally:\n",
    "            del df_A, df_B, last_records_A, first_records_B, boundary_check, stitchable_gaps\n",
    "            if 'missed_stitches' in locals(): del missed_stitches\n",
    "            gc.collect()\n",
    "\n",
    "    print(f\"RISULTATO FINALE: Trovate {total_missed_stitches} cuciture mancate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6c7807",
   "metadata": {},
   "source": [
    "#### Verifica presenza dei duplicati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be38478e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inizio scansione\n",
      "Controllo: blocco_000-segmentato.parquet...\n",
      "Trovate 7527 righe duplicate.\n",
      "Controllo: blocco_001-segmentato.parquet...\n",
      "Trovate 1245 righe duplicate.\n",
      "Controllo: blocco_002-segmentato.parquet...\n",
      "Trovate 1329 righe duplicate.\n",
      "Controllo: blocco_003-segmentato.parquet...\n",
      "Trovate 1064 righe duplicate.\n",
      "Controllo: blocco_004-segmentato.parquet...\n",
      "Trovate 2699 righe duplicate.\n",
      "Controllo: blocco_005-segmentato.parquet...\n",
      "Trovate 8737 righe duplicate.\n",
      "Controllo: blocco_006-segmentato.parquet...\n",
      "Trovate 5302 righe duplicate.\n",
      "Controllo: blocco_007-segmentato.parquet...\n",
      "Trovate 6240 righe duplicate.\n",
      "Controllo: blocco_008-segmentato.parquet...\n",
      "Trovate 5287 righe duplicate.\n",
      "Controllo: blocco_009-segmentato.parquet...\n",
      "Trovate 6058 righe duplicate.\n",
      "Controllo: blocco_010-segmentato.parquet...\n",
      "Trovate 6137 righe duplicate.\n",
      "Controllo: blocco_011-segmentato.parquet...\n",
      "Trovate 4072 righe duplicate.\n",
      "Controllo: blocco_012-segmentato.parquet...\n",
      "Trovate 4855 righe duplicate.\n",
      "Controllo: blocco_013-segmentato.parquet...\n",
      "Trovate 6558 righe duplicate.\n",
      "Controllo: blocco_014-segmentato.parquet...\n",
      "Trovate 7355 righe duplicate.\n",
      "Controllo: blocco_015-segmentato.parquet...\n",
      "Trovate 7089 righe duplicate.\n",
      "Controllo: blocco_016-segmentato.parquet...\n",
      "Trovate 9960 righe duplicate.\n",
      "Controllo: blocco_017-segmentato.parquet...\n",
      "Trovate 6473 righe duplicate.\n",
      "Controllo: blocco_018-segmentato.parquet...\n",
      "Trovate 8105 righe duplicate.\n",
      "Controllo: blocco_019-segmentato.parquet...\n",
      "Trovate 6077 righe duplicate.\n",
      "Controllo: blocco_020-segmentato.parquet...\n",
      "Trovate 12409 righe duplicate.\n",
      "Controllo: blocco_021-segmentato.parquet...\n",
      "Trovate 9882 righe duplicate.\n",
      "Controllo: blocco_022-segmentato.parquet...\n",
      "Trovate 8453 righe duplicate.\n",
      "Controllo: blocco_023-segmentato.parquet...\n",
      "Trovate 14830 righe duplicate.\n",
      "Controllo: blocco_024-segmentato.parquet...\n",
      "Trovate 3525 righe duplicate.\n",
      "Sono state trovate 161,268 righe duplicate in totale.\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIR = 'Dataset_Stitched_Finale' \n",
    "\n",
    "all_blocks = sorted(glob.glob(os.path.join(INPUT_DIR, '*.parquet')))\n",
    "\n",
    "if not all_blocks:\n",
    "    print(f\"ERRORE: Nessun file blocco trovato in {INPUT_DIR}.\")\n",
    "else:\n",
    "    total_duplicates_found = 0\n",
    "    print(\"Inizio scansione\")\n",
    "\n",
    " \n",
    "    for block_path in all_blocks:\n",
    "        try:\n",
    "            print(f\"Controllo: {os.path.basename(block_path)}...\")\n",
    "    \n",
    "            df_block = pd.read_parquet(block_path)\n",
    "            \n",
    "            local_duplicates = df_block.duplicated().sum()   \n",
    "            \n",
    "            if local_duplicates > 0:\n",
    "                print(f\"Trovate {local_duplicates} righe duplicate.\")\n",
    "                total_duplicates_found += local_duplicates\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERRORE durante il controllo del blocco: {e}\")\n",
    "        \n",
    "        finally:\n",
    "            if 'df_block' in locals():\n",
    "                del df_block\n",
    "            gc.collect()\n",
    "\n",
    "    print(f\"Sono state trovate {total_duplicates_found:,} righe duplicate in totale.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57e261e",
   "metadata": {},
   "source": [
    "#### Pulizia dei duplicati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b444f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lettura blocchi da: Dataset_Stitched_Finale\n",
      "Salvataggio in: /home/al3th3ia/Scrivania/Cybersecurity/Detecting-Trajectory-Spoofing-Attacks-on-AIS/Progetto/Pre-Elaborazione Dati/Dataset\n",
      "\n",
      "Inizio scansione...\n",
      "Blocco blocco_000-segmentato.parquet: Trovate e rimosse 7527 righe duplicate.\n",
      "Blocco blocco_001-segmentato.parquet: Trovate e rimosse 1245 righe duplicate.\n",
      "Blocco blocco_002-segmentato.parquet: Trovate e rimosse 1329 righe duplicate.\n",
      "Blocco blocco_003-segmentato.parquet: Trovate e rimosse 1064 righe duplicate.\n",
      "Blocco blocco_004-segmentato.parquet: Trovate e rimosse 2699 righe duplicate.\n",
      "Blocco blocco_005-segmentato.parquet: Trovate e rimosse 8737 righe duplicate.\n",
      "Blocco blocco_006-segmentato.parquet: Trovate e rimosse 5302 righe duplicate.\n",
      "Blocco blocco_007-segmentato.parquet: Trovate e rimosse 6240 righe duplicate.\n",
      "Blocco blocco_008-segmentato.parquet: Trovate e rimosse 5287 righe duplicate.\n",
      "Blocco blocco_009-segmentato.parquet: Trovate e rimosse 6058 righe duplicate.\n",
      "Blocco blocco_010-segmentato.parquet: Trovate e rimosse 6137 righe duplicate.\n",
      "Blocco blocco_011-segmentato.parquet: Trovate e rimosse 4072 righe duplicate.\n",
      "Blocco blocco_012-segmentato.parquet: Trovate e rimosse 4855 righe duplicate.\n",
      "Blocco blocco_013-segmentato.parquet: Trovate e rimosse 6558 righe duplicate.\n",
      "Blocco blocco_014-segmentato.parquet: Trovate e rimosse 7355 righe duplicate.\n",
      "Blocco blocco_015-segmentato.parquet: Trovate e rimosse 7089 righe duplicate.\n",
      "Blocco blocco_016-segmentato.parquet: Trovate e rimosse 9960 righe duplicate.\n",
      "Blocco blocco_017-segmentato.parquet: Trovate e rimosse 6473 righe duplicate.\n",
      "Blocco blocco_018-segmentato.parquet: Trovate e rimosse 8105 righe duplicate.\n",
      "Blocco blocco_019-segmentato.parquet: Trovate e rimosse 6077 righe duplicate.\n",
      "Blocco blocco_020-segmentato.parquet: Trovate e rimosse 12409 righe duplicate.\n",
      "Blocco blocco_021-segmentato.parquet: Trovate e rimosse 9882 righe duplicate.\n",
      "Blocco blocco_022-segmentato.parquet: Trovate e rimosse 8453 righe duplicate.\n",
      "Blocco blocco_023-segmentato.parquet: Trovate e rimosse 14830 righe duplicate.\n",
      "Blocco blocco_024-segmentato.parquet: Trovate e rimosse 3525 righe duplicate.\n",
      "Il dataset finale Ã¨ in: /home/al3th3ia/Scrivania/Cybersecurity/Detecting-Trajectory-Spoofing-Attacks-on-AIS/Progetto/Pre-Elaborazione Dati/Dataset\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIR = 'Dataset_Stitched_Finale' \n",
    "\n",
    "OUTPUT_DIR_NAME = 'Dataset' \n",
    "OUTPUT_DIR = os.path.join(os.getcwd(), OUTPUT_DIR_NAME)\n",
    "\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    shutil.rmtree(OUTPUT_DIR)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Lettura blocchi da: {INPUT_DIR}\")\n",
    "print(f\"Salvataggio in: {OUTPUT_DIR}\\n\")\n",
    "\n",
    "all_blocks = sorted(glob.glob(os.path.join(INPUT_DIR, '*.parquet')))\n",
    "\n",
    "if not all_blocks:\n",
    "    print(f\"ERRORE: Nessun file blocco trovato in {INPUT_DIR}.\")\n",
    "else:\n",
    "    total_duplicates_removed = 0\n",
    "    print(\"Inizio scansione...\")\n",
    "\n",
    "    for block_path in all_blocks:\n",
    "        try:\n",
    "            \n",
    "            df_block = pd.read_parquet(block_path)\n",
    "            rows_before = len(df_block)\n",
    "            \n",
    "            df_block.drop_duplicates(inplace=True)   # Rimuove le righe duplicate\n",
    "            \n",
    "            rows_after = len(df_block)\n",
    "            local_duplicates_removed = rows_before - rows_after\n",
    "            \n",
    "            if local_duplicates_removed > 0:\n",
    "                print(f\"Blocco {os.path.basename(block_path)}: Trovate e rimosse {local_duplicates_removed} righe duplicate.\")\n",
    "                total_duplicates_removed += local_duplicates_removed\n",
    "            else:\n",
    "                print(f\"Blocco {os.path.basename(block_path)}: Nessun duplicato trovato.\")\n",
    "\n",
    "            output_file = os.path.join(OUTPUT_DIR, os.path.basename(block_path))\n",
    "            df_block.to_parquet(output_file, index=False, engine='pyarrow', compression='snappy')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERRORE durante la pulizia del blocco {os.path.basename(block_path)}: {e}\")\n",
    "\n",
    "        finally:\n",
    "            if 'df_block' in locals():\n",
    "                del df_block\n",
    "            gc.collect()\n",
    "\n",
    "    print(f\"Il dataset finale Ã¨ in: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6e6541",
   "metadata": {},
   "source": [
    "#### Conteggio delle traiettorie uniche con fine valutativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1518358e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando blocco_000-segmentato.parquet...\n",
      "Processando blocco_001-segmentato.parquet...\n",
      "Processando blocco_002-segmentato.parquet...\n",
      "Processando blocco_003-segmentato.parquet...\n",
      "Processando blocco_004-segmentato.parquet...\n",
      "Processando blocco_005-segmentato.parquet...\n",
      "Processando blocco_006-segmentato.parquet...\n",
      "Processando blocco_007-segmentato.parquet...\n",
      "Processando blocco_008-segmentato.parquet...\n",
      "Processando blocco_009-segmentato.parquet...\n",
      "Processando blocco_010-segmentato.parquet...\n",
      "Processando blocco_011-segmentato.parquet...\n",
      "Processando blocco_012-segmentato.parquet...\n",
      "Processando blocco_013-segmentato.parquet...\n",
      "Processando blocco_014-segmentato.parquet...\n",
      "Processando blocco_015-segmentato.parquet...\n",
      "Processando blocco_016-segmentato.parquet...\n",
      "Processando blocco_017-segmentato.parquet...\n",
      "Processando blocco_018-segmentato.parquet...\n",
      "Processando blocco_019-segmentato.parquet...\n",
      "Processando blocco_020-segmentato.parquet...\n",
      "Processando blocco_021-segmentato.parquet...\n",
      "Processando blocco_022-segmentato.parquet...\n",
      "Processando blocco_023-segmentato.parquet...\n",
      "Processando blocco_024-segmentato.parquet...\n",
      "Il tuo dataset finale contiene 6,744,164 traiettorie uniche totali.\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIR = 'Dataset' \n",
    "\n",
    "global_unique_ids = set()  # Utilizziamo il set perchÃ¨ memorizza solo valori unici\n",
    "\n",
    "try:\n",
    "    all_blocks = sorted(glob.glob(os.path.join(INPUT_DIR, '*.parquet')))\n",
    "    \n",
    "    if not all_blocks:\n",
    "        print(f\"ERRORE: Nessun file blocco trovato in {INPUT_DIR}.\")\n",
    "    else:\n",
    "        \n",
    "        for block_path in all_blocks:\n",
    "            print(f\"Processando {os.path.basename(block_path)}...\")\n",
    "            \n",
    "            df = pd.read_parquet(block_path, columns=['TrajectoryID'])\n",
    "            \n",
    "            local_uniques = set(df['TrajectoryID'].unique())\n",
    "            \n",
    "            global_unique_ids.update(local_uniques)\n",
    "            \n",
    "            del df, local_uniques\n",
    "            gc.collect()\n",
    "\n",
    "        numero_totale_traiettorie = len(global_unique_ids)\n",
    "        \n",
    "        print(f\"Il tuo dataset finale contiene {numero_totale_traiettorie:,} traiettorie uniche totali.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERRORE\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f10aa62",
   "metadata": {},
   "source": [
    "#### Controllo finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93148560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MMSI</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>SOG</th>\n",
       "      <th>COG</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>TrajectoryID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.92981</td>\n",
       "      <td>-124.62096</td>\n",
       "      <td>5.6</td>\n",
       "      <td>180.3</td>\n",
       "      <td>2024-07-09 15:37:13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.93671</td>\n",
       "      <td>-124.61779</td>\n",
       "      <td>4.7</td>\n",
       "      <td>21.5</td>\n",
       "      <td>2024-07-09 20:39:09</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.93372</td>\n",
       "      <td>-124.61674</td>\n",
       "      <td>3.0</td>\n",
       "      <td>176.4</td>\n",
       "      <td>2024-07-09 20:56:43</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.93272</td>\n",
       "      <td>-124.61994</td>\n",
       "      <td>3.0</td>\n",
       "      <td>246.4</td>\n",
       "      <td>2024-07-09 20:58:02</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.95354</td>\n",
       "      <td>-124.62340</td>\n",
       "      <td>6.3</td>\n",
       "      <td>10.7</td>\n",
       "      <td>2024-07-09 23:49:54</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.95859</td>\n",
       "      <td>-124.62075</td>\n",
       "      <td>6.4</td>\n",
       "      <td>20.6</td>\n",
       "      <td>2024-07-09 23:53:08</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.96356</td>\n",
       "      <td>-124.61822</td>\n",
       "      <td>6.3</td>\n",
       "      <td>20.1</td>\n",
       "      <td>2024-07-09 23:56:14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.96884</td>\n",
       "      <td>-124.61549</td>\n",
       "      <td>6.7</td>\n",
       "      <td>20.4</td>\n",
       "      <td>2024-07-09 23:59:25</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.97417</td>\n",
       "      <td>-124.61268</td>\n",
       "      <td>6.8</td>\n",
       "      <td>20.8</td>\n",
       "      <td>2024-07-10 00:02:39</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.97894</td>\n",
       "      <td>-124.60952</td>\n",
       "      <td>6.3</td>\n",
       "      <td>25.4</td>\n",
       "      <td>2024-07-10 00:05:46</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.98712</td>\n",
       "      <td>-124.60252</td>\n",
       "      <td>7.4</td>\n",
       "      <td>173.0</td>\n",
       "      <td>2024-07-10 01:38:10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.95275</td>\n",
       "      <td>-124.61481</td>\n",
       "      <td>6.3</td>\n",
       "      <td>213.6</td>\n",
       "      <td>2024-07-10 01:57:13</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.94992</td>\n",
       "      <td>-124.62244</td>\n",
       "      <td>7.4</td>\n",
       "      <td>242.7</td>\n",
       "      <td>2024-07-10 02:00:24</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.94751</td>\n",
       "      <td>-124.63030</td>\n",
       "      <td>7.3</td>\n",
       "      <td>246.9</td>\n",
       "      <td>2024-07-10 02:03:36</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.94453</td>\n",
       "      <td>-124.63737</td>\n",
       "      <td>7.0</td>\n",
       "      <td>239.6</td>\n",
       "      <td>2024-07-10 02:06:44</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.93983</td>\n",
       "      <td>-124.64311</td>\n",
       "      <td>7.5</td>\n",
       "      <td>221.3</td>\n",
       "      <td>2024-07-10 02:09:59</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.93617</td>\n",
       "      <td>-124.64938</td>\n",
       "      <td>6.9</td>\n",
       "      <td>231.0</td>\n",
       "      <td>2024-07-10 02:13:06</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.93387</td>\n",
       "      <td>-124.65702</td>\n",
       "      <td>7.1</td>\n",
       "      <td>247.2</td>\n",
       "      <td>2024-07-10 02:16:19</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.93172</td>\n",
       "      <td>-124.66430</td>\n",
       "      <td>6.8</td>\n",
       "      <td>247.7</td>\n",
       "      <td>2024-07-10 02:19:30</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.92863</td>\n",
       "      <td>-124.66380</td>\n",
       "      <td>5.3</td>\n",
       "      <td>165.0</td>\n",
       "      <td>2024-07-10 02:25:56</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.91717</td>\n",
       "      <td>-124.66266</td>\n",
       "      <td>3.7</td>\n",
       "      <td>210.1</td>\n",
       "      <td>2024-07-10 02:35:32</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.91949</td>\n",
       "      <td>-124.67176</td>\n",
       "      <td>4.6</td>\n",
       "      <td>322.5</td>\n",
       "      <td>2024-07-10 02:41:49</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.93281</td>\n",
       "      <td>-124.68434</td>\n",
       "      <td>4.3</td>\n",
       "      <td>340.3</td>\n",
       "      <td>2024-07-10 02:54:32</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.91386</td>\n",
       "      <td>-124.65799</td>\n",
       "      <td>4.0</td>\n",
       "      <td>352.9</td>\n",
       "      <td>2024-07-10 08:43:43</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.91753</td>\n",
       "      <td>-124.66124</td>\n",
       "      <td>5.2</td>\n",
       "      <td>327.4</td>\n",
       "      <td>2024-07-10 08:46:48</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.92642</td>\n",
       "      <td>-124.66605</td>\n",
       "      <td>6.3</td>\n",
       "      <td>347.4</td>\n",
       "      <td>2024-07-10 08:53:10</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.90848</td>\n",
       "      <td>-124.65206</td>\n",
       "      <td>6.7</td>\n",
       "      <td>292.4</td>\n",
       "      <td>2024-07-10 11:22:27</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.90829</td>\n",
       "      <td>-124.66014</td>\n",
       "      <td>6.9</td>\n",
       "      <td>268.1</td>\n",
       "      <td>2024-07-10 11:25:33</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.90810</td>\n",
       "      <td>-124.66814</td>\n",
       "      <td>6.9</td>\n",
       "      <td>268.0</td>\n",
       "      <td>2024-07-10 11:28:39</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.90783</td>\n",
       "      <td>-124.67614</td>\n",
       "      <td>6.9</td>\n",
       "      <td>267.4</td>\n",
       "      <td>2024-07-10 11:31:45</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.90856</td>\n",
       "      <td>-124.68434</td>\n",
       "      <td>7.1</td>\n",
       "      <td>276.9</td>\n",
       "      <td>2024-07-10 11:34:56</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.90870</td>\n",
       "      <td>-124.69276</td>\n",
       "      <td>7.2</td>\n",
       "      <td>271.3</td>\n",
       "      <td>2024-07-10 11:38:08</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.90776</td>\n",
       "      <td>-124.70126</td>\n",
       "      <td>7.4</td>\n",
       "      <td>261.2</td>\n",
       "      <td>2024-07-10 11:41:18</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.90700</td>\n",
       "      <td>-124.70975</td>\n",
       "      <td>7.4</td>\n",
       "      <td>262.9</td>\n",
       "      <td>2024-07-10 11:44:28</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.90649</td>\n",
       "      <td>-124.71862</td>\n",
       "      <td>7.7</td>\n",
       "      <td>265.4</td>\n",
       "      <td>2024-07-10 11:47:40</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.90629</td>\n",
       "      <td>-124.72702</td>\n",
       "      <td>7.2</td>\n",
       "      <td>268.1</td>\n",
       "      <td>2024-07-10 11:50:47</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.90651</td>\n",
       "      <td>-124.73548</td>\n",
       "      <td>7.3</td>\n",
       "      <td>272.0</td>\n",
       "      <td>2024-07-10 11:54:00</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.90658</td>\n",
       "      <td>-124.74356</td>\n",
       "      <td>6.9</td>\n",
       "      <td>270.7</td>\n",
       "      <td>2024-07-10 11:57:13</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.90678</td>\n",
       "      <td>-124.75932</td>\n",
       "      <td>6.9</td>\n",
       "      <td>273.3</td>\n",
       "      <td>2024-07-10 12:03:32</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.90762</td>\n",
       "      <td>-124.76682</td>\n",
       "      <td>6.5</td>\n",
       "      <td>278.7</td>\n",
       "      <td>2024-07-10 12:06:40</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.90810</td>\n",
       "      <td>-124.77497</td>\n",
       "      <td>7.0</td>\n",
       "      <td>274.6</td>\n",
       "      <td>2024-07-10 12:09:51</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.90783</td>\n",
       "      <td>-124.78283</td>\n",
       "      <td>6.8</td>\n",
       "      <td>267.3</td>\n",
       "      <td>2024-07-10 12:12:59</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.90619</td>\n",
       "      <td>-124.79094</td>\n",
       "      <td>7.2</td>\n",
       "      <td>254.3</td>\n",
       "      <td>2024-07-10 12:16:08</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.90754</td>\n",
       "      <td>-124.79542</td>\n",
       "      <td>4.2</td>\n",
       "      <td>292.6</td>\n",
       "      <td>2024-07-10 12:19:21</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.91332</td>\n",
       "      <td>-124.87436</td>\n",
       "      <td>7.0</td>\n",
       "      <td>306.1</td>\n",
       "      <td>2024-07-10 13:10:01</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.93039</td>\n",
       "      <td>-124.87463</td>\n",
       "      <td>6.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2024-07-10 13:19:31</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.92585</td>\n",
       "      <td>-124.89269</td>\n",
       "      <td>6.1</td>\n",
       "      <td>287.9</td>\n",
       "      <td>2024-07-10 13:51:15</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.93127</td>\n",
       "      <td>-124.89259</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2024-07-10 13:54:25</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.93655</td>\n",
       "      <td>-124.89223</td>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2024-07-10 13:57:37</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>100011758</td>\n",
       "      <td>43.94198</td>\n",
       "      <td>-124.89179</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3.3</td>\n",
       "      <td>2024-07-10 14:00:56</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         MMSI  Latitude  Longitude  SOG    COG           Timestamp  \\\n",
       "0   100011758  43.92981 -124.62096  5.6  180.3 2024-07-09 15:37:13   \n",
       "1   100011758  43.93671 -124.61779  4.7   21.5 2024-07-09 20:39:09   \n",
       "2   100011758  43.93372 -124.61674  3.0  176.4 2024-07-09 20:56:43   \n",
       "3   100011758  43.93272 -124.61994  3.0  246.4 2024-07-09 20:58:02   \n",
       "4   100011758  43.95354 -124.62340  6.3   10.7 2024-07-09 23:49:54   \n",
       "5   100011758  43.95859 -124.62075  6.4   20.6 2024-07-09 23:53:08   \n",
       "6   100011758  43.96356 -124.61822  6.3   20.1 2024-07-09 23:56:14   \n",
       "7   100011758  43.96884 -124.61549  6.7   20.4 2024-07-09 23:59:25   \n",
       "8   100011758  43.97417 -124.61268  6.8   20.8 2024-07-10 00:02:39   \n",
       "9   100011758  43.97894 -124.60952  6.3   25.4 2024-07-10 00:05:46   \n",
       "10  100011758  43.98712 -124.60252  7.4  173.0 2024-07-10 01:38:10   \n",
       "11  100011758  43.95275 -124.61481  6.3  213.6 2024-07-10 01:57:13   \n",
       "12  100011758  43.94992 -124.62244  7.4  242.7 2024-07-10 02:00:24   \n",
       "13  100011758  43.94751 -124.63030  7.3  246.9 2024-07-10 02:03:36   \n",
       "14  100011758  43.94453 -124.63737  7.0  239.6 2024-07-10 02:06:44   \n",
       "15  100011758  43.93983 -124.64311  7.5  221.3 2024-07-10 02:09:59   \n",
       "16  100011758  43.93617 -124.64938  6.9  231.0 2024-07-10 02:13:06   \n",
       "17  100011758  43.93387 -124.65702  7.1  247.2 2024-07-10 02:16:19   \n",
       "18  100011758  43.93172 -124.66430  6.8  247.7 2024-07-10 02:19:30   \n",
       "19  100011758  43.92863 -124.66380  5.3  165.0 2024-07-10 02:25:56   \n",
       "20  100011758  43.91717 -124.66266  3.7  210.1 2024-07-10 02:35:32   \n",
       "21  100011758  43.91949 -124.67176  4.6  322.5 2024-07-10 02:41:49   \n",
       "22  100011758  43.93281 -124.68434  4.3  340.3 2024-07-10 02:54:32   \n",
       "23  100011758  43.91386 -124.65799  4.0  352.9 2024-07-10 08:43:43   \n",
       "24  100011758  43.91753 -124.66124  5.2  327.4 2024-07-10 08:46:48   \n",
       "25  100011758  43.92642 -124.66605  6.3  347.4 2024-07-10 08:53:10   \n",
       "26  100011758  43.90848 -124.65206  6.7  292.4 2024-07-10 11:22:27   \n",
       "27  100011758  43.90829 -124.66014  6.9  268.1 2024-07-10 11:25:33   \n",
       "28  100011758  43.90810 -124.66814  6.9  268.0 2024-07-10 11:28:39   \n",
       "29  100011758  43.90783 -124.67614  6.9  267.4 2024-07-10 11:31:45   \n",
       "30  100011758  43.90856 -124.68434  7.1  276.9 2024-07-10 11:34:56   \n",
       "31  100011758  43.90870 -124.69276  7.2  271.3 2024-07-10 11:38:08   \n",
       "32  100011758  43.90776 -124.70126  7.4  261.2 2024-07-10 11:41:18   \n",
       "33  100011758  43.90700 -124.70975  7.4  262.9 2024-07-10 11:44:28   \n",
       "34  100011758  43.90649 -124.71862  7.7  265.4 2024-07-10 11:47:40   \n",
       "35  100011758  43.90629 -124.72702  7.2  268.1 2024-07-10 11:50:47   \n",
       "36  100011758  43.90651 -124.73548  7.3  272.0 2024-07-10 11:54:00   \n",
       "37  100011758  43.90658 -124.74356  6.9  270.7 2024-07-10 11:57:13   \n",
       "38  100011758  43.90678 -124.75932  6.9  273.3 2024-07-10 12:03:32   \n",
       "39  100011758  43.90762 -124.76682  6.5  278.7 2024-07-10 12:06:40   \n",
       "40  100011758  43.90810 -124.77497  7.0  274.6 2024-07-10 12:09:51   \n",
       "41  100011758  43.90783 -124.78283  6.8  267.3 2024-07-10 12:12:59   \n",
       "42  100011758  43.90619 -124.79094  7.2  254.3 2024-07-10 12:16:08   \n",
       "43  100011758  43.90754 -124.79542  4.2  292.6 2024-07-10 12:19:21   \n",
       "44  100011758  43.91332 -124.87436  7.0  306.1 2024-07-10 13:10:01   \n",
       "45  100011758  43.93039 -124.87463  6.4    1.6 2024-07-10 13:19:31   \n",
       "46  100011758  43.92585 -124.89269  6.1  287.9 2024-07-10 13:51:15   \n",
       "47  100011758  43.93127 -124.89259  6.4    0.7 2024-07-10 13:54:25   \n",
       "48  100011758  43.93655 -124.89223  6.3    2.8 2024-07-10 13:57:37   \n",
       "49  100011758  43.94198 -124.89179  6.5    3.3 2024-07-10 14:00:56   \n",
       "\n",
       "    TrajectoryID  \n",
       "0              1  \n",
       "1              2  \n",
       "2              2  \n",
       "3              2  \n",
       "4              3  \n",
       "5              3  \n",
       "6              3  \n",
       "7              3  \n",
       "8              3  \n",
       "9              3  \n",
       "10             4  \n",
       "11             4  \n",
       "12             4  \n",
       "13             4  \n",
       "14             4  \n",
       "15             4  \n",
       "16             4  \n",
       "17             4  \n",
       "18             4  \n",
       "19             4  \n",
       "20             4  \n",
       "21             4  \n",
       "22             4  \n",
       "23             5  \n",
       "24             5  \n",
       "25             5  \n",
       "26             6  \n",
       "27             6  \n",
       "28             6  \n",
       "29             6  \n",
       "30             6  \n",
       "31             6  \n",
       "32             6  \n",
       "33             6  \n",
       "34             6  \n",
       "35             6  \n",
       "36             6  \n",
       "37             6  \n",
       "38             6  \n",
       "39             6  \n",
       "40             6  \n",
       "41             6  \n",
       "42             6  \n",
       "43             6  \n",
       "44             6  \n",
       "45             6  \n",
       "46             6  \n",
       "47             6  \n",
       "48             6  \n",
       "49             6  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FILE_PATH_TEST = 'Dataset/blocco_000-segmentato.parquet'\n",
    "COLUMNS_TO_READ = ['MMSI', 'Latitude', 'Longitude','SOG', 'COG', 'Timestamp','TrajectoryID']\n",
    "\n",
    "df = pd.read_parquet(\n",
    "        FILE_PATH_TEST, \n",
    "        columns=COLUMNS_TO_READ,\n",
    "        engine='pyarrow' \n",
    "    )\n",
    "\n",
    "df.head(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
